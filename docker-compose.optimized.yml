# SutazAI Optimized Docker Compose Configuration
# Full functionality with intelligent service organization and dependency management
# Systems Architect: Infrastructure DevOps Manager (INFRA-001) 
# Optimization Date: 2025-08-08
version: '3.8'

networks:
  sutazai-network:
    name: sutazai-network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16

# ========================================
# X-TEMPLATES FOR REUSABILITY
# ========================================

x-common-variables: &common-variables
  SUTAZAI_ENV: ${SUTAZAI_ENV:-production}
  TZ: ${TZ:-UTC}

x-database-config: &database-config
  DATABASE_URL: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-sutazai}
  REDIS_URL: redis://redis:6379/0

x-ollama-config: &ollama-config
  OLLAMA_API_KEY: local
  OLLAMA_BASE_URL: http://ollama:11434
  OLLAMA_HOST: 0.0.0.0
  OLLAMA_ORIGINS: '*'

x-vector-config: &vector-config
  CHROMADB_URL: http://chromadb:8000
  QDRANT_URL: http://qdrant:6333
  FAISS_INDEX_PATH: /data/faiss
  NEO4J_URI: bolt://neo4j:7687
  NEO4J_USER: neo4j
  NEO4J_PASSWORD: ${NEO4J_PASSWORD}

x-monitoring-healthcheck: &monitoring-healthcheck
  test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 30s

x-agent-healthcheck: &agent-healthcheck
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  interval: 60s
  timeout: 30s
  retries: 5
  start_period: 120s

x-resource-limits-small: &resource-limits-small
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: 512M
      reservations:
        cpus: '0.1'
        memory: 128M

x-resource-limits-medium: &resource-limits-medium
  deploy:
    resources:
      limits:
        cpus: '2'
        memory: 2G
      reservations:
        cpus: '0.5'
        memory: 512M

x-resource-limits-large: &resource-limits-large
  deploy:
    resources:
      limits:
        cpus: '4'
        memory: 4G
      reservations:
        cpus: '1'
        memory: 1G

services:
  # ========================================
  # TIER 1: CORE INFRASTRUCTURE LAYER
  # Critical services that everything depends on
  # ========================================

  postgres:
    image: postgres:16.3-alpine
    container_name: sutazai-postgres
    <<: *resource-limits-medium
    environment:
      <<: *common-variables
      POSTGRES_DB: ${POSTGRES_DB:-sutazai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER:-sutazai}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-sutazai}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - sutazai-network
    ports:
      - "10000:5432"
    restart: unless-stopped
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./IMPORTANT/DATABASE_SCHEMA.sql:/docker-entrypoint-initdb.d/01_schema.sql:ro
      - ./IMPORTANT/init_db.sql:/docker-entrypoint-initdb.d/02_init.sql:ro
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7.2-alpine
    container_name: sutazai-redis
    <<: *resource-limits-small
    command: redis-server --save "" --appendonly no --maxmemory 512mb --maxmemory-policy allkeys-lru
    environment:
      <<: *common-variables
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "10001:6379"
    restart: unless-stopped
    volumes:
      - redis_data:/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  neo4j:
    image: neo4j:5.13-community
    container_name: sutazai-neo4j
    <<: *resource-limits-medium
    environment:
      <<: *common-variables
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD}
      NEO4J_server_memory_heap_max__size: 512m
      NEO4J_server_memory_heap_initial__size: 256m
      NEO4J_server_memory_pagecache_size: 256m
      NEO4J_server_jvm_additional: -XX:+UseG1GC -XX:G1HeapRegionSize=4m -XX:+DisableExplicitGC -XX:+ExitOnOutOfMemoryError
      NEO4J_initial_dbms_default__database: sutazai
      NEO4J_db_checkpoint_interval_time: 30s
      NEO4J_db_transaction_timeout: 30s
      NEO4J_db_logs_query_enabled: OFF
      NEO4J_server_config_strict__validation_enabled: false
      NEO4J_db_transaction_bookmark_ready_timeout: 5s
      NEO4J_dbms_cluster_discovery_type: SINGLE
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474/ || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 45s
    networks:
      - sutazai-network
    ports:
      - "10002:7474"
      - "10003:7687"
    restart: unless-stopped
    volumes:
      - neo4j_data:/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # TIER 2: AI/ML INFRASTRUCTURE
  # Vector databases and model serving
  # ========================================

  ollama:
    image: ollama/ollama:latest
    container_name: sutazai-ollama
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G
    environment:
      <<: *common-variables
      <<: *ollama-config
      OLLAMA_DEBUG: false
      OLLAMA_FLASH_ATTENTION: 1
      OLLAMA_KEEP_ALIVE: 10m
      OLLAMA_MAX_LOADED_MODELS: 3
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_NUM_PARALLEL: 20
      OLLAMA_NUM_THREADS: 8
      OLLAMA_RUNNERS_DIR: /tmp
      OLLAMA_TMPDIR: /tmp
      DEFAULT_MODEL: tinyllama  # FIXED: Use available model
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      - sutazai-network
    ports:
      - "10104:11434"
    restart: unless-stopped
    sysctls:
      - net.core.somaxconn=65535
    ulimits:
      nofile:
        hard: 65536
        soft: 65536
    volumes:
      - ollama_data:/root/.ollama
      - models_data:/models
      - /opt/sutazaiapp/CLAUDE.md:/app/CLAUDE.md:ro
      - /opt/sutazaiapp/config/ollama.yaml:/app/config/ollama.yaml:ro
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  chromadb:
    image: chromadb/chroma:0.5.0
    container_name: sutazai-chromadb
    <<: *resource-limits-medium
    environment:
      <<: *common-variables
      CHROMA_SERVER_AUTH_PROVIDER: chromadb.auth.token.TokenAuthenticationServerProvider
      CHROMA_SERVER_AUTH_CREDENTIALS: ${CHROMADB_API_KEY:-test-token}
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
      CHROMA_SERVER_CORS_ALLOW_ORIGINS: '["http://localhost:8501", "http://backend:8000"]'
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:8000/api/v1/heartbeat || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s
    networks:
      - sutazai-network
    ports:
      - "10100:8000"
    restart: unless-stopped
    volumes:
      - chromadb_data:/chroma/chroma
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  qdrant:
    image: qdrant/qdrant:v1.9.2
    container_name: sutazai-qdrant
    <<: *resource-limits-medium
    environment:
      <<: *common-variables
      QDRANT__LOG_LEVEL: INFO
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__SERVICE__HTTP_PORT: 6333
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:6333/health || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "10101:6333"
      - "10102:6334"
    restart: unless-stopped
    volumes:
      - qdrant_data:/qdrant/storage
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  faiss:
    build:
      context: ./docker/faiss
      dockerfile: Dockerfile
    container_name: sutazai-faiss
    <<: *resource-limits-small
    environment:
      <<: *common-variables
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 60s
      timeout: 30s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "10103:8000"
    restart: unless-stopped
    volumes:
      - faiss_data:/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # TIER 3: SERVICE MESH & ORCHESTRATION
  # API Gateway, Service Discovery, Messaging
  # ========================================

  kong:
    image: kong:3.5
    container_name: sutazai-kong
    # SECURITY FIX: Added proper resource limits to prevent memory exhaustion
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G  # Increased from 1G to prevent 99.82% usage
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      <<: *common-variables
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: "/etc/kong/kong.yml"
      KONG_PROXY_LISTEN: "0.0.0.0:8000"
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
      KONG_LOG_LEVEL: "notice"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8001/status"]
      interval: 30s
      timeout: 5s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "10005:8000"
      - "10015:8001"
    restart: unless-stopped
    volumes:
      - ./config/kong/kong.yml:/etc/kong/kong.yml:ro
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  consul:
    image: hashicorp/consul:1.17
    container_name: sutazai-consul
    command: ["agent", "-server", "-bootstrap-expect=1", "-ui", "-client=0.0.0.0"]
    environment:
      <<: *common-variables
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8500/v1/status/leader"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 10s
    networks:
      - sutazai-network
    ports:
      - "10006:8500"
    restart: unless-stopped
    volumes:
      - ./config/consul/consul.hcl:/consul/config/consul.hcl:ro
      - consul_data:/consul/data
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: sutazai-rabbitmq
    environment:
      <<: *common-variables
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER:-sutazai}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:15672/api/health/checks/virtual-hosts"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - sutazai-network
    ports:
      - "10007:5672"
      - "10008:15672"
    restart: unless-stopped
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # ========================================
  # TIER 4: APPLICATION LAYER
  # Backend API and Frontend UI
  # ========================================

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: sutazai-backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload --workers 1
    <<: *resource-limits-large
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    environment:
      <<: *common-variables
      <<: *database-config
      <<: *ollama-config
      <<: *vector-config
      API_V1_STR: /api/v1
      BACKEND_CORS_ORIGINS: '["http://localhost:10011", "http://172.31.77.193:10011"]'
      CHROMADB_HOST: chromadb
      CHROMADB_PORT: 8000
      FAISS_INDEX_PATH: /data/faiss
      GRAFANA_PASSWORD: ${GRAFANA_PASSWORD}
      JWT_SECRET: ${JWT_SECRET}
      NEO4J_HOST: neo4j
      NEO4J_PORT: 7687
      DEFAULT_MODEL: tinyllama  # FIXED: Match available model
      FALLBACK_MODEL: tinyllama  # FIXED: Match available model  
      POSTGRES_DB: ${POSTGRES_DB:-sutazai}
      POSTGRES_HOST: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-sutazai}
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333
      REDIS_HOST: redis
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_PORT: 6379
      SECRET_KEY: ${SECRET_KEY}
    healthcheck:
      test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.settimeout(5); exit(0 if s.connect_ex((\"localhost\", 8000))==0 else 1)"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s
    networks:
      - sutazai-network
    ports:
      - "10010:8000"
    restart: unless-stopped
    volumes:
      - ./backend:/app
      - ./data:/data
      - ./logs:/logs
      - agent_workspaces:/app/agent_workspaces
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "5"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: sutazai-frontend
    command: streamlit run app.py --server.port 8501 --server.address 0.0.0.0
    <<: *resource-limits-small
    depends_on:
      backend:
        condition: service_healthy
    environment:
      <<: *common-variables
      BACKEND_URL: http://backend:8000
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
      STREAMLIT_SERVER_PORT: 8501
    healthcheck:
      test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.settimeout(5); exit(0 if s.connect_ex((\"localhost\", 8501))==0 else 1)"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s
    networks:
      - sutazai-network
    ports:
      - "10011:8501"
    restart: unless-stopped
    volumes:
      - ./frontend:/app
      - ./data:/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # TIER 5: MONITORING & OBSERVABILITY
  # Full production monitoring stack
  # ========================================

  prometheus:
    image: prom/prometheus:latest
    container_name: sutazai-prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
      - --web.enable-lifecycle
      - --storage.tsdb.retention.time=15d
      - --web.enable-admin-api
      - --storage.tsdb.max-block-duration=2h
      - --storage.tsdb.min-block-duration=2h
      - --storage.tsdb.retention.size=2GB
    <<: *resource-limits-medium
    environment:
      <<: *common-variables
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 90s
    networks:
      - sutazai-network
    ports:
      - "10200:9090"
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:latest
    container_name: sutazai-grafana
    <<: *resource-limits-small
    depends_on:
      - prometheus
      - loki
    environment:
      <<: *common-variables
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: ""
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH: /var/lib/grafana/dashboards/system-overview.json
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      - sutazai-network
    ports:
      - "10201:3000"
    restart: unless-stopped
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  loki:
    image: grafana/loki:2.9.0
    container_name: sutazai-loki
    command: -config.file=/etc/loki/local-config.yaml
    <<: *resource-limits-small
    environment:
      <<: *common-variables
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      - sutazai-network
    ports:
      - "10202:3100"
    restart: unless-stopped
    volumes:
      - loki_data:/loki
      - ./monitoring/loki/config.yml:/etc/loki/local-config.yaml
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  alertmanager:
    image: prom/alertmanager:latest
    container_name: sutazai-alertmanager
    command:
      - --config.file=/etc/alertmanager/config.yml
      - --storage.path=/alertmanager
      - --web.external-url=http://localhost:9093
    environment:
      <<: *common-variables
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
      SLACK_AI_WEBHOOK_URL: ${SLACK_AI_WEBHOOK_URL:-}
      SLACK_SECURITY_WEBHOOK_URL: ${SLACK_SECURITY_WEBHOOK_URL:-}
      PAGERDUTY_SERVICE_KEY: ${PAGERDUTY_SERVICE_KEY:-}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/ready"]
      interval: 60s
      timeout: 30s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "11108:9093"
    restart: unless-stopped
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  node-exporter:
    image: prom/node-exporter:latest
    container_name: sutazai-node-exporter
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/rootfs
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    networks:
      - sutazai-network
    ports:
      - "10205:9100"
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: sutazai-cadvisor
    command:
      - '--housekeeping_interval=30s'
      - '--max_housekeeping_interval=35s'
      - '--allow_dynamic_housekeeping=true'
      - '--global_housekeeping_interval=1m'
      - '--disable_metrics=advtcp,cpu_topology,disk,hugetlb,memory_numa,percpu,referenced_memory,resctrl,tcp,udp'
    <<: *resource-limits-small
    devices:
      - /dev/kmsg
    networks:
      - sutazai-network
    ports:
      - "10206:8080"
    privileged: true
    restart: unless-stopped
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Additional monitoring exporters
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: sutazai-postgres-exporter
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *common-variables
      DATA_SOURCE_NAME: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-sutazai}?sslmode=disable
    networks:
      - sutazai-network
    ports:
      - "10207:9187"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: sutazai-redis-exporter
    depends_on:
      redis:
        condition: service_healthy
    environment:
      <<: *common-variables
      REDIS_ADDR: redis://redis:6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    networks:
      - sutazai-network
    ports:
      - "10208:9121"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  blackbox-exporter:
    image: prom/blackbox-exporter:latest
    container_name: sutazai-blackbox-exporter
    command:
      - --config.file=/etc/blackbox_exporter/config.yml
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9115/"]
      interval: 60s
      timeout: 30s
      retries: 5
    networks:
      - sutazai-network
    ports:
      - "10204:9115"
    restart: unless-stopped
    volumes:
      - ./monitoring/blackbox/config.yml:/etc/blackbox_exporter/config.yml
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  promtail:
    image: grafana/promtail:2.9.0
    container_name: sutazai-promtail
    command: -config.file=/etc/promtail/config.yml
    <<: *resource-limits-small
    depends_on:
      - loki
    networks:
      - sutazai-network
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail/config.yml:/etc/promtail/config.yml
      - ./logs:/app/logs:ro
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # ========================================
  # TIER 6: AI AGENT ECOSYSTEM
  # Organized by functionality and priority
  # ========================================

  # Core Agent Infrastructure
  ollama-integration:
    build:
      context: ./agents/ollama_integration
      dockerfile: Dockerfile
    container_name: sutazai-ollama-integration
    <<: *resource-limits-medium
    depends_on:
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_TYPE: ollama-integration
      API_ENDPOINT: http://backend:8000
      LOG_LEVEL: INFO
      PORT: 8090
      REDIS_URL: redis://redis:6379/0
      MAX_RETRIES: 3
      BACKOFF_BASE: 2
      REQUEST_TIMEOUT: 30
      CONNECTION_POOL_SIZE: 10
    healthcheck:
      <<: *agent-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    networks:
      - sutazai-network
    ports:
      - "8090:8090"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # AI Framework Integration Services
  langflow:
    image: langflowai/langflow:latest
    container_name: sutazai-langflow
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *common-variables
      <<: *database-config
      DATABASE_URL: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD}@postgres:5432/langflow
      LANGFLOW_DATABASE_URL: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD}@postgres:5432/langflow
    healthcheck:
      <<: *agent-healthcheck
      test: ["CMD", "python3", "-c", "import socket; s=socket.socket(); s.settimeout(5); exit(0 if s.connect_ex((\"localhost\", 7860))==0 else 1)"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s
    networks:
      - sutazai-network
    ports:
      - "10400:7860"
    restart: unless-stopped
    volumes:
      - ./data/langflow:/app/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  flowise:
    image: flowiseai/flowise:latest
    container_name: sutazai-flowise
    environment:
      <<: *common-variables
      DATABASE_PATH: /opt/flowise/.flowise
      PORT: 3000
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/v1/ping"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      - sutazai-network
    ports:
      - "10401:3000"
    restart: unless-stopped
    volumes:
      - ./data/flowise:/opt/flowise/.flowise
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  n8n:
    image: n8nio/n8n:latest
    container_name: sutazai-n8n
    environment:
      <<: *common-variables
      N8N_BASIC_AUTH_ACTIVE: true
      N8N_BASIC_AUTH_USER: ${N8N_USER:-admin}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD:-sutazai_n8n}
      WEBHOOK_URL: http://localhost:5678/
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5678/healthz"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 60s
    networks:
      - sutazai-network
    ports:
      - "10403:5678"
    restart: unless-stopped
    volumes:
      - ./data/n8n:/home/node/.n8n
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  dify:
    image: langgenius/dify-api:latest
    container_name: sutazai-dify
    depends_on:
      - postgres
      - redis
      - ollama
    environment:
      <<: *common-variables
      <<: *database-config
      APP_WEB_URL: http://localhost:10412
      CONSOLE_API_URL: http://localhost:10412
      CONSOLE_WEB_URL: http://localhost:10412
      INIT_PASSWORD: admin
      LOG_LEVEL: INFO
      MODE: standalone
      SECRET_KEY: ${SECRET_KEY}
      SERVICE_API_URL: http://localhost:10412
      STORAGE_LOCAL_PATH: /app/storage
      STORAGE_TYPE: local
    networks:
      - sutazai-network
    ports:
      - "10412:5000"
    restart: unless-stopped
    volumes:
      - ./data/dify:/app/storage
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # TIER 7: SPECIALIZED AI AGENTS
  # Advanced AI agents with specific capabilities
  # ========================================

  # Programming and Development Agents
  autogpt:
    build:
      context: ./docker/autogpt
      dockerfile: Dockerfile
    container_name: sutazai-autogpt
    depends_on:
      - backend
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: AutoGPT
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /app/workspace
    networks:
      - sutazai-network
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
      - agent_outputs:/app/outputs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  crewai:
    build:
      context: ./docker/crewai
      dockerfile: Dockerfile
    container_name: sutazai-crewai
    depends_on:
      - backend
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: CrewAI
      BACKEND_URL: http://backend:8000
      CREW_SIZE: 5
    networks:
      - sutazai-network
    ports:
      - "10300:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
      - agent_outputs:/app/outputs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  aider:
    build:
      context: ./docker/aider
      dockerfile: Dockerfile
    container_name: sutazai-aider
    depends_on:
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: Aider
      MODEL: tinyllama:latest
    networks:
      - sutazai-network
    ports:
      - "10301:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
      - ./backend:/app/backend:ro
      - ./frontend:/app/frontend:ro
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  gpt-engineer:
    build:
      context: ./docker/gpt-engineer
      dockerfile: Dockerfile
    container_name: sutazai-gpt-engineer
    depends_on:
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: GPT-Engineer
      MODEL: tinyllama:latest
    networks:
      - sutazai-network
    ports:
      - "11109:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
      - agent_outputs:/app/outputs
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # AI Framework Agents  
  llamaindex:
    build:
      context: ./docker/llamaindex
      dockerfile: Dockerfile
    container_name: sutazai-llamaindex
    depends_on:
      - chromadb
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      <<: *vector-config
    networks:
      - sutazai-network
    ports:
      - "10402:8080"
    restart: unless-stopped
    volumes:
      - ./data/documents:/app/documents
      - chromadb_data:/app/vector_store
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # System and Infrastructure Agents
  hardware-resource-optimizer:
    build:
      context: ./agents/hardware-resource-optimizer
      dockerfile: Dockerfile
    container_name: sutazai-hardware-resource-optimizer
    <<: *resource-limits-medium
    depends_on:
      backend:
        condition: service_started
      ollama:
        condition: service_started
      redis:
        condition: service_healthy
    environment:
      <<: *common-variables
      <<: *database-config
      <<: *ollama-config
      AGENT_TYPE: hardware-resource-optimizer
      API_ENDPOINT: http://backend:8000
      LOG_LEVEL: INFO
      OLLAMA_MODEL: tinyllama:latest
      PORT: 8080
    healthcheck:
      <<: *agent-healthcheck
    networks:
      - sutazai-network
    pid: host
    ports:
      - "11110:8080"
    privileged: true
    restart: unless-stopped
    volumes:
      - ./data:/app/data
      - ./configs:/app/configs
      - ./logs:/app/logs
      # SECURITY: Removed dangerous system mounts
      # - /proc:/host/proc:ro  # REMOVED - host process exposure
      # - /sys:/host/sys:ro  # REMOVED - host system exposure
      # - /var/run/docker.sock:/var/run/docker.sock  # REMOVED - container escape risk
      # - /tmp:/host/tmp  # REMOVED - host temp access
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # Document and Knowledge Management
  documind:
    build:
      context: ./docker/documind
      dockerfile: Dockerfile
    container_name: sutazai-documind
    environment:
      <<: *common-variables
    networks:
      - sutazai-network
    ports:
      - "10308:8000"
    restart: unless-stopped
    volumes:
      - ./data/documents:/app/documents
      - agent_outputs:/app/processed
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  privategpt:
    build:
      context: ./docker/privategpt
      dockerfile: Dockerfile
    container_name: sutazai-privategpt
    depends_on:
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: PrivateGPT
    networks:
      - sutazai-network
    ports:
      - "10306:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
      - ./data/documents:/app/documents
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # Security and Analysis Agents
  shellgpt:
    build:
      context: ./docker/shellgpt
      dockerfile: Dockerfile
    container_name: sutazai-shellgpt
    depends_on:
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      OPENAI_API_HOST: http://ollama:11434/v1
      OPENAI_API_KEY: not_needed
    networks:
      - sutazai-network
    ports:
      - "10307:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ========================================
  # TIER 8: FUTURE/EXPERIMENTAL SERVICES
  # Services kept for future implementation
  # ========================================

  # Development and Testing Services
  browser-use:
    build:
      context: ./docker/browser-use
      dockerfile: Dockerfile
    container_name: sutazai-browser-use
    depends_on:
      - ollama
    environment:
      <<: *common-variables
      <<: *ollama-config
      AGENT_NAME: BrowserUse
      DISPLAY: :99
    networks:
      - sutazai-network
    ports:
      - "10304:8080"
    restart: unless-stopped
    volumes:
      - agent_workspaces:/app/workspace
    profiles:
      - experimental
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Financial Analysis
  finrobot:
    build:
      context: ./docker/finrobot
      dockerfile: Dockerfile
    container_name: sutazai-finrobot
    environment:
      <<: *common-variables
      <<: *database-config
      <<: *ollama-config
    networks:
      - sutazai-network
    ports:
      - "10407:8080"
    restart: unless-stopped
    volumes:
      - ./data/financial:/data
    profiles:
      - experimental
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Code Development Tools
  awesome-code-ai:
    build:
      context: ./docker/awesome-code-ai
      dockerfile: Dockerfile
    container_name: sutazai-awesome-code-ai
    environment:
      <<: *common-variables
      <<: *ollama-config
    networks:
      - sutazai-network
    ports:
      - "10410:8080"
    restart: unless-stopped
    profiles:
      - experimental
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  opendevin:
    build:
      context: ./docker/opendevin
      dockerfile: Dockerfile
    container_name: sutazai-opendevin
    environment:
      <<: *common-variables
      <<: *ollama-config
      WORKSPACE_DIR: /workspace
    networks:
      - sutazai-network
    ports:
      - "10406:3000"
    restart: unless-stopped
    volumes:
      - ./workspace:/workspace
      # Docker socket mount REMOVED for security
      # - /var/run/docker.sock:/var/run/docker.sock  # SECURITY: Removed
    profiles:
      - experimental
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Code Completion and AI Tools  
  tabbyml:
    image: tabbyml/tabby:latest
    container_name: sutazai-tabbyml
    command:
      - serve
      - --model
      - TabbyML/StarCoder-1B
      - --device
      - cpu
      - --no-webserver
    environment:
      <<: *common-variables
      CUDA_VISIBLE_DEVICES: ''
      RUST_LOG: error
      TABBY_DISABLE_GPU: 'true'
      TABBY_DISABLE_USAGE_COLLECTION: 'true'
      ENABLE_TABBY: ${ENABLE_TABBY:-false}
    networks:
      - sutazai-network
    ports:
      - "10303:8080"
    profiles:
      - tabby
      - optional
    restart: unless-stopped
    volumes:
      - ./data/tabby:/data
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # ML Frameworks - Resource intensive, optional
  pytorch:
    build:
      context: ./docker/pytorch
      dockerfile: Dockerfile
    container_name: sutazai-pytorch
    deploy:
      resources:
        limits:
          cpus: '20'
          memory: 16G
    environment:
      <<: *common-variables
      JUPYTER_ENABLE_LAB: true
    networks:
      - sutazai-network
    ports:
      - "10500:8888"
    restart: unless-stopped
    profiles:
      - ml-heavy
    volumes:
      - models_data:/workspace/models
      - ./data/training:/workspace/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  tensorflow:
    build:
      context: ./docker/tensorflow
      dockerfile: Dockerfile
    container_name: sutazai-tensorflow
    deploy:
      resources:
        limits:
          cpus: '20'
          memory: 16G
    environment:
      <<: *common-variables
    networks:
      - sutazai-network
    ports:
      - "10501:8888"
    restart: unless-stopped
    profiles:
      - ml-heavy
    volumes:
      - models_data:/workspace/models
      - ./data/training:/workspace/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  jax:
    build:
      context: ./docker/jax
      dockerfile: Dockerfile
    container_name: sutazai-jax
    deploy:
      resources:
        limits:
          cpus: '20'
          memory: 16G
    environment:
      <<: *common-variables
    networks:
      - sutazai-network
    ports:
      - "10502:8080"
    restart: unless-stopped
    profiles:
      - ml-heavy
    volumes:
      - models_data:/workspace/models
      - ./data/training:/workspace/data
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # Distributed Training - Experimental
  fsdp:
    build:
      context: ./docker/fsdp
      dockerfile: Dockerfile
    container_name: sutazai-fsdp
    environment:
      <<: *common-variables
      ENABLE_FSDP: ${ENABLE_FSDP:-false}
    networks:
      - sutazai-network
    restart: unless-stopped
    volumes:
      - ./data/models:/models
    profiles:
      - fsdp
      - optional
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # ========================================
  # TIER 9: UTILITY AND MAINTENANCE SERVICES
  # Supporting services for system health
  # ========================================

  # System Health and Monitoring
  health-monitor:
    build:
      context: ./docker/health-monitor
      dockerfile: Dockerfile
    container_name: sutazai-health-monitor
    environment:
      <<: *common-variables
      ALERT_WEBHOOK_URL: ${HEALTH_ALERT_WEBHOOK:-}
      MONITOR_INTERVAL: 30
      SERVICES_TO_CHECK: sutazai-backend,sutazai-frontend,sutazai-postgres,sutazai-redis,sutazai-neo4j,sutazai-chromadb,sutazai-qdrant,sutazai-ollama,sutazai-prometheus,sutazai-grafana,sutazai-langflow,sutazai-flowise,sutazai-dify,sutazai-n8n
    networks:
      - sutazai-network
    ports:
      - "10210:8000"
    restart: unless-stopped
    volumes:
      # Docker socket mount REMOVED for security
      # - /var/run/docker.sock:/var/run/docker.sock:ro  # SECURITY: Removed
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Mesh workers for distributed processing
  mesh-worker:
    build:
      context: .
      dockerfile: docker/mesh-worker/Dockerfile
    container_name: sutazai-mesh-worker
    depends_on:
      redis:
        condition: service_healthy
    environment:
      <<: *common-variables
      REDIS_URL: redis://redis:6379/0
      LOG_LEVEL: INFO
      MESH_TOPIC: nlp
      MESH_GROUP: nlp
      MESH_CONSUMER: worker-1
      MESH_RESULTS: nlp
    networks:
      - sutazai-network
    restart: unless-stopped
    profiles:
      - mesh
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  mesh-worker-2:
    build:
      context: .
      dockerfile: docker/mesh-worker/Dockerfile
    container_name: sutazai-mesh-worker-2
    depends_on:
      redis:
        condition: service_healthy
    environment:
      <<: *common-variables
      REDIS_URL: redis://redis:6379/0
      LOG_LEVEL: INFO
      MESH_TOPIC: nlp
      MESH_GROUP: nlp
      MESH_CONSUMER: worker-2
      MESH_RESULTS: nlp
    networks:
      - sutazai-network
    restart: unless-stopped
    profiles:
      - mesh
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Security scanning - on demand
  semgrep:
    image: returntocorp/semgrep:latest
    container_name: sutazai-semgrep
    command:
      - semgrep
      - --config=auto
      - --output=/outputs/semgrep-report.json
      - --json
      - /src
    environment:
      <<: *common-variables
    networks:
      - sutazai-network
    restart: 'no'
    volumes:
      - ./backend:/src/backend:ro
      - ./frontend:/src/frontend:ro
      - agent_outputs:/outputs
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Service Hub for coordination
  service-hub:
    build:
      context: ./docker/service-hub
      dockerfile: Dockerfile
    container_name: sutazai-service-hub
    depends_on:
      - redis
    environment:
      <<: *common-variables
      <<: *database-config
    networks:
      - sutazai-network
    ports:
      - "10409:8080"
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

  # Code Improvement Service
  code-improver:
    build:
      context: ./docker/code-improver
      dockerfile: Dockerfile
    container_name: sutazai-code-improver
    environment:
      <<: *common-variables
      <<: *database-config
      <<: *ollama-config
      GIT_REPO_PATH: /opt/sutazaiapp
      IMPROVEMENT_SCHEDULE: 0 */6 * * *
      REQUIRE_APPROVAL: 'true'
    networks:
      - sutazai-network
    ports:
      - "10408:8080"
    restart: unless-stopped
    volumes:
      - ./:/opt/sutazaiapp
      # Docker socket mount REMOVED for security
      # - /var/run/docker.sock:/var/run/docker.sock  # SECURITY: Removed
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"

# ========================================
# PERSISTENT VOLUMES
# ========================================

volumes:
  # Database storage
  postgres_data:
    driver: local
  redis_data:
    driver: local
  neo4j_data:
    driver: local
    
  # Vector database storage
  chromadb_data:
    driver: local
  qdrant_data:
    driver: local
  faiss_data:
    driver: local
    
  # Monitoring storage
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
  alertmanager_data:
    driver: local
    
  # Service mesh storage
  consul_data:
    driver: local
  rabbitmq_data:
    driver: local
    
  # AI/ML model storage
  ollama_data:
    driver: local
  models_data:
    driver: local
    
  # Application data
  agent_workspaces:
    driver: local
  agent_outputs:
    driver: local