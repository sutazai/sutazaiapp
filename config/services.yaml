# SutazAI Unified Service Registry Configuration
# This file defines all external AI services and their configurations

services:
  # Vector Databases
  vector_databases:
    chromadb:
      enabled: true
      adapter: "vector_db.chromadb_adapter.ChromaDBAdapter"
      config:
        host: "chromadb"
        port: 8000
        persist_directory: "/data/chromadb"
        collection_name: "sutazai_vectors"
        ssl_enabled: false
      health_check:
        endpoint: "/api/v1/heartbeat"
        interval: 30
      resources:
        cpu: "2"
        memory: "4Gi"
        
    faiss:
      enabled: true
      adapter: "vector_db.faiss_adapter.FAISSAdapter"
      config:
        index_path: "/data/faiss/index.faiss"
        metadata_path: "/data/faiss/metadata.pkl"
        dimension: 1536
        index_type: "IVF"  # Flat, IVF, HNSW
      resources:
        cpu: "4"
        memory: "8Gi"
        gpu: "optional"
        
    qdrant:
      enabled: true
      adapter: "vector_db.qdrant_adapter.QdrantAdapter"
      config:
        host: "qdrant"
        port: 6333
        collection_name: "sutazai_vectors"
        vector_size: 1536
        distance: "Cosine"
      health_check:
        endpoint: "/collections"
        interval: 30
      resources:
        cpu: "2"
        memory: "4Gi"

  # AI Frameworks
  ai_frameworks:
    pytorch:
      enabled: true
      adapter: "ai_frameworks.pytorch_adapter.PyTorchAdapter"
      config:
        device: "cuda"
        model_path: "/models/pytorch"
        gpu_memory_limit: 8192
      resources:
        cpu: "8"
        memory: "16Gi"
        gpu: "required"
        
    tensorflow:
      enabled: true
      adapter: "ai_frameworks.tensorflow_adapter.TensorFlowAdapter"
      config:
        model_path: "/models/tensorflow"
        gpu_memory_limit: 4096
      resources:
        cpu: "8"
        memory: "16Gi"
        gpu: "required"
        
    jax:
      enabled: false
      adapter: "ai_frameworks.jax_adapter.JAXAdapter"
      config:
        device: "gpu"
        model_path: "/models/jax"
      resources:
        cpu: "8"
        memory: "16Gi"
        gpu: "required"

  # Agent Systems
  agent_systems:
    letta:
      enabled: true
      adapter: "agent_systems.letta_adapter.LettaAdapter"
      config:
        api_endpoint: "http://letta:8283"
        default_persona: "assistant"
        default_human: "user"
        memory_path: "/data/letta/memories"
      health_check:
        endpoint: "/api/health"
        interval: 30
      resources:
        cpu: "4"
        memory: "8Gi"
        
    autogpt:
      enabled: true
      adapter: "agent_systems.autogpt_adapter.AutoGPTAdapter"
      config:
        base_url: "http://autogpt:8000"
        workspace_path: "/data/autogpt/workspaces"
        plugins_path: "/data/autogpt/plugins"
        memory_backend: "redis"
      health_check:
        endpoint: "/api/v1/health"
        interval: 30
      resources:
        cpu: "4"
        memory: "8Gi"
        
    localagi:
      enabled: true
      adapter: "agent_systems.localagi_adapter.LocalAGIAdapter"
      config:
        base_url: "http://localagi:8080"
        model_backend: "ollama"
        tools_path: "/data/localagi/tools"
      resources:
        cpu: "2"
        memory: "4Gi"
        
    tabbyml:
      enabled: true
      adapter: "agent_systems.tabbyml_adapter.TabbyMLAdapter"
      config:
        base_url: "http://tabbyml:8080"
        model: "TabbyML/StarCoder-1B"
        device: "cuda"
      resources:
        cpu: "4"
        memory: "8Gi"
        gpu: "optional"

  # Workflow Tools
  workflow_tools:
    langflow:
      enabled: true
      adapter: "workflow_tools.langflow_adapter.LangFlowAdapter"
      config:
        base_url: "http://langflow:7860"
        database_url: "postgresql://langflow:password@postgres:5432/langflow"
        flows_path: "/data/langflow/flows"
      health_check:
        endpoint: "/health"
        interval: 30
      resources:
        cpu: "2"
        memory: "4Gi"
        
    dify:
      enabled: true
      adapter: "workflow_tools.dify_adapter.DifyAdapter"
      config:
        base_url: "http://dify-api:5001"
        web_url: "http://dify-web:3000"
        database_url: "postgresql://dify:password@postgres:5432/dify"
      resources:
        cpu: "4"
        memory: "8Gi"
        
    flowise:
      enabled: true
      adapter: "workflow_tools.flowise_adapter.FlowiseAdapter"
      config:
        base_url: "http://flowise:3000"
        database_path: "/data/flowise/database.sqlite"
        uploads_path: "/data/flowise/uploads"
      resources:
        cpu: "2"
        memory: "4Gi"
        
    n8n:
      enabled: true
      adapter: "workflow_tools.n8n_adapter.N8NAdapter"
      config:
        base_url: "http://n8n:5678"
        webhook_url: "http://n8n:5678/webhook"
        database_type: "postgres"
      resources:
        cpu: "2"
        memory: "4Gi"

  # Specialized Tools
  specialized_tools:
    finrobot:
      enabled: true
      adapter: "specialized_tools.finrobot_adapter.FinRobotAdapter"
      config:
        base_url: "http://finrobot:8000"
        data_sources:
          - "yahoo_finance"
          - "alpha_vantage"
          - "finnhub"
        analysis_modules:
          - "technical_analysis"
          - "sentiment_analysis"
          - "portfolio_optimization"
      resources:
        cpu: "4"
        memory: "8Gi"
        
    documind:
      enabled: true
      adapter: "specialized_tools.documind_adapter.DocumindAdapter"
      config:
        base_url: "http://documind:8080"
        ocr_engine: "tesseract"
        nlp_model: "bert-base-uncased"
        storage_path: "/data/documind/documents"
      resources:
        cpu: "4"
        memory: "8Gi"
        
    gpt_engineer:
      enabled: true
      adapter: "specialized_tools.gpt_engineer_adapter.GPTEngineerAdapter"
      config:
        base_url: "http://gpt-engineer:8000"
        projects_path: "/data/gpt-engineer/projects"
        templates_path: "/data/gpt-engineer/templates"
      resources:
        cpu: "4"
        memory: "8Gi"
        
    aider:
      enabled: true
      adapter: "specialized_tools.aider_adapter.AiderAdapter"
      config:
        base_url: "http://aider:8000"
        git_path: "/data/aider/repos"
        model: "gpt-4"
      resources:
        cpu: "2"
        memory: "4Gi"
        
    continue_dev:
      enabled: true
      adapter: "specialized_tools.continue_adapter.ContinueAdapter"
      config:
        base_url: "http://continue:8080"
        models:
          - "codellama"
          - "gpt-3.5-turbo"
        extensions_path: "/data/continue/extensions"
      resources:
        cpu: "2"
        memory: "4Gi"
        
    sweep:
      enabled: true
      adapter: "specialized_tools.sweep_adapter.SweepAdapter"
      config:
        base_url: "http://sweep:8000"
        github_token: "${GITHUB_TOKEN}"
        repos_path: "/data/sweep/repos"
      resources:
        cpu: "2"
        memory: "4Gi"
        
    pydantic_ai:
      enabled: true
      adapter: "specialized_tools.pydantic_ai_adapter.PydanticAIAdapter"
      config:
        base_url: "http://pydantic-ai:8000"
        validation_models_path: "/data/pydantic-ai/models"
      resources:
        cpu: "1"
        memory: "2Gi"
        
    mem0:
      enabled: true
      adapter: "specialized_tools.mem0_adapter.Mem0Adapter"
      config:
        base_url: "http://mem0:8000"
        memory_store: "redis"
        embedding_model: "all-MiniLM-L6-v2"
      resources:
        cpu: "2"
        memory: "4Gi"

# Service Discovery Configuration
service_discovery:
  consul:
    enabled: true
    host: "consul"
    port: 8500
    
  kubernetes:
    enabled: false
    namespace: "sutazai"
    
# Load Balancing Configuration
load_balancing:
  strategy: "round_robin"  # round_robin, least_connections, weighted
  health_check_interval: 30
  failure_threshold: 3
  
# API Gateway Routes
api_gateway:
  base_path: "/api/v1"
  routes:
    # Vector DB routes
    - path: "/vectors/chromadb"
      service: "chromadb"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    - path: "/vectors/faiss"
      service: "faiss"
      methods: ["GET", "POST", "PUT"]
      
    - path: "/vectors/qdrant"
      service: "qdrant"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    # AI Framework routes
    - path: "/ml/pytorch"
      service: "pytorch"
      methods: ["POST"]
      
    - path: "/ml/tensorflow"
      service: "tensorflow"
      methods: ["POST"]
      
    # Agent routes
    - path: "/agents/letta"
      service: "letta"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    - path: "/agents/autogpt"
      service: "autogpt"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    # Workflow routes
    - path: "/workflows/langflow"
      service: "langflow"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    - path: "/workflows/dify"
      service: "dify"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
    # Specialized tool routes
    - path: "/tools/{tool_name}"
      service: "{tool_name}"
      methods: ["GET", "POST", "PUT", "DELETE"]
      
# Resource Allocation Policies
resource_policies:
  gpu_allocation:
    strategy: "fair_share"  # fair_share, priority, dedicated
    oversubscription_ratio: 1.2
    
  memory_limits:
    enforce: true
    swap_accounting: true
    
  cpu_shares:
    base_share: 1024
    priority_multiplier: 2
    
# Monitoring Configuration
monitoring:
  metrics:
    enabled: true
    port: 9090
    path: "/metrics"
    
  tracing:
    enabled: true
    jaeger_endpoint: "http://jaeger:14268/api/traces"
    
  logging:
    level: "INFO"
    format: "json"
    output: "stdout"