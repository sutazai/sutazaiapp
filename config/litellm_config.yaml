general_settings:
  master_key: sk-sutazai-local-2025
litellm_settings:
  drop_params: true
  set_verbose: false
model_list:
# Ultra-lightweight models for resource-constrained environments
- litellm_params:
    api_base: http://sutazai-ollama:11434
    model: ollama/smollm:135m
    # Ultra-light model - only 91MB - for basic tasks
  model_name: gpt-3.5-turbo-light
- litellm_params:
    api_base: http://sutazai-ollama:11434
    model: ollama/smollm:360m
    # Small model - 229MB - for better quality while staying light
  model_name: gpt-3.5-turbo
- litellm_params:
    api_base: http://sutazai-ollama:11434
    model: ollama/tinyllama:1.1b
    # Small model - 637MB - for when you need more capability
  model_name: gpt-4
- litellm_params:
    api_base: http://sutazai-ollama:11434
    model: ollama/qwen2.5:3b
    # Fallback to larger model only when needed - 1.9GB
  model_name: gpt-4-turbo
- litellm_params:
    api_base: http://sutazai-ollama:11434
    model: ollama/nomic-embed-text
  model_name: text-embedding-ada-002
# Remove heavy models to prevent system freezing
# - litellm_params:
#     api_base: http://sutazai-ollama:11434
#     model: ollama/codellama:7b
#   model_name: code-davinci-002
router_settings:
  max_parallel_requests: 5  # Reduced from 100 to prevent resource exhaustion
  num_retries: 2  # Reduced from 3 to fail faster
  routing_strategy: least-busy  # Use least-busy instead of simple-shuffle for better resource management
  timeout: 180  # Reduced from 600 to 3 minutes for faster failures
  # Resource management settings for lightweight models
  cooldown_time: 2  # Seconds between requests to same model
  max_queue_size: 10  # Limit queue size to prevent memory issues
