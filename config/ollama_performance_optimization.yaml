# Advanced Ollama Performance Optimization Configuration
# Optimized for SutazAI 69-agent deployment with TinyLlama default

ollama_optimization:
  # Core Performance Settings
  performance:
    model_loading:
      preload_models: ["tinyllama"]     # Always keep TinyLlama loaded
      max_loaded_models: 2              # Keep 2 models in memory max
      model_unload_timeout: 120s        # Unload unused models after 2 minutes
      lazy_loading: false               # Disable for critical models
      model_switching_delay: 100ms      # Minimize model switching overhead
    
    inference_optimization:
      batch_size: 16                    # Optimal batch size for TinyLlama
      max_concurrent_requests: 32       # Handle multiple requests efficiently
      queue_timeout: 30s                # Maximum queue wait time
      request_timeout: 120s             # Maximum inference time
      response_streaming: true          # Enable streaming for faster perceived response
      
    caching:
      enable_response_cache: true       # Cache frequent responses
      cache_size_mb: 512               # Response cache size
      cache_ttl: 3600                  # Cache TTL in seconds
      enable_model_cache: true         # Cache model weights in memory
      model_cache_size_gb: 4           # Model cache size
      
    memory_management:
      aggressive_gc: true              # Aggressive garbage collection
      gc_interval: 30s                 # Run GC every 30 seconds
      memory_threshold: 85             # Trigger cleanup at 85% usage
      swap_prevention: true            # Prevent swapping to disk
      memory_mapping: true             # Use memory mapping for models
      
  # TinyLlama Specific Optimizations
  tinyllama_config:
    temperature: 0.3                   # Consistent responses
    top_p: 0.85                       # Balanced creativity
    top_k: 25                         # Optimal token selection
    repeat_penalty: 1.05              # Minimize repetition
    context_length: 2048              # Optimal context window
    max_tokens: 512                   # Reasonable response length
    num_threads: 8                    # CPU threads for inference
    num_gpu_layers: 0                 # CPU-only inference
    rope_frequency_base: 10000        # RoPE frequency optimization
    rope_frequency_scale: 1.0         # RoPE scaling
    
  # Multi-Model Configuration
  model_configs:
    tinyllama:
      priority: 1                     # Highest priority
      keep_loaded: true              # Always keep in memory
      batch_size: 16                 # Optimal batch size
      concurrent_requests: 32        # High concurrency
      
    "llama3.2:3b":
      priority: 2                     # Medium priority
      keep_loaded: false             # Load on demand
      batch_size: 8                  # Smaller batch for larger model
      concurrent_requests: 16        # Lower concurrency
      
    "deepseek-r1:8b":
      priority: 3                     # Lower priority
      keep_loaded: false             # Load on demand
      batch_size: 4                  # Small batch for large model
      concurrent_requests: 8         # Lowest concurrency
      
  # Connection Pool Optimization
  connection_pool:
    pool_size: 100                   # Large connection pool
    max_overflow: 50                 # Allow pool overflow
    timeout: 30                      # Connection timeout
    recycle: 3600                    # Recycle connections hourly
    pre_ping: true                   # Validate connections
    retry_on_disconnect: true        # Auto-retry on disconnect
    
  # Load Balancing Configuration
  load_balancing:
    strategy: "least_connections"     # Balance by connection count
    health_check_interval: 15s       # Frequent health checks
    failover_timeout: 5s             # Quick failover
    sticky_sessions: false           # No session stickiness
    circuit_breaker:
      failure_threshold: 10          # Open circuit after 10 failures
      recovery_timeout: 60s          # Try recovery after 1 minute
      half_open_requests: 3          # Test with 3 requests
      
  # Monitoring and Metrics
  monitoring:
    enable_metrics: true             # Enable detailed metrics
    metrics_interval: 10s            # Collect metrics every 10 seconds
    enable_tracing: true             # Enable request tracing
    log_level: "INFO"                # Balanced logging
    performance_logging: true        # Log performance metrics
    
    metrics_to_collect:
      - request_latency
      - throughput
      - queue_length
      - model_load_time
      - memory_usage
      - cpu_usage
      - cache_hit_rate
      - error_rate
      
  # Auto-scaling Configuration
  autoscaling:
    enable: true                     # Enable auto-scaling
    min_instances: 2                 # Minimum Ollama instances
    max_instances: 4                 # Maximum instances
    target_cpu: 70                   # Target CPU utilization
    target_memory: 80                # Target memory utilization
    scale_up_threshold: 80           # Scale up at 80% utilization
    scale_down_threshold: 30         # Scale down at 30% utilization
    cooldown_period: 300s            # Wait 5 minutes between scaling
    
# Agent-Specific Optimizations
agent_optimizations:
  high_throughput_agents:
    - "jarvis-voice-interface"
    - "ai-qa-team-lead"
    - "automated-incident-responder"
    config:
      priority: "high"
      dedicated_instance: true
      cache_responses: true
      preload_model: true
      
  complex_reasoning_agents:
    - "ai-system-architect"
    - "ai-senior-full-stack-developer"
    - "complex-problem-solver"
    config:
      priority: "medium"
      allow_larger_models: true
      extended_timeout: 240s
      higher_context_length: 4096
      
  utility_agents:
    - "garbage-collector"
    - "metrics-collector-prometheus"
    - "log-aggregator-loki"
    config:
      priority: "low"
      shared_instance: true
      minimal_resources: true
      
# Performance Benchmarks
benchmarks:
  target_metrics:
    latency_p95: 2000ms              # 95% of requests under 2 seconds
    latency_p99: 5000ms              # 99% of requests under 5 seconds
    throughput: 500                  # Requests per second
    availability: 99.9               # 99.9% uptime
    error_rate: 0.1                  # Less than 0.1% errors
    
  load_test_scenarios:
    baseline:
      concurrent_users: 50
      duration: 300s
      ramp_up: 60s
      
    normal_load:
      concurrent_users: 100
      duration: 600s
      ramp_up: 120s
      
    peak_load:
      concurrent_users: 200
      duration: 300s
      ramp_up: 180s
      
    stress_test:
      concurrent_users: 400
      duration: 180s
      ramp_up: 60s
      
# Resource Limits and Quotas
resource_management:
  ollama_instances:
    primary:
      cpu_cores: 8
      memory_gb: 16
      max_models: 2
      
    secondary:
      cpu_cores: 6
      memory_gb: 12
      max_models: 1
      
    tertiary:
      cpu_cores: 4
      memory_gb: 8
      max_models: 1
      
  per_agent_limits:
    max_requests_per_minute: 1000
    max_concurrent_requests: 10
    max_response_size_kb: 100
    timeout_seconds: 120
    
# Emergency Controls
emergency_controls:
  circuit_breaker:
    enable: true
    failure_threshold: 20
    timeout: 60s
    
  rate_limiting:
    global_limit: 10000              # Global requests per minute
    per_agent_limit: 500             # Per agent requests per minute
    burst_allowance: 100             # Burst requests allowed
    
  resource_protection:
    max_memory_usage: 90             # Emergency stop at 90% memory
    max_cpu_usage: 95                # Emergency throttle at 95% CPU
    max_queue_length: 1000           # Drop requests if queue too long
    
  fallback_strategies:
    model_fallback: "tinyllama"       # Fallback to TinyLlama
    response_fallback: "I'm experiencing high load. Please try again shortly."
    graceful_degradation: true       # Reduce features under load