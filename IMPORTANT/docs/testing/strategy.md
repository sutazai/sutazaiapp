# SutazAI System Testing Strategy

**Document Version:** 1.0  
**Last Updated:** 2025-08-08  
**Author:** QA Team Lead  
**Target Coverage:** 80% minimum (90% for P0 components)  
**MVP Timeline:** 8 weeks from initiation

## Executive Summary

This document defines a comprehensive testing strategy for the SutazAI system to achieve professional-grade quality assurance. Based on current system analysis, we have **0% automated test coverage** across all components. This strategy provides a phased approach to implement robust testing infrastructure and reach **80% minimum coverage** within the 8-week MVP timeline.

## 1. Testing Overview

### 1.1 Current State Assessment

**Critical Findings from System Analysis:**

- **Zero automated test coverage** across all services
- **7 stub agents** returning hardcoded JSON responses
- **TinyLlama model** loaded (not gpt-oss as documented)
- **28 containers running** but many services not integrated
- **Existing test files** are mostly monitoring/validation scripts, not proper tests

**System Reality Check:**
- **Backend:** FastAPI v17.0.0 with 59 services defined, 28 actually running
- **Frontend:** Streamlit UI with basic functionality
- **Models:** TinyLlama 637MB loaded via Ollama
- **Agents:** 7 Flask stub applications with only health endpoints
- **Databases:** PostgreSQL, Redis, Neo4j running but   schema
- **Monitoring:** Full Prometheus/Grafana stack operational

### 1.2 Testing Philosophy and Principles

**Quality-First Approach:**
1. **Shift-Left Testing** - Integrate testing early in development cycle
2. **Risk-Based Testing** - Prioritize high-risk, high-impact components
3. **Test Pyramid** - Focus on unit tests, supplement with integration and E2E
4. **Continuous Testing** - Automated testing in CI/CD pipeline
5. **Production Parity** - Test environments mirror production closely

**Core Testing Principles:**
- **Fail Fast** - Catch bugs early in the development cycle
- **Comprehensive Coverage** - Test all critical paths and edge cases
- **Performance Validation** - Every test includes performance assertions
- **Security First** - Security testing integrated throughout
- **Maintainable Tests** - Clean, readable, and maintainable test code

### 1.3 Quality Gates and Metrics

**Mandatory Quality Gates:**
- **Unit Test Coverage:** â‰¥80% for all services, â‰¥90% for P0 components
- **Integration Test Coverage:** â‰¥70% for API endpoints
- **Performance Thresholds:** Response times <2s, success rate >95%
- **Security Scan:** Zero high/critical vulnerabilities
- **Code Quality:** SonarQube quality gate must pass

**Key Testing Metrics:**
- **Test Coverage Percentage** (line, branch, function coverage)
- **Test Execution Time** (target: <5 minutes for full suite)
- **Test Success Rate** (target: >99% stability)
- **Defect Escape Rate** (target: <5% defects reaching production)
- **Mean Time to Recovery** (target: <30 minutes for critical issues)

### 1.4 Testing Pyramid Approach

```
        E2E Tests (10%)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   User Journeys     â”‚
    â”‚   Cross-service     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Integration Tests (30%)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    API Contracts            â”‚
    â”‚    Service Integration      â”‚
    â”‚    Database Integration     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Unit Tests (60%)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Component Logic            â”‚
    â”‚        Business Logic             â”‚
    â”‚        Utility Functions          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Test Coverage Analysis

### 2.1 Current Coverage Gaps

**Critical Gaps Identified:**
- **Backend API Endpoints:** 0% coverage across 150+ endpoints
- **Agent Services:** 0% coverage for 7 Flask applications
- **Database Operations:** No integration testing with PostgreSQL/Redis/Neo4j
- **Ollama Integration:** No testing of model inference or communication
- **Frontend Components:** No Streamlit component testing
- **Cross-service Communication:** No integration testing between services
- **Performance Testing:** No load testing or performance validation
- **Security Testing:** No automated security testing

### 2.2 Target Coverage by Component

| Component | Current | Target | Priority | Timeline |
|-----------|---------|---------|----------|----------|
| Backend API | 0% | 85% | P0 | Week 1-2 |
| Agent Services | 0% | 80% | P0 | Week 1-2 |
| Database Layer | 0% | 90% | P0 | Week 1 |
| Ollama Integration | 0% | 85% | P0 | Week 2 |
| Frontend Components | 0% | 70% | P1 | Week 3 |
| Cross-service APIs | 0% | 75% | P1 | Week 3 |
| Monitoring Services | 0% | 60% | P2 | Week 4 |
| Security Components | 0% | 95% | P0 | Week 2 |

### 2.3 Coverage Measurement Tools

**Primary Tools:**
- **Python:** `pytest-cov` with HTML/XML reporting
- **JavaScript:** `Jest` with Istanbul coverage
- **API Testing:** `Newman` (Postman) with coverage tracking
- **Integration:** Custom coverage aggregation across services

**Coverage Types:**
- **Line Coverage:** Percentage of code lines executed
- **Branch Coverage:** Percentage of decision branches taken
- **Function Coverage:** Percentage of functions called
- **Statement Coverage:** Percentage of statements executed

## 3. Unit Testing Strategy

### 3.1 Python/Pytest Framework Setup

**Framework Configuration:**
```python
# pytest.ini (enhanced)
[pytest]
testpaths = tests backend/tests frontend/tests agents/tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*
addopts = 
    --cov=backend
    --cov=agents  
    --cov=frontend
    --cov-report=html:coverage/html
    --cov-report=xml:coverage/coverage.xml
    --cov-report=term-missing
    --cov-fail-under=80
    --strict-markers
    --verbose
    --tb=short
    --durations=10
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    security: Security tests
    slow: Slow running tests
    agent: Agent-specific tests
    ollama: Ollama integration tests
```

**Required Testing Dependencies:**
```txt
pytest>=7.0.0
pytest-cov>=4.0.0
pytest-asyncio>=0.21.0
pytest-mock>=3.10.0
pytest-httpx>=0.21.0
pytest-timeout>=2.1.0
coverage[toml]>=6.5.0
factory-boy>=3.2.0
Faker>=18.0.0
freezegun>=1.2.0
responses>=0.23.0
```

### 3.2 Test Structure and Naming Conventions

**Directory Structure:**
```
tests/
â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ api/            # API endpoint tests
â”‚   â”‚   â”œâ”€â”€ core/           # Core business logic
â”‚   â”‚   â””â”€â”€ services/       # Service layer tests
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ test_base_agent.py
â”‚   â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”‚   â””â”€â”€ test_individual_agents/
â”‚   â””â”€â”€ frontend/
â”œâ”€â”€ integration/             # Integration tests
â”‚   â”œâ”€â”€ test_api_integration.py
â”‚   â”œâ”€â”€ test_database_integration.py
â”‚   â””â”€â”€ test_ollama_integration.py
â”œâ”€â”€ e2e/                    # End-to-end tests
â”œâ”€â”€ performance/            # Performance tests
â”œâ”€â”€ security/               # Security tests
â”œâ”€â”€ fixtures/               # Test data and fixtures
â””â”€â”€ conftest.py            # Global pytest configuration
```

**Naming Conventions:**
- **Test Files:** `test_<module_name>.py`
- **Test Classes:** `Test<ClassName>`
- **Test Methods:** `test_<method_name>_<scenario>`
- **Fixtures:** `<resource_name>_fixture`

### 3.3 Mocking and Fixtures Strategy

**Core Fixtures (conftest.py):**
```python
import pytest
import asyncio
from unittest.mock import Mock, AsyncMock
from httpx import AsyncClient
from backend.app.main import app

@pytest.fixture
async def async_client():
    """Async HTTP client for testing FastAPI endpoints"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client

@pytest.fixture
def mock_ollama_service():
    """Mock Ollama service responses"""
    with responses.RequestsMock() as rsps:
        rsps.add(
            responses.GET,
            "http://ollama:11434/api/tags",
            json={"models": [{"name": "tinyllama"}]},
            status=200
        )
        rsps.add(
            responses.POST,
            "http://ollama:11434/api/generate",
            json={"response": "Mocked response"},
            status=200
        )
        yield rsps

@pytest.fixture
def mock_database():
    """Mock database connections and operations"""
    return Mock()

@pytest.fixture
def sample_agent_config():
    """Sample agent configuration for testing"""
    return {
        "id": "test-agent",
        "name": "Test Agent",
        "type": "task_coordinator",
        "config": {
            "model": "tinyllama",
            "temperature": 0.7
        }
    }
```

### 3.4 Agent Unit Testing Patterns

**Testing Stub Agents:**
```python
# tests/unit/agents/test_base_agent.py
import pytest
from unittest.mock import AsyncMock, Mock
from agents.core.base_agent_v2 import BaseAgentV2

class TestBaseAgentV2:
    """Test suite for BaseAgentV2 agent implementation"""
    
    @pytest.fixture
    def agent(self, sample_agent_config):
        """Create a test agent instance"""
        return BaseAgentV2(config=sample_agent_config)
    
    @pytest.mark.unit
    def test_agent_initialization(self, agent):
        """Test agent initializes with correct configuration"""
        assert agent.id == "test-agent"
        assert agent.name == "Test Agent"
        assert agent.type == "task_coordinator"
        assert agent.config["model"] == "tinyllama"
    
    @pytest.mark.unit
    async def test_health_check_endpoint(self, agent):
        """Test agent health check returns proper status"""
        health = await agent.health_check()
        assert health["status"] == "healthy"
        assert "timestamp" in health
        assert "uptime" in health
    
    @pytest.mark.unit
    async def test_process_task_with_mock_ollama(self, agent, mock_ollama_service):
        """Test task processing with mocked Ollama service"""
        task = {
            "id": "test-task",
            "type": "general",
            "description": "Test task processing"
        }
        
        result = await agent.process_task(task)
        
        assert result["status"] == "completed"
        assert result["task_id"] == "test-task"
        assert "response" in result
        assert "processing_time" in result
    
    @pytest.mark.unit
    async def test_error_handling(self, agent):
        """Test agent error handling with invalid input"""
        with pytest.raises(ValueError, match="Task description required"):
            await agent.process_task({})
    
    @pytest.mark.performance
    async def test_response_time_performance(self, agent, mock_ollama_service):
        """Test agent response time meets performance requirements"""
        import time
        
        start_time = time.time()
        task = {"id": "perf-test", "type": "general", "description": "Performance test"}
        await agent.process_task(task)
        processing_time = time.time() - start_time
        
        assert processing_time < 2.0, f"Agent response too slow: {processing_time}s"
```

### 3.5 FastAPI Endpoint Testing

**API Endpoint Testing Pattern:**
```python
# tests/unit/backend/api/test_chat_endpoints.py
import pytest
from httpx import AsyncClient
from unittest.mock import patch

class TestChatEndpoints:
    """Test suite for chat API endpoints"""
    
    @pytest.mark.unit
    async def test_chat_endpoint_success(self, async_client, mock_ollama_service):
        """Test successful chat interaction"""
        response = await async_client.post("/chat", json={
            "message": "Hello, AI!",
            "model": "tinyllama",
            "temperature": 0.7
        })
        
        assert response.status_code == 200
        data = response.json()
        assert "response" in data
        assert data["model"] == "tinyllama"
        assert "timestamp" in data
    
    @pytest.mark.unit
    async def test_chat_endpoint_validation(self, async_client):
        """Test input validation for chat endpoint"""
        response = await async_client.post("/chat", json={
            "message": "",  # Empty message should fail
            "model": "tinyllama"
        })
        
        assert response.status_code == 422
        assert "validation error" in response.json()["detail"][0]["msg"].lower()
    
    @pytest.mark.unit
    async def test_chat_endpoint_ollama_unavailable(self, async_client):
        """Test chat endpoint when Ollama service is unavailable"""
        with patch('backend.app.main.check_ollama', return_value=False):
            response = await async_client.post("/chat", json={
                "message": "Test message",
                "model": "tinyllama"
            })
            
            assert response.status_code == 200
            data = response.json()
            assert "error" in data or "temporarily unavailable" in data["response"].lower()
    
    @pytest.mark.security
    async def test_chat_xss_protection(self, async_client, mock_ollama_service):
        """Test XSS protection in chat input"""
        malicious_input = "<script>alert('xss')</script>"
        
        response = await async_client.post("/chat", json={
            "message": malicious_input,
            "model": "tinyllama"
        })
        
        assert response.status_code == 200
        # Ensure no script tags in response
        assert "<script>" not in response.text
        assert "alert(" not in response.text
```

## 4. Integration Testing

### 4.1 Service Integration Tests

**Database Integration Testing:**
```python
# tests/integration/test_database_integration.py
import pytest
import asyncpg
import redis
from neo4j import GraphDatabase

class TestDatabaseIntegration:
    """Test database connectivity and operations"""
    
    @pytest.mark.integration
    async def test_postgresql_connection(self):
        """Test PostgreSQL database connectivity"""
        try:
            conn = await asyncpg.connect(
                "postgresql://sutazai:password@sutazai-postgres:5432/sutazai"
            )
            result = await conn.fetchrow("SELECT version()")
            assert result is not None
            await conn.close()
        except Exception as e:
            pytest.fail(f"PostgreSQL connection failed: {e}")
    
    @pytest.mark.integration
    def test_redis_connection(self):
        """Test Redis cache connectivity"""
        try:
            r = redis.Redis(host='sutazai-redis', port=6379, decode_responses=True)
            r.ping()
            r.set('test_key', 'test_value', ex=10)
            assert r.get('test_key') == 'test_value'
            r.delete('test_key')
        except Exception as e:
            pytest.fail(f"Redis connection failed: {e}")
    
    @pytest.mark.integration
    def test_neo4j_connection(self):
        """Test Neo4j graph database connectivity"""
        try:
            driver = GraphDatabase.driver("bolt://sutazai-neo4j:7687")
            with driver.session() as session:
                result = session.run("RETURN 'Neo4j is running' as message")
                record = result.single()
                assert record["message"] == "Neo4j is running"
            driver.close()
        except Exception as e:
            pytest.fail(f"Neo4j connection failed: {e}")
```

**Ollama Integration Testing:**
```python
# tests/integration/test_ollama_integration.py
import pytest
import httpx
from backend.app.main import query_ollama, get_ollama_models

class TestOllamaIntegration:
    """Test Ollama service integration"""
    
    @pytest.mark.integration
    @pytest.mark.ollama
    async def test_ollama_service_health(self):
        """Test Ollama service health and availability"""
        async with httpx.AsyncClient() as client:
            response = await client.get("http://sutazai-ollama:11434/api/tags")
            assert response.status_code == 200
            
            data = response.json()
            assert "models" in data
            assert isinstance(data["models"], list)
    
    @pytest.mark.integration
    @pytest.mark.ollama
    async def test_get_available_models(self):
        """Test retrieving available models from Ollama"""
        models = await get_ollama_models()
        assert isinstance(models, list)
        assert len(models) > 0
        assert "tinyllama" in models or any("llama" in model.lower() for model in models)
    
    @pytest.mark.integration
    @pytest.mark.ollama
    @pytest.mark.slow
    async def test_model_inference(self):
        """Test actual model inference with TinyLlama"""
        response = await query_ollama("tinyllama", "What is 2+2?")
        
        assert isinstance(response, str)
        assert len(response) > 0
        assert response != "Model temporarily unavailable - please ensure Ollama is running with models installed"
    
    @pytest.mark.integration
    @pytest.mark.performance
    async def test_ollama_response_time(self):
        """Test Ollama response time meets performance requirements"""
        import time
        
        start_time = time.time()
        await query_ollama("tinyllama", "Hello")
        response_time = time.time() - start_time
        
        assert response_time < 10.0, f"Ollama response too slow: {response_time}s"
```

### 4.2 API Contract Testing

**Contract Testing with Postman/Newman:**
```json
// postman/sutazai_api_contracts.json
{
    "info": {
        "name": "SutazAI API Contract Tests",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Health Check Contract",
            "request": {
                "method": "GET",
                "url": "{{base_url}}/health"
            },
            "response": [],
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test('Status code is 200', () => {",
                            "    pm.response.to.have.status(200);",
                            "});",
                            "",
                            "pm.test('Response has required fields', () => {",
                            "    const response = pm.response.json();",
                            "    pm.expect(response).to.have.property('status');",
                            "    pm.expect(response).to.have.property('service');",
                            "    pm.expect(response).to.have.property('version');",
                            "    pm.expect(response).to.have.property('timestamp');",
                            "});",
                            "",
                            "pm.test('Service version is correct', () => {",
                            "    const response = pm.response.json();",
                            "    pm.expect(response.version).to.equal('17.0.0');",
                            "});"
                        ]
                    }
                }
            ]
        }
    ]
}
```

### 4.3 Message Queue Testing (RabbitMQ)

**Message Queue Integration:**
```python
# tests/integration/test_message_queue.py
import pytest
import pika
import json
import asyncio

class TestRabbitMQIntegration:
    """Test RabbitMQ message queue integration"""
    
    @pytest.fixture
    def rabbitmq_connection(self):
        """Create RabbitMQ connection for testing"""
        connection = pika.BlockingConnection(
            pika.ConnectionParameters('sutazai-rabbitmq', 5672)
        )
        yield connection
        connection.close()
    
    @pytest.mark.integration
    def test_rabbitmq_connection(self, rabbitmq_connection):
        """Test basic RabbitMQ connectivity"""
        channel = rabbitmq_connection.channel()
        
        # Declare test queue
        queue_name = 'test_queue'
        channel.queue_declare(queue=queue_name, durable=False)
        
        # Test message publishing and consumption
        test_message = {"test": "message", "timestamp": "2025-08-08T10:00:00Z"}
        channel.basic_publish(
            exchange='',
            routing_key=queue_name,
            body=json.dumps(test_message)
        )
        
        # Consume message
        method, properties, body = channel.basic_get(queue=queue_name, auto_ack=True)
        received_message = json.loads(body.decode())
        
        assert received_message == test_message
        
        # Cleanup
        channel.queue_delete(queue=queue_name)
```

## 5. End-to-End Testing

### 5.1 User Journey Testing

**Critical User Journeys:**
1. **New User Chat Session** - User opens app â†’ selects agent â†’ sends message â†’ receives response
2. **Multi-Agent Workflow** - User creates task â†’ system orchestrates agents â†’ provides result
3. **Model Switching** - User switches between different AI models â†’ system adapts responses
4. **System Monitoring** - Admin views metrics â†’ checks agent health â†’ reviews performance
5. **Error Recovery** - Service failure occurs â†’ system degrades gracefully â†’ recovers automatically

**E2E Test Example:**
```python
# tests/e2e/test_user_journeys.py
import pytest
from playwright.async_api import async_playwright

class TestUserJourneys:
    """End-to-end user journey tests"""
    
    @pytest.mark.e2e
    async def test_complete_chat_session(self):
        """Test complete user chat session flow"""
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            
            # Navigate to application
            await page.goto("http://sutazai-frontend:10011")
            
            # Wait for page load
            await page.wait_for_selector("text=SutazAI")
            
            # Select agent
            await page.click("text=Task Coordinator")
            
            # Send message
            await page.fill("[data-testid=message-input]", "What is artificial intelligence?")
            await page.click("[data-testid=send-button]")
            
            # Wait for response
            await page.wait_for_selector("[data-testid=response-message]", timeout=30000)
            
            # Verify response exists and is meaningful
            response_text = await page.inner_text("[data-testid=response-message]")
            assert len(response_text) > 50
            assert "artificial intelligence" in response_text.lower() or "ai" in response_text.lower()
            
            await browser.close()
    
    @pytest.mark.e2e
    @pytest.mark.slow
    async def test_system_health_monitoring_flow(self):
        """Test system health monitoring and dashboard access"""
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            
            # Navigate to monitoring dashboard
            await page.goto("http://localhost:10201")  # Grafana
            
            # Login (if required)
            if await page.query_selector("text=Sign in"):
                await page.fill("[name=user]", "admin")
                await page.fill("[name=password]", "admin") 
                await page.click("button[type=submit]")
            
            # Verify dashboard loads
            await page.wait_for_selector("text=SutazAI", timeout=10000)
            
            # Check key metrics are displayed
            await page.wait_for_selector("[data-testid=cpu-metric]")
            await page.wait_for_selector("[data-testid=memory-metric]")
            
            await browser.close()
```

### 5.2 Cross-Service Workflow Testing

**Multi-Service Integration:**
```python
# tests/e2e/test_cross_service_workflows.py
import pytest
import httpx
import asyncio

class TestCrossServiceWorkflows:
    """Test workflows that span multiple services"""
    
    @pytest.mark.e2e
    async def test_orchestrated_multi_agent_task(self):
        """Test task execution across multiple agents"""
        async with httpx.AsyncClient() as client:
            # Step 1: Submit complex task
            task_request = {
                "description": "Analyze the current system architecture and provide recommendations",
                "type": "multi_agent",
                "agents": ["research-agent", "task_coordinator"]
            }
            
            response = await client.post(
                "http://sutazai-backend:10010/execute",
                json=task_request,
                timeout=30.0
            )
            
            assert response.status_code == 200
            result = response.json()
            
            # Step 2: Verify orchestration occurred
            assert result["status"] == "completed"
            assert "task_id" in result
            assert "orchestrated" in result
            
            # Step 3: Check task result quality
            assert len(result["result"]) > 100
            assert "architecture" in result["result"].lower()
            
            # Step 4: Verify performance metrics
            assert "execution_time" in result
            # Parse execution time (e.g., "3.4s")
            exec_time = float(result["execution_time"].replace('s', ''))
            assert exec_time < 15.0  # Should complete within 15 seconds
```

## 6. Performance Testing

### 6.1 Load Testing with K6

**Performance Testing Strategy:**
```javascript
// tests/performance/load_test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// Custom metrics
const failureRate = new Rate('failures');
const responseTimeThreshold = new Trend('response_time_threshold');

export let options = {
    // Load testing scenarios
    scenarios: {
        // Baseline load test
        baseline_load: {
            executor: 'constant-vus',
            vus: 10,
            duration: '5m',
        },
        // Stress test
        stress_test: {
            executor: 'ramping-vus',
            startVUs: 0,
            stages: [
                { duration: '2m', target: 20 },
                { duration: '5m', target: 20 },
                { duration: '2m', target: 40 },
                { duration: '5m', target: 40 },
                { duration: '2m', target: 0 },
            ],
        },
    },
    thresholds: {
        http_req_duration: ['p(95)<2000'], // 95% of requests under 2s
        http_req_failed: ['rate<0.05'], // Error rate under 5%
        failures: ['rate<0.05'],
    },
};

const BASE_URL = 'http://sutazai-backend:10010';

export default function () {
    // Test chat endpoint performance
    const chatPayload = {
        message: 'What is the current system status?',
        model: 'tinyllama',
        temperature: 0.7
    };
    
    const chatResponse = http.post(`${BASE_URL}/chat`, JSON.stringify(chatPayload), {
        headers: { 'Content-Type': 'application/json' },
    });
    
    const chatSuccess = check(chatResponse, {
        'chat status is 200': (r) => r.status === 200,
        'chat response time < 2s': (r) => r.timings.duration < 2000,
        'chat has response field': (r) => JSON.parse(r.body).response !== undefined,
    });
    
    failureRate.add(!chatSuccess);
    responseTimeThreshold.add(chatResponse.timings.duration);
    
    // Test health endpoint
    const healthResponse = http.get(`${BASE_URL}/health`);
    check(healthResponse, {
        'health status is 200': (r) => r.status === 200,
        'health response time < 500ms': (r) => r.timings.duration < 500,
    });
    
    // Test metrics endpoint
    const metricsResponse = http.get(`${BASE_URL}/public/metrics`);
    check(metricsResponse, {
        'metrics status is 200': (r) => r.status === 200,
        'metrics response time < 1s': (r) => r.timings.duration < 1000,
    });
    
    sleep(1);
}
```

### 6.2 Stress Testing Scenarios

**System Resource Limits Testing:**
```python
# tests/performance/test_resource_limits.py
import pytest
import asyncio
import httpx
import psutil
import time

class TestResourceLimits:
    """Test system behavior under resource constraints"""
    
    @pytest.mark.performance
    @pytest.mark.slow
    async def test_concurrent_requests_limit(self):
        """Test system behavior with high concurrent request load"""
        
        async def make_request(session, request_id):
            try:
                response = await session.post(
                    "http://sutazai-backend:10010/chat",
                    json={"message": f"Request {request_id}", "model": "tinyllama"},
                    timeout=30.0
                )
                return response.status_code == 200
            except Exception:
                return False
        
        # Test with increasing concurrent loads
        concurrent_loads = [10, 25, 50, 75, 100]
        success_rates = []
        
        for load in concurrent_loads:
            async with httpx.AsyncClient() as client:
                tasks = [make_request(client, i) for i in range(load)]
                start_time = time.time()
                results = await asyncio.gather(*tasks, return_exceptions=True)
                duration = time.time() - start_time
                
                successful_requests = sum(1 for r in results if r is True)
                success_rate = successful_requests / load
                success_rates.append(success_rate)
                
                # Log system resources during test
                cpu_percent = psutil.cpu_percent()
                memory_percent = psutil.virtual_memory().percent
                
                print(f"Load: {load}, Success Rate: {success_rate:.2%}, "
                      f"Duration: {duration:.2f}s, CPU: {cpu_percent}%, Memory: {memory_percent}%")
                
                # System should maintain >90% success rate up to 50 concurrent requests
                if load <= 50:
                    assert success_rate >= 0.90, f"Success rate too low at load {load}: {success_rate:.2%}"
        
        # Verify graceful degradation under extreme load
        assert all(rate >= 0.50 for rate in success_rates), "System failed catastrophically under load"
```

### 6.3 Latency and Throughput Targets

**Performance Requirements:**
- **API Response Time:** <2s for 95th percentile
- **Chat Response Time:** <5s for simple queries, <15s for complex reasoning
- **Health Check Response:** <500ms
- **Throughput:** >100 requests/minute sustained
- **Concurrent Users:** Support 50+ concurrent chat sessions
- **Memory Usage:** <4GB total system memory
- **CPU Usage:** <80% average under normal load

## 7. Security Testing

### 7.1 OWASP Top 10 Coverage

**Automated Security Testing:**
```python
# tests/security/test_owasp_top10.py
import pytest
import httpx
from urllib.parse import quote

class TestOWASPTop10:
    """Test coverage for OWASP Top 10 security vulnerabilities"""
    
    @pytest.mark.security
    async def test_injection_protection(self):
        """Test SQL injection protection"""
        injection_payloads = [
            "'; DROP TABLE users; --",
            "1' OR '1'='1",
            "<script>alert('xss')</script>",
            "{{7*7}}",  # Template injection
            "${7*7}",   # Expression injection
        ]
        
        async with httpx.AsyncClient() as client:
            for payload in injection_payloads:
                response = await client.post(
                    "http://sutazai-backend:10010/chat",
                    json={"message": payload, "model": "tinyllama"}
                )
                
                # Should not return 500 (server error) or contain payload directly
                assert response.status_code != 500
                response_text = response.text.lower()
                assert "drop table" not in response_text
                assert "<script>" not in response_text
                assert payload not in response.text
    
    @pytest.mark.security
    async def test_authentication_bypass(self):
        """Test authentication bypass attempts"""
        protected_endpoints = [
            "/api/v1/system/status",
            "/api/v1/orchestration/status",
            "/api/v1/improvement/analyze"
        ]
        
        async with httpx.AsyncClient() as client:
            for endpoint in protected_endpoints:
                # Test without authentication
                response = await client.get(f"http://sutazai-backend:10010{endpoint}")
                
                # Should either require auth or gracefully handle anonymous access
                assert response.status_code in [200, 401, 403]
                if response.status_code == 200:
                    # If accessible, should not leak sensitive information
                    assert "password" not in response.text.lower()
                    assert "secret" not in response.text.lower()
    
    @pytest.mark.security
    async def test_csrf_protection(self):
        """Test CSRF protection mechanisms"""
        async with httpx.AsyncClient() as client:
            # Test state-changing operations
            response = await client.post(
                "http://sutazai-backend:10010/execute",
                json={"description": "Test task", "type": "general"},
                headers={"Origin": "http://malicious-site.com"}
            )
            
            # Should handle cross-origin requests appropriately
            assert response.status_code in [200, 400, 403, 429]
    
    @pytest.mark.security
    async def test_rate_limiting(self):
        """Test rate limiting protection"""
        async with httpx.AsyncClient() as client:
            # Make rapid requests to trigger rate limiting
            responses = []
            for i in range(50):
                response = await client.get("http://sutazai-backend:10010/health")
                responses.append(response.status_code)
            
            # Should eventually start rate limiting (status 429)
            # Note: May not trigger with current implementation
            status_codes = set(responses)
            assert all(code in [200, 429, 503] for code in status_codes)
```

### 7.2 Input Validation Testing

**Input Sanitization and Validation:**
```python
# tests/security/test_input_validation.py
import pytest
import httpx

class TestInputValidation:
    """Test input validation and sanitization"""
    
    @pytest.mark.security
    async def test_message_length_limits(self):
        """Test message length validation"""
        async with httpx.AsyncClient() as client:
            # Test extremely long message
            long_message = "A" * 50000
            response = await client.post(
                "http://sutazai-backend:10010/chat",
                json={"message": long_message, "model": "tinyllama"}
            )
            
            # Should handle gracefully (either reject or truncate)
            assert response.status_code in [200, 400, 413]
            
            if response.status_code == 200:
                # If accepted, should not cause system instability
                result = response.json()
                assert "response" in result
    
    @pytest.mark.security
    async def test_special_character_handling(self):
        """Test handling of special characters and encodings"""
        special_inputs = [
            "Hello ðŸ¤– AI!",  # Emoji
            "æµ‹è¯•ä¸­æ–‡",  # Chinese characters
            "Ð¢ÐµÑÑ‚ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼",  # Cyrillic
            "Ù…Ø±Ø­Ø¨Ø§",  # Arabic
            "\x00\x01\x02",  # Null bytes and control characters
            "../../etc/passwd",  # Path traversal
            "%00%2e%2e%2f",  # Encoded path traversal
        ]
        
        async with httpx.AsyncClient() as client:
            for input_text in special_inputs:
                response = await client.post(
                    "http://sutazai-backend:10010/chat",
                    json={"message": input_text, "model": "tinyllama"}
                )
                
                # Should handle all inputs gracefully
                assert response.status_code in [200, 400]
                
                if response.status_code == 200:
                    result = response.json()
                    assert "response" in result
                    # Should not contain null bytes or control characters in response
                    assert "\x00" not in str(result)
```

### 7.3 Vulnerability Scanning

**Automated Security Scanning:**
```bash
#!/bin/bash
# tests/security/security_scan.sh

echo "Running comprehensive security scans..."

# Trivy container scanning
echo "Scanning containers for vulnerabilities..."
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  aquasec/trivy image sutazaiapp-backend:latest

# OWASP ZAP API scanning
echo "Running OWASP ZAP API scan..."
docker run -t owasp/zap2docker-stable zap-api-scan.py \
  -t http://sutazai-backend:10010/openapi.json \
  -f openapi

# Bandit static analysis for Python
echo "Running Bandit security analysis..."
pip install bandit
bandit -r backend/ -f json -o security_report.json

# Safety check for Python dependencies
echo "Checking Python dependencies for known vulnerabilities..."
pip install safety
safety check --json > safety_report.json

echo "Security scans completed. Review reports in security_reports/"
```

## 8. Chaos Engineering

### 8.1 Failure Injection Testing

**Service Failure Scenarios:**
```python
# tests/chaos/test_failure_scenarios.py
import pytest
import docker
import asyncio
import httpx
import time

class TestChaosScenarios:
    """Chaos engineering tests for system resilience"""
    
    @pytest.fixture
    def docker_client(self):
        """Docker client for container manipulation"""
        return docker.from_env()
    
    @pytest.mark.chaos
    async def test_ollama_service_failure_recovery(self, docker_client):
        """Test system behavior when Ollama service fails"""
        # Get Ollama container
        try:
            ollama_container = docker_client.containers.get("sutazai-ollama")
        except docker.errors.NotFound:
            pytest.skip("Ollama container not found")
        
        async with httpx.AsyncClient() as client:
            # Verify system works normally
            response = await client.post(
                "http://sutazai-backend:10010/chat",
                json={"message": "Hello", "model": "tinyllama"}
            )
            assert response.status_code == 200
            
            # Stop Ollama service
            ollama_container.stop()
            
            # Wait for failure detection
            await asyncio.sleep(5)
            
            # Test graceful degradation
            response = await client.post(
                "http://sutazai-backend:10010/chat",
                json={"message": "Hello", "model": "tinyllama"}
            )
            
            # System should handle failure gracefully
            assert response.status_code == 200
            result = response.json()
            assert "temporarily unavailable" in result.get("response", "").lower() or \
                   "error" in result
            
            # Restart Ollama service
            ollama_container.start()
            
            # Wait for recovery
            await asyncio.sleep(30)
            
            # Verify system recovers
            response = await client.post(
                "http://sutazai-backend:10010/chat",
                json={"message": "Hello", "model": "tinyllama"}
            )
            assert response.status_code == 200
            result = response.json()
            assert "temporarily unavailable" not in result.get("response", "").lower()
    
    @pytest.mark.chaos
    async def test_database_connection_loss(self):
        """Test system behavior during database connection issues"""
        # This would require more complex setup to safely test
        # For now, we'll test the error handling code paths
        
        async with httpx.AsyncClient() as client:
            # Test health endpoint which checks database
            response = await client.get("http://sutazai-backend:10010/health")
            
            # Should always respond with some status
            assert response.status_code == 200
            result = response.json()
            assert "status" in result
            assert result["status"] in ["healthy", "degraded", "unhealthy"]
    
    @pytest.mark.chaos
    async def test_network_partition_simulation(self):
        """Test system behavior during network issues"""
        # Test with various timeout scenarios
        timeout_values = [0.1, 0.5, 1.0, 2.0, 5.0]
        
        for timeout in timeout_values:
            async with httpx.AsyncClient(timeout=timeout) as client:
                try:
                    response = await client.get("http://sutazai-backend:10010/health")
                    
                    if response.status_code == 200:
                        # Verify response time is reasonable
                        assert response.elapsed.total_seconds() <= timeout + 1
                except httpx.TimeoutException:
                    # Timeout is acceptable for very short timeouts
                    assert timeout < 2.0, f"Request timed out with {timeout}s timeout"
```

### 8.2 Network Partition Scenarios

**Network Resilience Testing:**
```python
# tests/chaos/test_network_partitions.py
import pytest
import subprocess
import asyncio
import httpx

class TestNetworkPartitions:
    """Test network partition resilience"""
    
    def _add_network_delay(self, delay_ms=1000):
        """Add network delay using tc (traffic control)"""
        try:
            subprocess.run([
                "sudo", "tc", "qdisc", "add", "dev", "eth0", 
                "root", "netem", "delay", f"{delay_ms}ms"
            ], check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            pytest.skip("Network traffic control not available")
    
    def _remove_network_delay(self):
        """Remove network delay"""
        try:
            subprocess.run([
                "sudo", "tc", "qdisc", "del", "dev", "eth0", "root"
            ])
        except subprocess.CalledProcessError:
            pass  # May not exist
    
    @pytest.mark.chaos
    @pytest.mark.skipif(os.geteuid() != 0, reason="requires root privileges")
    async def test_high_latency_resilience(self):
        """Test system resilience under high network latency"""
        
        try:
            # Add 2 second delay
            self._add_network_delay(2000)
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                # Test that system still functions under high latency
                start_time = time.time()
                response = await client.get("http://sutazai-backend:10010/health")
                response_time = time.time() - start_time
                
                assert response.status_code == 200
                assert response_time >= 2.0  # Should reflect added delay
                assert response_time < 8.0   # But not timeout
                
        finally:
            self._remove_network_delay()
```

## 9. Test Data Management

### 9.1 Test Data Generation

**Factory-based Test Data:**
```python
# tests/fixtures/factories.py
import factory
from factory import Faker, SubFactory
from datetime import datetime, timezone

class AgentConfigFactory(factory.Factory):
    """Factory for generating agent configurations"""
    class Meta:
        model = dict
    
    id = factory.Sequence(lambda n: f"agent-{n}")
    name = Faker('word')
    type = factory.Iterator([
        'task_coordinator', 'research-agent', 'code-agent',
        'autogpt', 'crewai', 'aider', 'gpt-engineer'
    ])
    model = factory.Iterator(['tinyllama', 'tinyllama3:8b', 'tinyllama:7b'])
    temperature = factory.Faker('pyfloat', min_value=0.1, max_value=1.0)
    max_tokens = factory.Iterator([256, 512, 1024, 2048])

class TaskFactory(factory.Factory):
    """Factory for generating test tasks"""
    class Meta:
        model = dict
    
    id = factory.Sequence(lambda n: f"task-{n}")
    description = Faker('sentence')
    type = factory.Iterator(['general', 'complex', 'multi_agent', 'workflow', 'coding', 'analysis'])
    priority = factory.Iterator(['low', 'medium', 'high', 'critical'])
    created_at = factory.LazyFunction(lambda: datetime.now(timezone.utc).isoformat())

class ChatMessageFactory(factory.Factory):
    """Factory for generating chat messages"""
    class Meta:
        model = dict
    
    message = Faker('paragraph')
    model = factory.Iterator(['tinyllama', 'tinyllama3:8b'])
    agent = factory.Iterator([
        'task_coordinator', 'research-agent', 'code-agent', 'autogpt'
    ])
    temperature = factory.Faker('pyfloat', min_value=0.3, max_value=0.9)

# Usage in tests:
# agent_config = AgentConfigFactory()
# task = TaskFactory(type='complex', priority='high')
# message = ChatMessageFactory(agent='task_coordinator')
```

### 9.2 Database Seeding

**Test Database Setup:**
```python
# tests/fixtures/database_fixtures.py
import pytest
import asyncio
import asyncpg
import redis
from neo4j import GraphDatabase

@pytest.fixture(scope="session")
async def test_database():
    """Setup test database with sample data"""
    
    # PostgreSQL setup
    conn = await asyncpg.connect(
        "postgresql://sutazai:password@sutazai-postgres:5432/sutazai"
    )
    
    # Create test tables if they don't exist
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS test_agents (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            type TEXT NOT NULL,
            config JSONB,
            created_at TIMESTAMP DEFAULT NOW()
        )
    """)
    
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS test_tasks (
            id TEXT PRIMARY KEY,
            description TEXT NOT NULL,
            type TEXT NOT NULL,
            status TEXT DEFAULT 'pending',
            agent_id TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        )
    """)
    
    # Insert sample test data
    sample_agents = [
        ('agent-1', 'Test Coordinator', 'task_coordinator', '{"model": "tinyllama"}'),
        ('agent-2', 'Test Researcher', 'research-agent', '{"model": "tinyllama"}'),
    ]
    
    for agent_data in sample_agents:
        await conn.execute(
            "INSERT INTO test_agents (id, name, type, config) VALUES ($1, $2, $3, $4) ON CONFLICT (id) DO NOTHING",
            *agent_data
        )
    
    yield conn
    
    # Cleanup
    await conn.execute("DROP TABLE IF EXISTS test_agents")
    await conn.execute("DROP TABLE IF EXISTS test_tasks")
    await conn.close()

@pytest.fixture
def clean_redis():
    """Clean Redis database for testing"""
    r = redis.Redis(host='sutazai-redis', port=6379, decode_responses=True)
    
    # Store existing keys for restoration
    existing_keys = r.keys("test_*")
    
    yield r
    
    # Cleanup test keys
    test_keys = r.keys("test_*")
    if test_keys:
        r.delete(*test_keys)
```

### 9.3 Test Environment Isolation

**Test Environment Configuration:**
```python
# tests/fixtures/environment.py
import os
import pytest
from unittest.mock import patch

@pytest.fixture(autouse=True)
def test_environment():
    """Set up test environment variables"""
    test_env = {
        'TESTING': 'true',
        'LOG_LEVEL': 'WARNING',
        'SUTAZAI_ENTERPRISE_FEATURES': '1',
        'SUTAZAI_ENABLE_KNOWLEDGE_GRAPH': '1',
        'SUTAZAI_ENABLE_COGNITIVE': '1',
        'DATABASE_URL': 'postgresql://sutazai:password@sutazai-postgres:5432/sutazai_test',
        'REDIS_URL': 'redis://sutazai-redis:6379/1',  # Use different DB for tests
        'OLLAMA_URL': 'http://sutazai-ollama:11434',
    }
    
    with patch.dict(os.environ, test_env):
        yield test_env

@pytest.fixture
def isolated_file_system(tmp_path):
    """Provide isolated file system for tests"""
    original_cwd = os.getcwd()
    os.chdir(tmp_path)
    
    yield tmp_path
    
    os.chdir(original_cwd)
```

## 10. Test Automation

### 10.1 CI/CD Pipeline Integration

**GitHub Actions Workflow:**
```yaml
# .github/workflows/test_pipeline.yml
name: SutazAI Test Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=backend --cov=agents \
          --cov-report=xml --cov-report=html \
          --junitxml=test-results/unit-tests.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: coverage.xml
        flags: unit-tests
        name: unit-test-coverage
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: test-results/

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: password
          POSTGRES_USER: sutazai
          POSTGRES_DB: sutazai_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Compose
      run: |
        docker-compose -f docker-compose.test.yml up -d
        sleep 30  # Wait for services to start
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --junitxml=test-results/integration-tests.xml
    
    - name: Run API contract tests
      run: |
        newman run postman/sutazai_api_contracts.json \
          --environment postman/test_environment.json \
          --reporters junit,cli \
          --reporter-junit-export test-results/api-contract-tests.xml
    
    - name: Clean up
      if: always()
      run: docker-compose -f docker-compose.test.yml down

  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up full system
      run: |
        docker-compose up -d
        sleep 60  # Wait for all services
    
    - name: Install Playwright
      run: |
        npm install -g @playwright/test
        playwright install
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -v --junitxml=test-results/e2e-tests.xml
    
    - name: Upload E2E artifacts
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: e2e-artifacts
        path: |
          test-results/
          screenshots/
          videos/

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up system
      run: docker-compose up -d
    
    - name: Install K6
      run: |
        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
    
    - name: Run performance tests
      run: |
        k6 run tests/performance/load_test.js \
          --out json=test-results/performance-results.json
    
    - name: Performance regression check
      run: |
        python tests/performance/check_regression.py \
          test-results/performance-results.json

  security-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security tests
      run: |
        pytest tests/security/ -v --junitxml=test-results/security-tests.xml
    
    - name: Run Trivy container scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'sutazaiapp-backend:latest'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  test-report:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate comprehensive test report
      run: |
        python scripts/generate_test_report.py \
          --output test-summary.html \
          --artifacts-dir .
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: test-summary.html
```

### 10.2 Pre-commit Hooks

**Pre-commit Configuration:**
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      - id: debug-statements
  
  - repo: https://github.com/psf/black
    rev: 22.10.0
    hooks:
      - id: black
        language_version: python3
  
  - repo: https://github.com/pycqa/flake8
    rev: 5.0.4
    hooks:
      - id: flake8
        additional_dependencies: [flake8-docstrings]
  
  - repo: https://github.com/pycqa/isort
    rev: 5.11.4
    hooks:
      - id: isort
        args: ["--profile", "black"]
  
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v0.991
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
        args: [--ignore-missing-imports]
  
  - repo: local
    hooks:
      - id: pytest-unit
        name: pytest-unit
        entry: pytest tests/unit/ -x --tb=short
        language: system
        pass_filenames: false
        always_run: true
      
      - id: security-check
        name: security-check
        entry: bandit -r backend/ agents/ -f json
        language: system
        pass_filenames: false
        always_run: true
```

### 10.3 Automated Test Reporting

**Test Results Dashboard:**
```python
# scripts/generate_test_report.py
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import argparse
from datetime import datetime

class TestReportGenerator:
    """Generate comprehensive test reports"""
    
    def __init__(self, artifacts_dir: str, output_file: str):
        self.artifacts_dir = Path(artifacts_dir)
        self.output_file = output_file
        self.report_data = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'unit_tests': {},
            'integration_tests': {},
            'e2e_tests': {},
            'performance_tests': {},
            'security_tests': {},
            'coverage': {}
        }
    
    def parse_junit_xml(self, file_path: Path):
        """Parse JUnit XML test results"""
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            return {
                'tests': int(root.get('tests', 0)),
                'failures': int(root.get('failures', 0)),
                'errors': int(root.get('errors', 0)),
                'skipped': int(root.get('skipped', 0)),
                'time': float(root.get('time', 0)),
                'success_rate': self.calculate_success_rate(
                    int(root.get('tests', 0)),
                    int(root.get('failures', 0)) + int(root.get('errors', 0))
                )
            }
        except Exception as e:
            return {'error': str(e)}
    
    def calculate_success_rate(self, total: int, failed: int) -> float:
        """Calculate test success rate"""
        if total == 0:
            return 0.0
        return ((total - failed) / total) * 100
    
    def parse_coverage_xml(self, file_path: Path):
        """Parse coverage XML report"""
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            coverage_elem = root.find('.//coverage')
            if coverage_elem is not None:
                return {
                    'line_rate': float(coverage_elem.get('line-rate', 0)) * 100,
                    'branch_rate': float(coverage_elem.get('branch-rate', 0)) * 100,
                    'lines_covered': int(coverage_elem.get('lines-covered', 0)),
                    'lines_valid': int(coverage_elem.get('lines-valid', 0)),
                }
            return {}
        except Exception as e:
            return {'error': str(e)}
    
    def generate_html_report(self):
        """Generate HTML test report"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>SutazAI Test Report - {self.report_data['timestamp']}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; margin-bottom: 20px; }}
                .section {{ margin-bottom: 30px; }}
                .metrics {{ display: flex; gap: 20px; margin: 20px 0; }}
                .metric {{ background: #e7f3ff; padding: 15px; border-radius: 5px; text-align: center; }}
                .success {{ background-color: #d4edda; color: #155724; }}
                .warning {{ background-color: #fff3cd; color: #856404; }}
                .danger {{ background-color: #f8d7da; color: #721c24; }}
                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>SutazAI Comprehensive Test Report</h1>
                <p>Generated: {self.report_data['timestamp']}</p>
                <p>Build: {self.get_build_info()}</p>
            </div>
            
            <div class="section">
                <h2>Test Summary</h2>
                <div class="metrics">
                    {self.generate_summary_metrics()}
                </div>
            </div>
            
            <div class="section">
                <h2>Test Coverage</h2>
                {self.generate_coverage_section()}
            </div>
            
            <div class="section">
                <h2>Test Results by Category</h2>
                {self.generate_test_categories()}
            </div>
            
            <div class="section">
                <h2>Performance Metrics</h2>
                {self.generate_performance_section()}
            </div>
            
            <div class="section">
                <h2>Security Test Results</h2>
                {self.generate_security_section()}
            </div>
        </body>
        </html>
        """
        
        with open(self.output_file, 'w') as f:
            f.write(html_content)
    
    def collect_test_results(self):
        """Collect all test results from artifacts"""
        # Look for JUnit XML files
        for xml_file in self.artifacts_dir.rglob("*.xml"):
            if "unit-tests" in xml_file.name:
                self.report_data['unit_tests'] = self.parse_junit_xml(xml_file)
            elif "integration-tests" in xml_file.name:
                self.report_data['integration_tests'] = self.parse_junit_xml(xml_file)
            elif "e2e-tests" in xml_file.name:
                self.report_data['e2e_tests'] = self.parse_junit_xml(xml_file)
            elif "security-tests" in xml_file.name:
                self.report_data['security_tests'] = self.parse_junit_xml(xml_file)
            elif "coverage" in xml_file.name:
                self.report_data['coverage'] = self.parse_coverage_xml(xml_file)
        
        # Look for performance results
        for json_file in self.artifacts_dir.rglob("performance-results.json"):
            try:
                with open(json_file) as f:
                    perf_data = json.load(f)
                    self.report_data['performance_tests'] = perf_data
            except Exception as e:
                self.report_data['performance_tests'] = {'error': str(e)}

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate comprehensive test report')
    parser.add_argument('--artifacts-dir', required=True, help='Directory containing test artifacts')
    parser.add_argument('--output', required=True, help='Output HTML file')
    
    args = parser.parse_args()
    
    generator = TestReportGenerator(args.artifacts_dir, args.output)
    generator.collect_test_results()
    generator.generate_html_report()
    
    print(f"Test report generated: {args.output}")
```

## 11. Testing Tools & Infrastructure

### 11.1 Testing Framework Setup

**Core Testing Dependencies:**
```txt
# requirements-test.txt
# Core testing framework
pytest==7.4.0
pytest-asyncio==0.21.1
pytest-cov==4.1.0
pytest-mock==3.11.1
pytest-xdist==3.3.1
pytest-timeout==2.1.0
pytest-html==3.2.0
pytest-json-report==1.5.0

# HTTP testing
httpx==0.24.1
responses==0.23.3
pytest-httpx==0.21.3

# Database testing
asyncpg==0.28.0
redis==4.6.0
neo4j==5.11.0
pytest-postgresql==5.0.0

# API testing
newman==5.3.2
postman==1.0.0

# Performance testing
locust==2.16.1
pytest-benchmark==4.0.0

# Security testing  
bandit==1.7.5
safety==2.3.5
semgrep==1.35.0

# Test data generation
factory-boy==3.3.0
faker==19.3.0
freezegun==1.2.2

# Browser automation
playwright==1.37.0
selenium==4.11.2

# Load testing
k6==0.45.1

# Mocking and fixtures
responses==0.23.3
pytest-mock==3.11.1
pytest-freezegun==0.4.2

# Reporting and analysis
coverage[toml]==7.2.7
pytest-html==3.2.0
allure-pytest==2.13.2
```

**Docker Test Environment:**
```yaml
# docker-compose.test.yml
version: '3.8'

services:
  # Test database services
  test-postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: sutazai_test
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: password
    ports:
      - "15432:5432"
    volumes:
      - test_postgres_data:/var/lib/postgresql/data

  test-redis:
    image: redis:7-alpine
    ports:
      - "16379:6379"
    command: redis-server --appendonly yes
    volumes:
      - test_redis_data:/data

  test-neo4j:
    image: neo4j:5.11
    environment:
      NEO4J_AUTH: neo4j/password
      NEO4J_dbms_memory_heap_initial__size: 512m
      NEO4J_dbms_memory_heap_max__size: 512m
    ports:
      - "17687:7687"
      - "17474:7474"
    volumes:
      - test_neo4j_data:/data

  # Test Ollama service (lightweight)
  test-ollama:
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - test_ollama_data:/root/.ollama
    environment:
      - OLLAMA_MODELS=tinyllama

  # Test runner service
  test-runner:
    build: 
      context: .
      dockerfile: docker/Dockerfile.test
    depends_on:
      - test-postgres
      - test-redis
      - test-neo4j
      - test-ollama
    environment:
      - TESTING=true
      - DATABASE_URL=postgresql://sutazai:password@test-postgres:5432/sutazai_test
      - REDIS_URL=redis://test-redis:6379/0
      - NEO4J_URL=bolt://test-neo4j:7687
      - OLLAMA_URL=http://test-ollama:11434
    volumes:
      - .:/app
      - test_results:/app/test-results
    command: ["pytest", "--cov=backend", "--cov=agents", "--junitxml=test-results/results.xml"]

volumes:
  test_postgres_data:
  test_redis_data:
  test_neo4j_data:
  test_ollama_data:
  test_results:
```

### 11.2 Continuous Testing Pipeline

**Test Automation Script:**
```bash
#!/bin/bash
# scripts/run_comprehensive_tests.sh

set -e

echo "ðŸš€ Starting SutazAI Comprehensive Test Suite"
echo "============================================"

# Configuration
export TESTING=true
export LOG_LEVEL=WARNING
export PYTEST_CURRENT_TEST=true

# Create test results directory
mkdir -p test-results/{unit,integration,e2e,performance,security}
mkdir -p coverage/{html,xml}

# Function to run tests with proper error handling
run_test_suite() {
    local suite_name=$1
    local test_path=$2
    local extra_args=${3:-}
    
    echo "ðŸ“‹ Running $suite_name tests..."
    
    if pytest "$test_path" \
        --junitxml="test-results/${suite_name}/results.xml" \
        --html="test-results/${suite_name}/report.html" \
        --self-contained-html \
        $extra_args; then
        echo "âœ… $suite_name tests passed"
        return 0
    else
        echo "âŒ $suite_name tests failed"
        return 1
    fi
}

# Function to check coverage thresholds
check_coverage() {
    echo "ðŸ“Š Checking code coverage..."
    
    if coverage report --fail-under=80; then
        echo "âœ… Coverage threshold met"
        coverage html -d coverage/html
        coverage xml -o coverage/coverage.xml
        return 0
    else
        echo "âŒ Coverage threshold not met"
        return 1
    fi
}

# Start test services
echo "ðŸ”§ Starting test environment..."
docker-compose -f docker-compose.test.yml up -d
sleep 30  # Wait for services to initialize

# Initialize test database
echo "ðŸ—„ï¸  Initializing test database..."
python scripts/init_test_db.py

# Test execution
FAILED_SUITES=()

# Unit Tests
if ! run_test_suite "unit" "tests/unit/" "--cov=backend --cov=agents --cov-append"; then
    FAILED_SUITES+=("unit")
fi

# Integration Tests
if ! run_test_suite "integration" "tests/integration/" "--cov-append"; then
    FAILED_SUITES+=("integration")
fi

# Security Tests
if ! run_test_suite "security" "tests/security/"; then
    FAILED_SUITES+=("security")
fi

# Performance Tests
echo "âš¡ Running performance tests..."
k6 run tests/performance/load_test.js \
    --out json=test-results/performance/k6-results.json \
    --summary-export=test-results/performance/summary.json

if ! run_test_suite "performance" "tests/performance/" "--tb=short"; then
    FAILED_SUITES+=("performance")
fi

# E2E Tests (if environment supports it)
if command -v playwright &> /dev/null; then
    if ! run_test_suite "e2e" "tests/e2e/" "--tb=short --timeout=300"; then
        FAILED_SUITES+=("e2e")
    fi
else
    echo "âš ï¸  Skipping E2E tests (Playwright not available)"
fi

# Coverage check
if ! check_coverage; then
    FAILED_SUITES+=("coverage")
fi

# API Contract Tests
echo "ðŸ“‘ Running API contract tests..."
if command -v newman &> /dev/null; then
    newman run postman/sutazai_api_contracts.json \
        --environment postman/test_environment.json \
        --reporters junit,cli \
        --reporter-junit-export test-results/api-contracts.xml
else
    echo "âš ï¸  Skipping API contract tests (Newman not available)"
fi

# Generate comprehensive report
echo "ðŸ“ˆ Generating test report..."
python scripts/generate_test_report.py \
    --artifacts-dir test-results \
    --output test-results/comprehensive-report.html

# Cleanup
echo "ðŸ§¹ Cleaning up test environment..."
docker-compose -f docker-compose.test.yml down

# Summary
echo "============================================"
if [ ${#FAILED_SUITES[@]} -eq 0 ]; then
    echo "ðŸŽ‰ All test suites passed successfully!"
    echo "ðŸ“Š Coverage report: coverage/html/index.html"
    echo "ðŸ“‹ Full report: test-results/comprehensive-report.html"
    exit 0
else
    echo "âŒ Failed test suites: ${FAILED_SUITES[*]}"
    echo "ðŸ“‹ Check detailed reports in test-results/"
    exit 1
fi
```

### 11.3 Test Result Analysis

**Coverage Analysis Tool:**
```python
# scripts/analyze_coverage.py
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import argparse

class CoverageAnalyzer:
    """Analyze test coverage and identify gaps"""
    
    def __init__(self, coverage_file: str):
        self.coverage_file = Path(coverage_file)
        self.coverage_data = {}
        self.load_coverage_data()
    
    def load_coverage_data(self):
        """Load coverage data from XML file"""
        if not self.coverage_file.exists():
            raise FileNotFoundError(f"Coverage file not found: {self.coverage_file}")
        
        tree = ET.parse(self.coverage_file)
        root = tree.getroot()
        
        # Parse packages and classes
        for package in root.findall('.//package'):
            package_name = package.get('name')
            self.coverage_data[package_name] = {
                'line_rate': float(package.get('line-rate', 0)),
                'branch_rate': float(package.get('branch-rate', 0)),
                'classes': {}
            }
            
            # Parse classes within package
            for class_elem in package.findall('.//class'):
                class_name = class_elem.get('name')
                self.coverage_data[package_name]['classes'][class_name] = {
                    'filename': class_elem.get('filename'),
                    'line_rate': float(class_elem.get('line-rate', 0)),
                    'branch_rate': float(class_elem.get('branch-rate', 0)),
                    'lines': {},
                    'missing_lines': []
                }
                
                # Parse line coverage
                for line in class_elem.findall('.//line'):
                    line_num = int(line.get('number'))
                    hits = int(line.get('hits', 0))
                    self.coverage_data[package_name]['classes'][class_name]['lines'][line_num] = hits
                    
                    if hits == 0:
                        self.coverage_data[package_name]['classes'][class_name]['missing_lines'].append(line_num)
    
    def find_low_coverage_files(self, threshold: float = 0.8):
        """Find files with coverage below threshold"""
        low_coverage_files = []
        
        for package_name, package_data in self.coverage_data.items():
            for class_name, class_data in package_data['classes'].items():
                if class_data['line_rate'] < threshold:
                    low_coverage_files.append({
                        'package': package_name,
                        'file': class_data['filename'],
                        'coverage': class_data['line_rate'],
                        'missing_lines': class_data['missing_lines']
                    })
        
        return sorted(low_coverage_files, key=lambda x: x['coverage'])
    
    def generate_coverage_recommendations(self):
        """Generate recommendations for improving coverage"""
        recommendations = []
        
        low_coverage = self.find_low_coverage_files(0.8)
        
        for file_info in low_coverage[:10]:  # Top 10 priorities
            missing_count = len(file_info['missing_lines'])
            
            if missing_count < 5:
                priority = "HIGH"
                effort = "LOW"
            elif missing_count < 20:
                priority = "MEDIUM" 
                effort = "MEDIUM"
            else:
                priority = "LOW"
                effort = "HIGH"
            
            recommendations.append({
                'file': file_info['file'],
                'current_coverage': f"{file_info['coverage']:.1%}",
                'missing_lines': missing_count,
                'priority': priority,
                'effort': effort,
                'recommendation': self.get_test_recommendation(file_info)
            })
        
        return recommendations
    
    def get_test_recommendation(self, file_info):
        """Get specific testing recommendation for a file"""
        filename = Path(file_info['file']).name.lower()
        
        if 'api' in filename or 'endpoint' in filename:
            return "Add API endpoint unit tests with mocked dependencies"
        elif 'agent' in filename:
            return "Add agent behavior tests with mocked Ollama responses"
        elif 'database' in filename or 'db' in filename:
            return "Add database operation tests with test fixtures"
        elif 'service' in filename:
            return "Add service layer tests with dependency injection"
        elif 'utils' in filename or 'helper' in filename:
            return "Add utility function tests with edge cases"
        else:
            return "Add comprehensive unit tests covering all methods"
    
    def print_coverage_report(self):
        """Print detailed coverage report"""
        print("ðŸ” Coverage Analysis Report")
        print("=" * 50)
        
        # Overall statistics
        total_files = sum(len(pkg['classes']) for pkg in self.coverage_data.values())
        low_coverage_files = len(self.find_low_coverage_files(0.8))
        
        print(f"Total files analyzed: {total_files}")
        print(f"Files below 80% coverage: {low_coverage_files}")
        print()
        
        # Recommendations
        recommendations = self.generate_coverage_recommendations()
        if recommendations:
            print("ðŸ“‹ Priority Testing Recommendations:")
            print("-" * 50)
            
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec['file']}")
                print(f"   Current: {rec['current_coverage']} | "
                      f"Missing: {rec['missing_lines']} lines | "
                      f"Priority: {rec['priority']} | "
                      f"Effort: {rec['effort']}")
                print(f"   â†’ {rec['recommendation']}")
                print()
        
        # Package-level summary
        print("ðŸ“¦ Package Coverage Summary:")
        print("-" * 50)
        for package_name, package_data in self.coverage_data.items():
            print(f"{package_name}: {package_data['line_rate']:.1%} line coverage")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Analyze test coverage')
    parser.add_argument('--coverage-file', default='coverage/coverage.xml', 
                       help='Path to coverage XML file')
    
    args = parser.parse_args()
    
    try:
        analyzer = CoverageAnalyzer(args.coverage_file)
        analyzer.print_coverage_report()
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("Run tests with coverage first: pytest --cov=backend --cov-report=xml")
```

## 12. Testing Workflow

### 12.1 Developer Testing Responsibilities

**Daily Development Workflow:**
1. **Before Starting Work:**
   - Pull latest code and run `pytest tests/unit/` 
   - Ensure all existing tests pass
   - Check current coverage with `coverage report`

2. **During Development:**
   - Write tests first (TDD) or immediately after (Test-Driven Development)
   - Run related unit tests frequently: `pytest tests/unit/test_specific_module.py`
   - Use `pytest --lf` to run only last failed tests

3. **Before Committing:**
   - Run pre-commit hooks: `pre-commit run --all-files`
   - Ensure new code has â‰¥80% coverage
   - Run integration tests if changes affect APIs: `pytest tests/integration/`

4. **Code Review Testing Checklist:**
   ```
   â–¡ New functionality has corresponding unit tests
   â–¡ Tests cover both happy path and error scenarios  
   â–¡ Test names clearly describe what is being tested
   â–¡ No hardcoded values or automated numbers in tests
   â–¡ Appropriate mocking of external dependencies
   â–¡ Tests are independent and can run in any order
   â–¡ Performance assertions included for critical paths
   â–¡ Security test cases for input validation
   ```

### 12.2 Release Testing Procedures

**Pre-Release Testing Checklist:**
1. **Full Test Suite Execution:**
   ```bash
   # Run complete test suite
   ./scripts/run_comprehensive_tests.sh
   
   # Verify all quality gates pass
   pytest --cov=backend --cov=agents --cov-fail-under=80
   
   # Run performance benchmarks
   k6 run tests/performance/release_benchmark.js
   
   # Execute security scan
   bandit -r backend/ agents/ -f json
   ```

2. **Manual Testing:**
   - Smoke test critical user journeys
   - Verify UI functionality in browser
   - Test agent interactions manually
   - Confirm monitoring dashboards work

3. **Performance Validation:**
   - Load test with expected production traffic
   - Memory leak detection over 1-hour run
   - Response time validation under load
   - Resource utilization monitoring

### 12.3 Bug Triage and Testing Process

**Bug Investigation Process:**
1. **Reproduce Bug:**
   - Create   reproduction case
   - Document steps and environment
   - Add reproduction test to test suite

2. **Root Cause Analysis:**
   - Run tests in debug mode: `pytest -vvv -s --tb=long`
   - Check logs and error traces
   - Use debugger for complex issues

3. **Fix Validation:**
   - Write test that demonstrates fix
   - Ensure fix doesn't break existing functionality
   - Run regression tests: `pytest tests/regression/`

**Bug Triage Categories:**
- **P0 Critical:** System down, data loss, security breach
- **P1 High:** Major feature broken, performance degradation
- **P2 Medium:** Minor feature issues, UI problems
- **P3 Low:** Cosmetic issues, nice-to-have improvements

## 13. Phased Implementation Plan

### 13.1 8-Week MVP Timeline

**Week 1: Foundation & Critical Path (P0 Components)**
- **Days 1-2:** Set up testing infrastructure
  - Install and configure pytest, coverage, CI/CD pipeline
  - Create test directories and basic fixtures
  - Set up Docker test environment
- **Days 3-5:** Backend API unit tests (Target: 80% coverage)
  - Test all health check endpoints
  - Test chat endpoint with mocked Ollama
  - Test basic authentication/validation
  - Test error handling paths
- **Days 6-7:** Database integration tests
  - PostgreSQL connection and basic operations
  - Redis caching functionality
  - Neo4j graph operations

**Week 2: Ollama Integration & Agent Testing (P0)**
- **Days 1-3:** Ollama integration comprehensive testing
  - Model loading and inference testing
  - Performance and timeout handling
  - Error scenarios (service down, model unavailable)
- **Days 4-7:** Agent service testing
  - Test all 7 stub agents
  - Mock realistic agent responses
  - Test agent health endpoints
  - Performance benchmarking for agents

**Week 3: Cross-Service Integration & API Contracts (P1)**
- **Days 1-3:** Cross-service integration tests
  - Agent-to-backend communication
  - Backend-to-Ollama flows
  - Message queue integration (if used)
- **Days 4-5:** API contract testing with Newman/Postman
  - All public API endpoints
  - Request/response validation
  - Error response consistency
- **Days 6-7:** Frontend component testing
  - Streamlit UI basic functionality
  - User interaction flows

**Week 4: Performance & Security Testing (P0)**
- **Days 1-3:** Performance testing implementation
  - K6 load tests for critical endpoints  
  - Response time benchmarks
  - Resource utilization monitoring
- **Days 4-5:** Security testing
  - OWASP Top 10 coverage
  - Input validation testing
  - Authentication/authorization testing
- **Days 6-7:** Performance optimization based on test results

**Week 5: End-to-End Testing & Real Agent Implementation**
- **Days 1-3:** E2E testing with Playwright
  - Critical user journeys
  - Multi-service workflows
  - Error recovery scenarios
- **Days 4-7:** Convert 1-2 stub agents to real implementations
  - Replace hardcoded responses with actual logic
  - Test real Ollama model integration
  - Performance validation

**Week 6: Chaos Engineering & Resilience**
- **Days 1-3:** Failure scenario testing
  - Service failure recovery
  - Network partition handling
  - Resource exhaustion scenarios
- **Days 4-5:** Chaos engineering implementation
  - Automated failure injection
  - Recovery time measurement
- **Days 6-7:** System resilience improvements

**Week 7: CI/CD Integration & Automation**
- **Days 1-3:** Complete CI/CD pipeline setup
  - GitHub Actions workflow
  - Automated test execution
  - Coverage reporting and gates
- **Days 4-5:** Test automation and reporting
  - Comprehensive test reports
  - Performance regression detection
  - Security scanning automation
- **Days 6-7:** Pre-commit hooks and developer workflow

**Week 8: Final Validation & Production Readiness**
- **Days 1-2:** Final test coverage validation (Target: â‰¥80%)
  - Fill remaining coverage gaps
  - Document testing procedures
- **Days 3-4:** Production-like environment testing
  - Full system load testing
  - Monitoring and alerting validation
- **Days 5:** Final security and compliance validation
- **Days 6-7:** Testing documentation and team training

### 13.2 Success Criteria by Phase

**Week 1 Success Criteria:**
- âœ… Test infrastructure operational
- âœ… Unit test coverage â‰¥60% for backend APIs
- âœ… Database integration tests passing
- âœ… CI pipeline executing basic tests

**Week 2 Success Criteria:**
- âœ… Ollama integration fully tested
- âœ… All agent endpoints tested 
- âœ… Unit test coverage â‰¥70% overall
- âœ… Performance baseline established

**Week 4 Success Criteria:**
- âœ… API contract tests operational
- âœ… Load testing framework operational
- âœ… Security tests covering OWASP Top 10
- âœ… Unit test coverage â‰¥75% overall

**Week 6 Success Criteria:**
- âœ… E2E test suite operational
- âœ… At least 1 real agent implemented and tested
- âœ… Chaos engineering framework operational
- âœ… System resilience validated

**Week 8 Success Criteria (MVP Complete):**
- âœ… **Unit test coverage â‰¥80%** (â‰¥90% for P0 components)
- âœ… **Integration test coverage â‰¥70%** for API endpoints
- âœ… **E2E tests covering critical user journeys**
- âœ… **Performance testing automated** with regression detection
- âœ… **Security testing integrated** in CI/CD
- âœ… **Chaos engineering** validating system resilience
- âœ… **Full CI/CD pipeline** with quality gates
- âœ… **Production readiness** validated

### 13.3 Risk Mitigation

**Technical Risks:**
- **Risk:** Ollama model inference too slow for testing
  - **Mitigation:** Use smaller test models, implement comprehensive mocking
- **Risk:** Container startup times affect test execution
  - **Mitigation:** Pre-built test containers, parallel test execution
- **Risk:** Network flakiness in container communication
  - **Mitigation:** Retry logic, health check validation, network isolation

**Timeline Risks:**
- **Risk:** Complex integration testing takes longer than expected
  - **Mitigation:** Prioritize P0 components, use comprehensive mocking
- **Risk:** Performance testing reveals major bottlenecks
  - **Mitigation:** Focus on testing framework first, optimize later
- **Risk:** Security testing uncovers critical vulnerabilities
  - **Mitigation:** Basic security hygiene from Week 1, dedicated security sprint

**Resource Risks:**
- **Risk:** Limited CI/CD resources for parallel test execution
  - **Mitigation:** Optimize test suite, use test sharding, prioritize critical tests
- **Risk:** Insufficient monitoring/observability for test analysis
  - **Mitigation:** Leverage existing Prometheus/Grafana stack for test metrics

## Conclusion

This comprehensive testing strategy provides a structured approach to achieve professional-grade quality assurance for the SutazAI system. By implementing this strategy, we will:

1. **Achieve 80% minimum test coverage** across all critical components
2. **Implement robust CI/CD pipeline** with automated quality gates
3. **Ensure system reliability** through comprehensive failure testing
4. **Validate performance** under realistic load conditions
5. **Secure the system** against common vulnerabilities
6. **Enable confident deployment** with comprehensive validation

The phased 8-week approach balances thorough coverage with practical delivery timelines, ensuring that critical components receive the highest testing priority while building a sustainable testing infrastructure for long-term quality assurance.

**Key Success Metrics:**
- **80% Unit Test Coverage** (90% for P0 components)
- **70% Integration Test Coverage** 
- **100% Security Test Coverage** for OWASP Top 10
- **<2s Response Time** for 95th percentile
- **>95% Test Success Rate** in CI/CD pipeline
- **<30 minute Recovery Time** for critical failures

This strategy transforms SutazAI from a **0% tested system** to a **professionally tested, production-ready platform** within 8 weeks, providing the quality foundation necessary for reliable AI agent orchestration and autonomous system operations.