# ADR-0002: 100% Local Execution with Ollama Models

- Context: No external LLMs permitted; mismatch between backend default and loaded model
- Decision: Standardize on Ollama; define approved model set and preload policy
- Alternatives: External APIs (rejected)
- Consequences: Update configs/tests; resource planning for GPU/CPU fallback
- Status: Accepted
- Sources: `SUTAZAI_PRD.md`