{
  "id": "ollama-integration-specialist",
  "name": "ollama-integration-specialist",
  "version": "1.0.0",
  "description": "Use this agent when you need to:\\n\\n- Configure and optimize Ollama for local LLM inference\\n- Manage and deploy local language models\\n- Optimize model performance and memory usage\\n- Implement model quantization strategies\\n- Configure Ollama API endpoints and compatibility\\n- Set up model caching and preloading\\n- Implement model switching and routing\\n- Create custom model configurations\\n- Design multi-model inference pipelines\\n- Optimize GPU/CPU utilization for inference\\n- Implement model versioning strategies\\n- Build model performance benchmarks\\n- Create model selection algorithms\\n- Design fallback mechanisms for model failures\\n- Implement model warm-up procedures\\n- Build model monitoring and metrics\\n- Create model deployment automation\\n- Design model scaling strategies\\n- Implement context window optimization\\n- Build prompt caching systems\\n- Create model fine-tuning workflows\\n- Design model security measures\\n- Implement model access control\\n- Build model testing frameworks\\n- Create model documentation\\n- Design model cost optimization\\n- Implement streaming inference\\n- Build batch inference systems\\n- Create model API compatibility layers\\n- Design model integration patterns\\n\\nDo NOT use this agent for:\\n- General AI development (use senior-ai-engineer)\\n- Infrastructure setup (use infrastructure-devops-manager)\\n- Agent orchestration (use ai-agent-orchestrator)\\n- Frontend development (use senior-frontend-developer)\\n\\nThis agent specializes in making Ollama work efficiently for local LLM inference at scale.",
  "provider": "universal",
  "type": "system",
  "status": "active",
  "capabilities": [
    "security_analysis",
    "code_generation",
    "testing",
    "deployment",
    "monitoring",
    "optimization",
    "automation",
    "documentation"
  ],
  "configuration": {
    "enabled": true,
    "priority": 5,
    "timeout": 300,
    "max_retries": 3,
    "rate_limit": {
      "requests_per_minute": 60,
      "burst_size": 10
    }
  },
  "resources": {
    "cpu_limit": "1.0",
    "memory_limit": "512Mi",
    "gpu_required": false
  },
  "endpoints": {
    "health": "/api/v1/agents/ollama-integration-specialist/health",
    "execute": "/api/v1/agents/ollama-integration-specialist/execute",
    "status": "/api/v1/agents/ollama-integration-specialist/status"
  },
  "dependencies": [],
  "metadata": {
    "created_at": "2025-08-16T07:12:23.630232",
    "created_by": "AgentConfigurationFixer",
    "tags": [
      "quality",
      "compliance",
      "automation",
      "observability",
      "security",
      "performance",
      "knowledge",
      "docs",
      "devops",
      "efficiency",
      "workflow",
      "infrastructure",
      "validation",
      "metrics",
      "development"
    ],
    "category": "security"
  },
  "security": {
    "scan_on_startup": true,
    "vulnerability_threshold": "medium",
    "compliance_checks": [
      "OWASP",
      "CIS"
    ]
  },
  "monitoring": {
    "metrics_enabled": true,
    "logging_level": "INFO",
    "telemetry": true
  },
  "deployment": {
    "strategy": "rolling",
    "health_check_interval": 30,
    "rollback_on_failure": true
  }
}