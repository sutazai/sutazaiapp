#!/usr/bin/env python3
"""
ULTRAFIX DOCKERFILE ANALYSIS ENGINE
===================================
Comprehensive analysis of Docker chaos in SutazAI system
Identifies exact duplicates, patterns, and consolidation opportunities

Author: DevOps Infrastructure Manager - ULTRAFIX
Date: August 10, 2025
"""

import os
import re
import hashlib
import json
from collections import defaultdict, Counter
from pathlib import Path

class DockerfileAnalyzer:
    def __init__(self, root_dir="/opt/sutazaiapp"):
        self.root_dir = Path(root_dir)
        self.dockerfiles = []
        self.patterns = defaultdict(list)
        self.base_images = Counter()
        self.duplicates = defaultdict(list)
        self.analysis = {
            'total_files': 0,
            'categories': {},
            'base_image_usage': {},
            'exact_duplicates': [],
            'near_duplicates': [],
            'consolidation_opportunities': [],
            'security_issues': [],
            'optimization_potential': []
        }
    
    def find_dockerfiles(self):
        """Find all Dockerfiles excluding archives and node_modules"""
        excluded_paths = ['node_modules', 'archive', 'backups', '.git']
        
        for dockerfile in self.root_dir.rglob("Dockerfile*"):
            # Skip excluded directories
            if any(excluded in str(dockerfile) for excluded in excluded_paths):
                continue
                
            self.dockerfiles.append(dockerfile)
        
        self.analysis['total_files'] = len(self.dockerfiles)
        print(f"Found {len(self.dockerfiles)} Dockerfiles")
    
    def analyze_dockerfile(self, filepath):
        """Analyze individual Dockerfile for patterns"""
        try:
            with open(filepath, 'r') as f:
                content = f.read()
            
            # Calculate content hash for duplicate detection
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # Extract base image
            base_image_match = re.search(r'^FROM\s+([^\s]+)', content, re.MULTILINE)
            base_image = base_image_match.group(1) if base_image_match else "unknown"
            
            # Extract exposed ports
            ports = re.findall(r'EXPOSE\s+(\d+)', content)
            
            # Extract environment variables
            env_vars = re.findall(r'ENV\s+([A-Z_]+)=([^\s]+)', content)
            
            # Check for security patterns
            security_issues = []
            if 'USER root' in content or ('USER' not in content and 'FROM sutazai-python-agent-master' not in content):
                security_issues.append('runs_as_root')
            
            # Check for optimization opportunities
            optimization_issues = []
            if content.count('RUN') > 5:
                optimization_issues.append('too_many_layers')
            if 'apt-get update' in content and 'rm -rf /var/lib/apt/lists/*' not in content:
                optimization_issues.append('missing_apt_cleanup')
            
            return {
                'path': str(filepath.relative_to(self.root_dir)),
                'size': len(content),
                'lines': len(content.splitlines()),
                'hash': content_hash,
                'base_image': base_image,
                'ports': ports,
                'env_vars': dict(env_vars),
                'security_issues': security_issues,
                'optimization_issues': optimization_issues,
                'content': content
            }
            
        except Exception as e:
            print(f"Error analyzing {filepath}: {e}")
            return None
    
    def categorize_dockerfiles(self):
        """Categorize Dockerfiles by purpose and technology"""
        categories = {
            'python_agents': [],
            'nodejs_services': [],
            'databases': [],
            'monitoring': [],
            'frontend': [],
            'backend': [],
            'base_images': [],
            'templates': [],
            'security': [],
            'ai_ml': [],
            'other': []
        }
        
        for dockerfile in self.dockerfiles:
            analysis = self.analyze_dockerfile(dockerfile)
            if not analysis:
                continue
                
            path = analysis['path'].lower()
            base_image = analysis['base_image'].lower()
            
            # Categorize based on path and base image
            if 'base' in path or 'template' in path:
                categories['base_images'].append(analysis)
            elif 'agent' in path and ('python' in base_image or 'sutazai-python-agent' in base_image):
                categories['python_agents'].append(analysis)
            elif 'node' in base_image or 'npm' in path:
                categories['nodejs_services'].append(analysis)
            elif any(db in path for db in ['postgres', 'redis', 'neo4j', 'qdrant', 'chroma']):
                categories['databases'].append(analysis)
            elif any(mon in path for mon in ['prometheus', 'grafana', 'monitor']):
                categories['monitoring'].append(analysis)
            elif 'frontend' in path:
                categories['frontend'].append(analysis)
            elif 'backend' in path:
                categories['backend'].append(analysis)
            elif 'secure' in path:
                categories['security'].append(analysis)
            elif any(ai in path for ai in ['ai', 'ml', 'model', 'ollama', 'langflow']):
                categories['ai_ml'].append(analysis)
            else:
                categories['other'].append(analysis)
            
            # Track base image usage
            self.base_images[analysis['base_image']] += 1
        
        self.analysis['categories'] = {k: len(v) for k, v in categories.items()}
        self.analysis['base_image_usage'] = dict(self.base_images.most_common())
        return categories
    
    def find_duplicates(self, categories):
        """Find exact and near duplicates"""
        all_files = []
        for category_files in categories.values():
            all_files.extend(category_files)
        
        # Group by content hash for exact duplicates
        hash_groups = defaultdict(list)
        for analysis in all_files:
            hash_groups[analysis['hash']].append(analysis)
        
        exact_duplicates = []
        for files in hash_groups.values():
            if len(files) > 1:
                exact_duplicates.append({
                    'files': [f['path'] for f in files],
                    'count': len(files),
                    'size': files[0]['size']
                })
        
        # Find near duplicates (similar content patterns)
        near_duplicates = []
        patterns = defaultdict(list)
        
        for analysis in all_files:
            # Create pattern signature
            content = analysis['content']
            pattern = {
                'base_image': analysis['base_image'],
                'port_count': len(analysis['ports']),
                'env_count': len(analysis['env_vars']),
                'has_healthcheck': 'HEALTHCHECK' in content,
                'has_copy': 'COPY' in content,
                'commands': len(re.findall(r'^(RUN|COPY|ENV|EXPOSE)', content, re.MULTILINE))
            }
            pattern_key = str(sorted(pattern.items()))
            patterns[pattern_key].append(analysis)
        
        for pattern_files in patterns.values():
            if len(pattern_files) > 3:  # Similar patterns with 4+ files
                near_duplicates.append({
                    'pattern': pattern_files[0]['base_image'],
                    'files': [f['path'] for f in pattern_files],
                    'count': len(pattern_files)
                })
        
        self.analysis['exact_duplicates'] = exact_duplicates
        self.analysis['near_duplicates'] = near_duplicates
        return exact_duplicates, near_duplicates
    
    def generate_consolidation_plan(self, categories):
        """Generate specific consolidation recommendations"""
        opportunities = []
        
        # Python agent consolidation
        python_agents = categories['python_agents']
        if len(python_agents) > 5:
            opportunities.append({
                'category': 'python_agents',
                'current_count': len(python_agents),
                'target_count': 3,  # Base + FastAPI + Flask variants
                'consolidation_strategy': 'Use sutazai-python-agent-master base with service-specific overrides',
                'savings': f"Reduce from {len(python_agents)} to 3 files (~{((len(python_agents)-3)/len(python_agents)*100):.0f}% reduction)"
            })
        
        # Database services
        databases = categories['databases']
        if len(databases) > 5:
            opportunities.append({
                'category': 'databases',
                'current_count': len(databases),
                'target_count': 2,  # Secure + Standard variants
                'consolidation_strategy': 'Unified database base with security overlays',
                'savings': f"Reduce from {len(databases)} to 2 files (~{((len(databases)-2)/len(databases)*100):.0f}% reduction)"
            })
        
        # AI/ML services
        ai_ml = categories['ai_ml']
        if len(ai_ml) > 10:
            opportunities.append({
                'category': 'ai_ml',
                'current_count': len(ai_ml),
                'target_count': 4,  # PyTorch, TensorFlow, Ollama, Generic ML
                'consolidation_strategy': 'Framework-specific base images',
                'savings': f"Reduce from {len(ai_ml)} to 4 files (~{((len(ai_ml)-4)/len(ai_ml)*100):.0f}% reduction)"
            })
        
        self.analysis['consolidation_opportunities'] = opportunities
        return opportunities
    
    def run_analysis(self):
        """Run complete analysis"""
        print("ULTRAFIX DOCKERFILE ANALYSIS - Starting...")
        
        # Find all Dockerfiles
        self.find_dockerfiles()
        
        # Categorize by purpose
        print("Categorizing Dockerfiles...")
        categories = self.categorize_dockerfiles()
        
        # Find duplicates
        print("Finding duplicates...")
        exact_dups, near_dups = self.find_duplicates(categories)
        
        # Generate consolidation plan
        print("Generating consolidation plan...")
        opportunities = self.generate_consolidation_plan(categories)
        
        return self.analysis, categories
    
    def generate_report(self):
        """Generate comprehensive analysis report"""
        analysis, categories = self.run_analysis()
        
        report = f"""
ULTRAFIX DOCKERFILE CONSOLIDATION ANALYSIS REPORT
===============================================
Date: August 10, 2025
Total Dockerfiles Found: {analysis['total_files']}

CATEGORY BREAKDOWN:
------------------
Python Agents: {analysis['categories']['python_agents']}
Node.js Services: {analysis['categories']['nodejs_services']}
Databases: {analysis['categories']['databases']}
Monitoring: {analysis['categories']['monitoring']}
Frontend: {analysis['categories']['frontend']}
Backend: {analysis['categories']['backend']}
Base Images: {analysis['categories']['base_images']}
Security: {analysis['categories']['security']}
AI/ML Services: {analysis['categories']['ai_ml']}
Other: {analysis['categories']['other']}

BASE IMAGE USAGE:
----------------
"""
        
        for base_image, count in analysis['base_image_usage'].items():
            report += f"{base_image}: {count} files\n"
        
        report += f"""

EXACT DUPLICATES FOUND: {len(analysis['exact_duplicates'])}
------------------------
"""
        
        for dup in analysis['exact_duplicates']:
            report += f"- {dup['count']} identical files ({dup['size']} bytes each)\n"
            for file in dup['files']:
                report += f"  - {file}\n"
        
        report += f"""

CONSOLIDATION OPPORTUNITIES:
---------------------------
"""
        
        total_reduction = 0
        for opp in analysis['consolidation_opportunities']:
            reduction = opp['current_count'] - opp['target_count']
            total_reduction += reduction
            report += f"""
{opp['category'].upper()}:
  Current: {opp['current_count']} files
  Target: {opp['target_count']} files
  Strategy: {opp['consolidation_strategy']}
  Savings: {opp['savings']}
"""
        
        report += f"""

TOTAL CONSOLIDATION POTENTIAL:
-----------------------------
Current Files: {analysis['total_files']}
Potential Target: {analysis['total_files'] - total_reduction}
Total Reduction: {total_reduction} files ({(total_reduction/analysis['total_files']*100):.1f}%)

RECOMMENDED ACTIONS:
-------------------
1. Consolidate Python agents using sutazai-python-agent-master base
2. Create unified database security overlay system
3. Implement framework-specific AI/ML base images
4. Remove exact duplicate files immediately
5. Apply security hardening (non-root users) across all images
"""
        
        return report

if __name__ == "__main__":
    analyzer = DockerfileAnalyzer()
    report = analyzer.generate_report()
    print(report)
    
    # Save detailed analysis to JSON
    analysis, categories = analyzer.run_analysis()
    with open('/opt/sutazaiapp/ULTRA_DOCKERFILE_ANALYSIS_RESULTS.json', 'w') as f:
        json.dump({
            'analysis': analysis,
            'categories': {k: [{'path': item['path'], 'base_image': item['base_image'], 'size': item['size']} 
                             for item in v] for k, v in categories.items()}
        }, f, indent=2)
    
    print("\nDetailed analysis saved to: ULTRA_DOCKERFILE_ANALYSIS_RESULTS.json")