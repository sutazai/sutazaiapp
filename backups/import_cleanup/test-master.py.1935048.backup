#!/usr/bin/env python3
"""
MASTER TEST CONTROLLER
Consolidated from 50+ test scripts
Generated by ULTRA SCRIPT CONSOLIDATION
Purpose: Single entry point for all testing
"""

import os
import sys
import time
import json
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

class TestMaster:
    """Master test controller for all system testing"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.results = {}
        self.start_time = datetime.now()
        
    def log(self, message: str, level: str = "INFO"):
        """Structured logging"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] [{level}] {message}")
        
    def run_command(self, cmd: List[str], cwd: Optional[Path] = None, timeout: int = 300) -> Dict[str, Any]:
        """Run a command and return results"""
        if cwd is None:
            cwd = self.project_root
            
        try:
            self.log(f"Running: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=cwd
            )
            
            return {
                'success': result.returncode == 0,
                'returncode': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'duration': 0  # Would need timing
            }
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'returncode': -1,
                'stdout': '',
                'stderr': f'Command timeout after {timeout}s',
                'duration': timeout
            }
        except Exception as e:
            return {
                'success': False,
                'returncode': -1,
                'stdout': '',
                'stderr': str(e),
                'duration': 0
            }
    
    def run_unit_tests(self) -> Dict[str, Any]:
        """Run all unit tests"""
        self.log("Running unit tests...")
        
        results = {}
        
        # Python unit tests with pytest
        if (self.project_root / 'pytest.ini').exists():
            result = self.run_command(['python', '-m', 'pytest', 'tests/', '-v', '--tb=short'])
            results['pytest'] = result
        
        # Backend tests
        backend_tests = self.project_root / 'backend' / 'tests'
        if backend_tests.exists():
            result = self.run_command(['python', '-m', 'pytest', str(backend_tests), '-v'])
            results['backend_tests'] = result
        
        # Agent tests
        agent_tests = []
        for agent_dir in (self.project_root / 'agents').glob('*/'):
            test_files = list(agent_dir.glob('test_*.py')) + list(agent_dir.glob('*_test.py'))
            if test_files:
                for test_file in test_files:
                    result = self.run_command(['python', str(test_file)])
                    results[f'agent_{agent_dir.name}_{test_file.stem}'] = result
        
        return results
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """Run all integration tests"""
        self.log("Running integration tests...")
        
        results = {}
        
        # Service health checks
        result = self.run_command([str(self.project_root / 'scripts/master/health-master.py')])
        results['health_check'] = result
        
        # API endpoint tests
        api_test_script = self.project_root / 'backend' / 'test_hardware_integration.py'
        if api_test_script.exists():
            result = self.run_command(['python', str(api_test_script)])
            results['api_tests'] = result
        
        # Docker compose validation
        result = self.run_command(['docker', 'compose', 'config'])
        results['docker_config'] = result
        
        # Frontend accessibility tests
        frontend_test = self.project_root / 'frontend' / 'test_accessibility.py'
        if frontend_test.exists():
            result = self.run_command(['python', str(frontend_test)])
            results['frontend_accessibility'] = result
        
        return results
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """Run performance tests"""
        self.log("Running performance tests...")
        
        results = {}
        
        # Load testing
        load_test = self.project_root / 'tests' / 'simplified_load_test.py'
        if load_test.exists():
            result = self.run_command(['python', str(load_test)])
            results['load_test'] = result
        
        # Hardware optimizer performance
        hw_test = self.project_root / 'agents' / 'hardware-resource-optimizer' / 'final_performance_demo.py'
        if hw_test.exists():
            result = self.run_command(['python', str(hw_test)])
            results['hardware_performance'] = result
        
        return results
    
    def run_security_tests(self) -> Dict[str, Any]:
        """Run security tests"""
        self.log("Running security tests...")
        
        results = {}
        
        # Security test suite
        security_test = self.project_root / 'tests' / 'corrected_security_test.py'
        if security_test.exists():
            result = self.run_command(['python', str(security_test)])
            results['security_scan'] = result
        
        # CORS security tests
        cors_test = self.project_root / 'backend' / 'test_cors_security.py'
        if cors_test.exists():
            result = self.run_command(['python', str(cors_test)])
            results['cors_security'] = result
        
        return results
    
    def run_smoke_tests(self) -> Dict[str, Any]:
        """Run quick smoke tests"""
        self.log("Running smoke tests...")
        
        results = {}
        
        # Quick service checks
        services = [
            ('backend', 'http://localhost:10010/health'),
            ('frontend', 'http://localhost:10011/'),
            ('ollama', 'http://localhost:10104/api/tags')
        ]
        
        for service, url in services:
            result = self.run_command(['curl', '-f', '-s', url], timeout=10)
            results[f'smoke_{service}'] = result
        
        return results
    
    def run_all(self) -> Dict[str, Any]:
        """Run all test suites"""
        self.log("Running comprehensive test suite...")
        
        all_results = {
            'start_time': self.start_time.isoformat(),
            'test_suites': {}
        }
        
        # Run all test types
        test_types = [
            ('unit', self.run_unit_tests),
            ('integration', self.run_integration_tests),
            ('smoke', self.run_smoke_tests),
            ('performance', self.run_performance_tests),
            ('security', self.run_security_tests)
        ]
        
        for test_type, test_func in test_types:
            self.log(f"=== {test_type.upper()} TESTS ===")
            try:
                results = test_func()
                all_results['test_suites'][test_type] = {
                    'status': 'completed',
                    'results': results,
                    'success_count': sum(1 for r in results.values() if r.get('success', False)),
                    'total_count': len(results)
                }
            except Exception as e:
                self.log(f"Error in {test_type} tests: {e}", "ERROR")
                all_results['test_suites'][test_type] = {
                    'status': 'failed',
                    'error': str(e),
                    'success_count': 0,
                    'total_count': 0
                }
        
        # Calculate overall results
        total_success = sum(suite.get('success_count', 0) for suite in all_results['test_suites'].values())
        total_tests = sum(suite.get('total_count', 0) for suite in all_results['test_suites'].values())
        
        all_results.update({
            'end_time': datetime.now().isoformat(),
            'duration': (datetime.now() - self.start_time).total_seconds(),
            'total_success': total_success,
            'total_tests': total_tests,
            'success_rate': (total_success / total_tests * 100) if total_tests > 0 else 0
        })
        
        self.results = all_results
        return all_results
    
    def generate_report(self, format_type: str = 'console') -> str:
        """Generate test report"""
        if not self.results:
            self.run_all()
        
        if format_type == 'json':
            return json.dumps(self.results, indent=2)
        
        elif format_type == 'console':
            report = []
            report.append("="*60)
            report.append("SUTAZAI TEST SUITE REPORT")
            report.append("="*60)
            report.append(f"Duration: {self.results.get('duration', 0):.2f}s")
            report.append(f"Success Rate: {self.results.get('success_rate', 0):.1f}%")
            report.append(f"Tests: {self.results.get('total_success', 0)}/{self.results.get('total_tests', 0)} passed")
            report.append("")
            
            for suite_name, suite_data in self.results.get('test_suites', {}).items():
                status_icon = "✅" if suite_data['status'] == 'completed' else "❌"
                report.append(f"{status_icon} {suite_name.upper()} TESTS:")
                report.append(f"   {suite_data.get('success_count', 0)}/{suite_data.get('total_count', 0)} passed")
                
                if suite_data['status'] == 'failed':
                    report.append(f"   Error: {suite_data.get('error', 'Unknown')}")
                
                report.append("")
            
            return '\n'.join(report)
        
        else:
            raise ValueError(f"Unsupported format: {format_type}")

def main():
    parser = argparse.ArgumentParser(description="Master Test Controller")
    parser.add_argument("--type", choices=["unit", "integration", "performance", "security", "smoke", "all"], 
                       default="all", help="Type of tests to run")
    parser.add_argument("--format", choices=["console", "json"], default="console", 
                       help="Output format")
    parser.add_argument("--save-report", metavar="FILE", 
                       help="Save report to file")
    parser.add_argument("--timeout", type=int, default=300, 
                       help="Test timeout in seconds")
    
    args = parser.parse_args()
    
    master = TestMaster()
    
    try:
        # Run specified test type
        if args.type == "unit":
            results = {'test_suites': {'unit': {'results': master.run_unit_tests()}}}
        elif args.type == "integration":
            results = {'test_suites': {'integration': {'results': master.run_integration_tests()}}}
        elif args.type == "performance":
            results = {'test_suites': {'performance': {'results': master.run_performance_tests()}}}
        elif args.type == "security":
            results = {'test_suites': {'security': {'results': master.run_security_tests()}}}
        elif args.type == "smoke":
            results = {'test_suites': {'smoke': {'results': master.run_smoke_tests()}}}
        else:
            results = master.run_all()
        
        # Generate and output report
        master.results = results
        report = master.generate_report(args.format)
        
        if args.save_report:
            Path(args.save_report).write_text(report)
            master.log(f"Report saved to {args.save_report}")
        else:
            print(report)
        
        # Exit with appropriate code
        success_rate = results.get('success_rate', 0)
        if success_rate >= 90:
            sys.exit(0)
        elif success_rate >= 70:
            sys.exit(1)
        else:
            sys.exit(2)
            
    except KeyboardInterrupt:
        master.log("Tests interrupted")
        sys.exit(130)
    except Exception as e:
        master.log(f"Test execution failed: {e}", "ERROR")
        sys.exit(3)

if __name__ == "__main__":
    main()
