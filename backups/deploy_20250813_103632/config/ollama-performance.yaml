# ULTRA-PERFORMANCE Ollama Configuration
# Optimized for stability and sub-10s response times

# Core Performance Settings
performance:
  # Parallel request handling - reduced for stability
  num_parallel: 1  # Process one request at a time for stability
  max_queue: 10    # Limit queue size to prevent overload
  
  # Model management
  keep_alive: 5m   # Keep models in memory for 5 minutes
  max_loaded_models: 1  # Only keep one model loaded at a time
  
  # Threading and CPU
  num_threads: 8   # Optimal thread count for CPU inference
  num_gpu: 0       # CPU-only for stability
  
  # Memory management
  memory_limit: 8192  # 8GB memory limit
  swap_limit: 0       # Disable swap for predictable performance

# Request handling
requests:
  timeout: 30s          # 30 second timeout per request
  max_retries: 3        # Retry failed requests up to 3 times
  backoff_base: 2       # Exponential backoff base
  connection_pool: 10   # Connection pool size

# Model-specific optimizations
models:
  tinyllama:
    context_size: 2048
    batch_size: 8
    temperature: 0.7
    top_p: 0.9
    repeat_penalty: 1.1
    num_predict: 512     # Limit response length
    
  llama2:
    context_size: 4096
    batch_size: 4
    temperature: 0.7
    top_p: 0.9
    repeat_penalty: 1.1
    num_predict: 1024

# Caching configuration
cache:
  enabled: true
  ttl: 300              # Cache responses for 5 minutes
  max_size: 1000        # Maximum cached items
  
# Health check settings
health:
  interval: 30s
  timeout: 5s
  retries: 3
  
# Logging
logging:
  level: INFO
  format: json
  max_file_size: 100MB
  max_files: 5