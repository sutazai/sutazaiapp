# SutazAI Production Docker Compose Configuration
# Consolidated from 22 Docker files to 7 active configurations
# Last Updated: 2025-08-20
# Fixed: Removed extends pattern that caused depends_on conflict

services:
  # ===================================
  # CORE INFRASTRUCTURE SERVICES
  # ===================================
  
  postgres:
    image: postgres:16-alpine
    container_name: sutazai-postgres
    mem_limit: 2g
    mem_reservation: 1g
    environment:
      POSTGRES_DB: sutazai
      POSTGRES_USER: ${POSTGRES_USER:-sutazai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai123}
      POSTGRES_SHARED_BUFFERS: 512MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 1GB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 256MB
      POSTGRES_MAX_CONNECTIONS: 200
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "10000:5432"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sutazai"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: sutazai-redis
    mem_limit: 512m
    mem_reservation: 256m
    command: redis-server --maxmemory 400mb --maxmemory-policy allkeys-lru --save "" --appendonly no
    volumes:
      - redis-data:/data
    ports:
      - "10001:6379"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    restart: unless-stopped

  neo4j:
    image: neo4j:5-community
    container_name: sutazai-neo4j
    mem_limit: 1g
    mem_reservation: 512m
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-neo4j123}
      NEO4J_dbms_memory_heap_initial__size: 512M
      NEO4J_dbms_memory_heap_max__size: 512M
      NEO4J_dbms_memory_pagecache_size: 256M
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    ports:
      - "10002:7474"  # HTTP
      - "10003:7687"  # Bolt
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:7474 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
    restart: unless-stopped

  # ===================================
  # MESSAGING & API GATEWAY
  # ===================================
  
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: sutazai-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-sutazai}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-sutazai123}
      RABBITMQ_VM_MEMORY_HIGH_WATERMARK: 0.4
      RABBITMQ_DISK_FREE_LIMIT: 1GB
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    ports:
      - "10007:5672"   # AMQP
      - "10008:15672"  # Management UI
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
    restart: unless-stopped

  kong:
    image: kong:3.4-alpine
    container_name: sutazai-kong
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /kong/declarative/kong.yml
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_MEM_CACHE_SIZE: 64m
      KONG_WORKER_CONNECTIONS: 1024
    volumes:
      - ./config/kong:/kong/declarative
    ports:
      - "10005:8000"  # Proxy
      - "10015:8001"  # Admin API
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
    restart: unless-stopped

  consul:
    image: consul:1.17-alpine
    container_name: sutazai-consul
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0
    volumes:
      - consul-data:/consul/data
    ports:
      - "10006:8500"  # HTTP API & UI
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ===================================
  # APPLICATION SERVICES
  # ===================================
  
  backend:
    build:
      context: ./backend
      dockerfile: ../docker/backend/Dockerfile
    image: sutazai-backend:latest
    container_name: sutazai-backend
    mem_limit: 1g
    mem_reservation: 512m
    memswap_limit: 2g
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD:-sutazai123}@postgres:5432/sutazai
      REDIS_URL: redis://redis:6379
      NEO4J_URI: bolt://neo4j:7687
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-neo4j123}
      RABBITMQ_URL: amqp://${RABBITMQ_USER:-sutazai}:${RABBITMQ_PASS:-sutazai123}@rabbitmq:5672/
      JWT_SECRET_KEY: ${JWT_SECRET_KEY:-your-secret-key-here}
      PYTHONPATH: /app
      PYTHONUNBUFFERED: 1
      MALLOC_TRIM_THRESHOLD_: 100000
    volumes:
      - ./backend:/app
      - backend-logs:/var/log/backend
    ports:
      - "10010:10010"
    networks:
      - sutazai-network
    depends_on:
      - postgres
      - redis
      - neo4j
      - rabbitmq
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:10010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: ../docker/frontend/Dockerfile
    image: sutazai-frontend:latest
    container_name: sutazai-frontend
    mem_limit: 256m
    mem_reservation: 128m
    memswap_limit: 512m
    environment:
      BACKEND_URL: http://backend:10010
      STREAMLIT_SERVER_PORT: 10011
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
    volumes:
      - ./frontend:/app
    ports:
      - "10011:10011"
    networks:
      - sutazai-network
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:10011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
    restart: unless-stopped

  # ===================================
  # AI & VECTOR SERVICES
  # ===================================
  
  chromadb:
    image: chromadb/chroma:latest
    container_name: sutazai-chromadb
    mem_limit: 512m
    mem_reservation: 256m
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_PORT: 8000
      ANONYMIZED_TELEMETRY: "false"
    volumes:
      - chromadb-data:/chroma/chroma
    ports:
      - "10100:8000"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: sutazai-qdrant
    mem_limit: 512m
    mem_reservation: 256m
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    volumes:
      - qdrant-data:/qdrant/storage
    ports:
      - "10101:6333"  # HTTP API
      - "10102:6334"  # gRPC
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  faiss:
    build:
      context: ./docker/faiss
      dockerfile: Dockerfile
    image: sutazai-faiss:latest
    container_name: sutazai-faiss
    volumes:
      - faiss-data:/app/indices
    ports:
      - "10103:8080"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: sutazai-ollama
    environment:
      OLLAMA_NUM_PARALLEL: 1
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_MODELS_MEMORY: 256M
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "10104:11434"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
    restart: unless-stopped

  # ===================================
  # MONITORING STACK
  # ===================================
  
  prometheus:
    image: prom/prometheus:latest
    container_name: sutazai-prometheus
    mem_limit: 512m
    mem_reservation: 256m
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=1GB'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    ports:
      - "10200:9090"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: sutazai-grafana
    mem_limit: 256m
    mem_reservation: 128m
    environment:
      GF_SECURITY_ADMIN_USER: ${GF_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_ADMIN_PASSWORD:-admin123}
      GF_INSTALL_PLUGINS: redis-datasource,redis-app
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    ports:
      - "10201:3000"
    networks:
      - sutazai-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
    restart: unless-stopped

  loki:
    image: grafana/loki:latest
    container_name: sutazai-loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./config/loki:/etc/loki
      - loki-data:/loki
    ports:
      - "10202:3100"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: sutazai-alertmanager
    volumes:
      - ./config/alertmanager:/etc/alertmanager
      - alertmanager-data:/alertmanager
    ports:
      - "10203:9093"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: '0.1'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: sutazai-node-exporter
    command:
      - '--path.rootfs=/host'
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($$|/)'
    volumes:
      - /:/host:ro,rslave
    ports:
      - "10205:9100"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: '0.1'
    restart: unless-stopped

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: sutazai-redis-exporter
    environment:
      REDIS_ADDR: redis:6379
    ports:
      - "10207:9121"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9121/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
    restart: unless-stopped
    depends_on:
      - redis

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: sutazai-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "10206:8080"
    networks:
      - sutazai-network
    cap_add:
      - SYS_ADMIN
      - SYS_RESOURCE
      - SYS_TIME
    security_opt:
      - no-new-privileges:true
    devices:
      - /dev/kmsg
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: '0.1'
    restart: unless-stopped

  # ===================================
  # MCP ORCHESTRATION (Docker-in-Docker)
  # ===================================
  
  mcp-orchestrator:
    image: docker:25.0.5-dind-alpine3.19
    container_name: sutazai-mcp-orchestrator
    privileged: true
    environment:
      DOCKER_TLS_CERTDIR: /certs
      DOCKER_DRIVER: overlay2
      DOCKER_STORAGE_DRIVER: overlay2
      MCP_ORCHESTRATOR_PORT: 2376
      MCP_API_PORT: 8080
      MCP_METRICS_PORT: 9090
    command:
      - dockerd
      - -H
      - tcp://0.0.0.0:2376
      - -H
      - tcp://0.0.0.0:2375
    volumes:
      - mcp-docker-certs-ca:/certs/ca
      - mcp-docker-certs-client:/certs/client
      - mcp-orchestrator-data:/var/lib/docker
      - mcp-docker-socket:/var/run
      - ./docker/dind/orchestrator/mcp-manifests:/mcp-manifests:ro
      - ./docker/dind/orchestrator/scripts:/orchestrator-scripts:ro
      - ./docker/dind/orchestrator/configs:/orchestrator-configs:ro
      - mcp-shared-data:/mcp-shared
      - mcp-logs:/var/log/mcp
    ports:
      - "12376:2376"  # Docker daemon API (TLS)
      - "12375:2375"  # Docker daemon API (no TLS, for orchestrator)
      - "18080:8080"  # MCP Orchestrator API
      - "19090:9090"  # MCP Metrics endpoint
    networks:
      - sutazai-network
      - sutazai-dind-internal
    healthcheck:
      test: ["CMD", "sh", "-c", "DOCKER_HOST=tcp://localhost:2375 docker version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G
    restart: unless-stopped

  mcp-manager:
    build:
      context: ./docker/dind/orchestrator/manager
      dockerfile: Dockerfile
    image: sutazai-mcp-manager:v1.0.0
    container_name: sutazai-mcp-manager
    depends_on:
      mcp-orchestrator:
        condition: service_healthy
    environment:
      DOCKER_HOST: tcp://mcp-orchestrator:2375
      MCP_REGISTRY_URL: http://mcp-orchestrator:8080
      MESH_GATEWAY_URL: http://host.docker.internal:10005
      CONSUL_URL: http://host.docker.internal:10006
    volumes:
      - ./docker/dind/orchestrator/manager/app:/app:ro
      - mcp-manager-logs:/var/log/manager
      - mcp-docker-socket:/var/run:rw
    ports:
      - "18081:8081"  # MCP Manager Web UI
    networks:
      - sutazai-network
      - sutazai-dind-internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    restart: unless-stopped

# ===================================
# NETWORKS
# ===================================

networks:
  sutazai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
  
  sutazai-dind-internal:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.30.0.0/16

# ===================================
# VOLUMES
# ===================================

volumes:
  # Database volumes
  postgres-data:
  redis-data:
  neo4j-data:
  neo4j-logs:
  
  # AI/Vector volumes
  chromadb-data:
  qdrant-data:
  faiss-data:
  ollama-models:
  
  # Messaging volumes
  rabbitmq-data:
  consul-data:
  
  # Monitoring volumes
  prometheus-data:
  grafana-data:
  loki-data:
  alertmanager-data:
  
  # Application volumes
  backend-logs:
  
  # MCP volumes
  mcp-docker-certs-ca:
  mcp-docker-certs-client:
  mcp-orchestrator-data:
  mcp-docker-socket:
  mcp-shared-data:
  mcp-logs:
  mcp-manager-logs: