# Version field removed as it's obsolete in modern Docker Compose

x-common-variables: &common-variables
  TZ: ${TZ:-UTC}

x-gpu-config: &gpu-config
  deploy:
    resources:
      limits:
        cpus: '4'
        memory: 8G

services:
  # Core Infrastructure
  postgres:
    image: postgres:16.3-alpine
    container_name: sutazai-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-sutazai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai_password}
      POSTGRES_DB: ${POSTGRES_DB:-sutazai}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=trust"
      POSTGRES_HOST_AUTH_METHOD: scram-sha-256
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c log_statement=none
      -c log_duration=off
      -c log_lock_waits=on
      -c log_min_duration_statement=1000
      -c idle_in_transaction_session_timeout=300000
      -c tcp_keepalives_idle=600
      -c tcp_keepalives_interval=30
      -c tcp_keepalives_count=3
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-sutazai} -d ${POSTGRES_DB:-sutazai}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - sutazai-network

  redis:
    image: redis:7.2-alpine
    container_name: sutazai-redis
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-redis_password}
      --protected-mode yes
      --port 6379
      --tcp-backlog 511
      --timeout 0
      --tcp-keepalive 300
      --maxclients 10000
      --save 900 1
      --save 300 10
      --save 60 10000
      --stop-writes-on-bgsave-error yes
      --rdbcompression yes
      --rdbchecksum yes
      --dbfilename dump.rdb
      --dir /data
      --appendonly yes
      --appendfilename appendonly.aof
      --appendfsync everysec
      --no-appendfsync-on-rewrite no
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
      --aof-load-truncated yes
      --rename-command FLUSHDB ""
      --rename-command FLUSHALL ""
      --rename-command DEBUG ""
      --rename-command CONFIG ""
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-redis_password}", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - sutazai-network

  # Vector Databases
  chromadb:
    image: chromadb/chroma:0.5.0
    container_name: sutazai-chromadb
    restart: unless-stopped
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_AUTH_PROVIDER=chromadb.auth.token.TokenAuthenticationServerProvider
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMADB_API_KEY:-test-token}
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
    ports:
      - "8001:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - sutazai-network

  qdrant:
    image: qdrant/qdrant:v1.9.2
    container_name: sutazai-qdrant
    restart: unless-stopped
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__LOG_LEVEL: INFO
    ports:
      - "6333:6333"
      - "6334:6334"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - sutazai-network

  # Model Serving
  ollama:
    image: ollama/ollama:latest
    container_name: sutazai-ollama
    restart: unless-stopped
    <<: *gpu-config
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS="*"
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend Services
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: sutazai-backend
    restart: unless-stopped
    volumes:
      - ./backend:/app
      - ./data:/data
      - ./logs:/logs
    environment:
      <<: *common-variables
      DATABASE_URL: postgresql://${POSTGRES_USER:-sutazai}:${POSTGRES_PASSWORD:-sutazai_password}@postgres:5432/${POSTGRES_DB:-sutazai}
      REDIS_URL: redis://:${REDIS_PASSWORD:-redis_password}@redis:6379/0
      OLLAMA_URL: http://ollama:11434
      CHROMADB_URL: http://chromadb:8000
      QDRANT_URL: http://qdrant:6333
      # Additional host configurations
      POSTGRES_HOST: postgres
      POSTGRES_USER: ${POSTGRES_USER:-sutazai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai_password}
      POSTGRES_DB: ${POSTGRES_DB:-sutazai}
      REDIS_HOST: redis
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
      CHROMADB_HOST: chromadb
      QDRANT_HOST: qdrant
      SECRET_KEY: ${SECRET_KEY:-dev-secret-key-change-in-production}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
      chromadb:
        condition: service_healthy
      qdrant:
        condition: service_started
    networks:
      - sutazai-network
    command: uvicorn app.working_main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: sutazai-frontend
    restart: unless-stopped
    volumes:
      - ./frontend:/app
      - ./data:/data
    environment:
      <<: *common-variables
      BACKEND_URL: http://backend:8000
      STREAMLIT_SERVER_PORT: 8501
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
    ports:
      - "8501:8501"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    command: streamlit run app.py --server.port 8501 --server.address 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    container_name: sutazai-prometheus
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - sutazai-network

  grafana:
    image: grafana/grafana:latest
    container_name: sutazai-grafana
    restart: unless-stopped
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - loki
    networks:
      - sutazai-network

  loki:
    image: grafana/loki:2.9.0
    container_name: sutazai-loki
    restart: unless-stopped
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki_data:/loki
      - ./monitoring/loki/config.yml:/etc/loki/local-config.yaml
    networks:
      - sutazai-network

  promtail:
    image: grafana/promtail:2.9.0
    container_name: sutazai-promtail
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail/config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - sutazai-network

  # Neo4j Graph Database
  neo4j:
    image: neo4j:5.15.0
    container_name: sutazai-neo4j
    restart: unless-stopped
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-neo4j_password}
      NEO4J_PLUGINS: '["graph-data-science","apoc"]'
      NEO4J_dbms_security_procedures_unrestricted: gds.*,apoc.*
      NEO4J_dbms_security_procedures_allowlist: gds.*,apoc.*
      NEO4J_dbms_memory_heap_initial__size: 512m
      NEO4J_dbms_memory_heap_max__size: 2G
      NEO4J_dbms_memory_pagecache_size: 1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    ports:
      - "7474:7474"
      - "7687:7687"
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "${NEO4J_PASSWORD:-neo4j_password}", "MATCH () RETURN count(*) as count"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - sutazai-network

  # AI Agents
  autogpt:
    build:
      context: ./docker/autogpt
      dockerfile: Dockerfile
    container_name: sutazai-autogpt
    restart: unless-stopped
    volumes:
      - ./data/workspaces/autogpt:/workspace
      - autogpt_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8080:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  crewai:
    build:
      context: ./docker/crewai
      dockerfile: Dockerfile
    container_name: sutazai-crewai
    restart: unless-stopped
    volumes:
      - ./data/workspaces/crewai:/workspace
      - crewai_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8081:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  aider:
    build:
      context: ./docker/aider
      dockerfile: Dockerfile
    container_name: sutazai-aider
    restart: unless-stopped
    volumes:
      - ./data/workspaces/aider:/workspace
      - aider_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8082:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  gpt-engineer:
    build:
      context: ./docker/gpt-engineer
      dockerfile: Dockerfile
    container_name: sutazai-gpt-engineer
    restart: unless-stopped
    volumes:
      - ./data/workspaces/gpt-engineer:/workspace
      - gpteng_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8083:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  letta:
    build:
      context: ./docker/letta
      dockerfile: Dockerfile
    container_name: sutazai-letta
    restart: unless-stopped
    volumes:
      - ./data/workspaces/letta:/workspace
      - letta_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8084:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  browser-use:
    build:
      context: ./docker/browser-use
      dockerfile: Dockerfile
    container_name: sutazai-browser-use
    restart: unless-stopped
    volumes:
      - ./data/workspaces/browser-use:/workspace
      - browseruse_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8085:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  langchain-agents:
    build:
      context: ./docker/langchain-agents
      dockerfile: Dockerfile
    container_name: sutazai-langchain
    restart: unless-stopped
    volumes:
      - ./data/workspaces/langchain:/workspace
      - langchain_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8086:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  llamaindex:
    build:
      context: ./docker/llamaindex
      dockerfile: Dockerfile
    container_name: sutazai-llamaindex
    restart: unless-stopped
    volumes:
      - ./data/workspaces/llamaindex:/workspace
      - llamaindex_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8087:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  privategpt:
    build:
      context: ./docker/privategpt
      dockerfile: Dockerfile
    container_name: sutazai-privategpt
    restart: unless-stopped
    volumes:
      - ./data/workspaces/privategpt:/workspace
      - privategpt_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8088:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  skyvern:
    build:
      context: ./docker/skyvern
      dockerfile: Dockerfile
    container_name: sutazai-skyvern
    restart: unless-stopped
    volumes:
      - ./data/workspaces/skyvern:/workspace
      - skyvern_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8089:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  tabbyml:
    build:
      context: ./docker/tabbyml
      dockerfile: Dockerfile
    container_name: sutazai-tabbyml
    restart: unless-stopped
    volumes:
      - ./data/workspaces/tabbyml:/workspace
      - tabbyml_data:/data
    environment:
      <<: *common-variables
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      WORKSPACE_PATH: /workspace
    ports:
      - "8090:8080"
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_healthy
    networks:
      - sutazai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  sutazai-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  chromadb_data:
  qdrant_data:
  ollama_data:
  prometheus_data:
  grafana_data:
  loki_data:
  neo4j_data:
  neo4j_logs:
  autogpt_data:
  crewai_data:
  aider_data:
  gpteng_data:
  letta_data:
  browseruse_data:
  langchain_data:
  llamaindex_data:
  privategpt_data:
  skyvern_data:
  tabbyml_data: