version: '3.8'
networks:
  sutazai-minimal:
    driver: bridge
volumes:
  postgres_data: null
  redis_data: null
  ollama_data: null
services:
  postgres:
    image: postgres:16-alpine
    container_name: sutazai-postgres-minimal
    environment:
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: sutazai123
      POSTGRES_DB: sutazai_db
      POSTGRES_INITDB_ARGS: --encoding=UTF8 --data-checksums
    volumes:
    - postgres_data:/var/lib/postgresql/data
    ports:
    - 5432:5432
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test:
      - CMD-SHELL
      - pg_isready -U sutazai
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
    - sutazai-minimal
  redis:
    image: redis:7-alpine
    container_name: sutazai-redis-minimal
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
    - redis_data:/data
    ports:
    - 6379:6379
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test:
      - CMD
      - redis-cli
      - ping
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
    - sutazai-minimal
  ollama:
    image: ollama/ollama:latest
    container_name: sutazai-ollama-minimal
    volumes:
    - ollama_data:/root/.ollama
    - /dev/shm:/dev/shm:rw
    ports:
    - 11434:11434
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_ORIGINS: '*'
      OLLAMA_KEEP_ALIVE: 5m
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_NUM_PARALLEL: 2
      OLLAMA_MAX_QUEUE: 4
      OLLAMA_NUM_THREADS: 4
      OLLAMA_MMAP: 1
      OLLAMA_LLM_LIBRARY: cpu_avx2
      OLLAMA_KV_CACHE_TYPE: q8_0
      OLLAMA_FLASH_ATTENTION: 1
      GOMAXPROCS: 4
      GOMEMLIMIT: 3GiB
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    cpu_shares: 512
    mem_swappiness: 10
    healthcheck:
      test:
      - CMD-SHELL
      - ollama list || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
    - sutazai-minimal
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.minimal
    container_name: sutazai-backend-minimal
    environment:
      DATABASE_URL: postgresql://sutazai:sutazai123@postgres:5432/sutazai_db
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: sutazai123
      POSTGRES_DB: sutazai_db
      REDIS_URL: redis://redis:6379/0
      REDIS_HOST: redis
      REDIS_PORT: 6379
      OLLAMA_BASE_URL: http://ollama:11434
      CHROMADB_HOST: chromadb
      QDRANT_HOST: qdrant
      API_V1_STR: /api/v1
      SECRET_KEY: minimal-secret-key
      BACKEND_CORS_ORIGINS: '["*"]'
      LOG_LEVEL: INFO
    volumes:
    - ./backend:/app
    - ./data:/data
    ports:
    - 8000:8000
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    depends_on:
    - postgres
    - redis
    - ollama
    healthcheck:
      test:
      - CMD
      - curl
      - -f
      - http://localhost:8000/health
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
    - sutazai-minimal
  code-improver:
    build:
      context: ./agents/code-generation-improver
      dockerfile: Dockerfile
    container_name: sutazai-code-improver
    environment:
      AGENT_NAME: code-generation-improver
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      MODEL_NAME: tinyllama
      MAX_TOKENS: 2048
      TEMPERATURE: 0.7
    volumes:
    - ./workspace:/workspace
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    depends_on:
    - backend
    - ollama
    networks:
    - sutazai-minimal
  qa-validator:
    build:
      context: ./agents/testing-qa-validator
      dockerfile: Dockerfile
    container_name: sutazai-qa-validator
    environment:
      AGENT_NAME: testing-qa-validator
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      MODEL_NAME: tinyllama
      MAX_TOKENS: 2048
    volumes:
    - ./workspace:/workspace
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    depends_on:
    - backend
    - ollama
    networks:
    - sutazai-minimal
  ai-engineer:
    build:
      context: ./agents/senior-ai-engineer
      dockerfile: Dockerfile
    container_name: sutazai-ai-engineer
    environment:
      AGENT_NAME: senior-ai-engineer
      OLLAMA_URL: http://ollama:11434
      BACKEND_URL: http://backend:8000
      MODEL_NAME: tinyllama
      MAX_TOKENS: 2048
      FOCUS: code_analysis,optimization,ai_integration
    volumes:
    - ./workspace:/workspace
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    depends_on:
    - backend
    - ollama
    networks:
    - sutazai-minimal
  frontend:
    image: python:3.11-slim
    container_name: sutazai-frontend-minimal
    working_dir: /app
    command: "sh -c \"\n  pip install --no-cache-dir streamlit requests\n  streamlit\
      \ run minimal_app.py --server.port=8501 --server.address=0.0.0.0\n\"\n"
    volumes:
    - ./frontend:/app
    ports:
    - 8501:8501
    environment:
      BACKEND_URL: http://backend:8000
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    depends_on:
    - backend
    networks:
    - sutazai-minimal
  init-models:
    image: ollama/ollama:latest
    container_name: sutazai-init-models
    command: "sh -c \"\n  echo 'Waiting for Ollama to be ready...'\n  until curl -s\
      \ http://ollama:11434/api/tags > /dev/null; do\n    sleep 2\n  done\n  echo\
      \ 'Pulling TinyLlama model...'\n  ollama pull tinyllama:latest || true\n  echo\
      \ 'Model initialization complete'\n\"\n"
    depends_on:
    - ollama
    networks:
    - sutazai-minimal
    deploy:
      replicas: 0
