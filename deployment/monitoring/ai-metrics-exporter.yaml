# Enhanced AI Metrics Exporter for SutazAI auto-scaling
# Exports custom metrics for AI workloads, inference latency, and resource utilization

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-metrics-exporter
  namespace: sutazai-monitoring
  labels:
    app: ai-metrics-exporter
    component: monitoring
    tier: metrics
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ai-metrics-exporter
  template:
    metadata:
      labels:
        app: ai-metrics-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9200"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: ai-metrics-exporter
        image: python:3.11-slim
        ports:
        - name: metrics
          containerPort: 9200
        env:
        - name: OLLAMA_BASE_URL
          value: "http://sutazai-ollama.sutazai-core:10104"
        - name: BACKEND_URL
          value: "http://sutazai-backend.sutazai-core:8000"
        - name: CHROMADB_URL
          value: "http://sutazai-chromadb.sutazai-infrastructure:8000"
        - name: QDRANT_URL
          value: "http://sutazai-qdrant.sutazai-infrastructure:6333"
        - name: POSTGRES_URL
          valueFrom:
            secretKeyRef:
              name: sutazai-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: sutazai-secrets
              key: redis-url
        - name: METRICS_PORT
          value: "9200"
        - name: COLLECTION_INTERVAL
          value: "15"
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        volumeMounts:
        - name: exporter-script
          mountPath: /app
        - name: config
          mountPath: /config
        command: ["python", "/app/ai_metrics_exporter.py"]
        livenessProbe:
          httpGet:
            path: /health
            port: 9200
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /metrics
            port: 9200
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: exporter-script
        configMap:
          name: ai-metrics-exporter-script
          defaultMode: 0755
      - name: config
        configMap:
          name: ai-metrics-exporter-config

---
# AI Metrics Exporter Service
apiVersion: v1
kind: Service
metadata:
  name: ai-metrics-exporter
  namespace: sutazai-monitoring
  labels:
    app: ai-metrics-exporter
    component: monitoring
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9200"
spec:
  ports:
  - name: metrics
    port: 9200
    targetPort: 9200
  selector:
    app: ai-metrics-exporter

---
# ConfigMap with AI Metrics Exporter Script
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-metrics-exporter-script
  namespace: sutazai-monitoring
data:
  ai_metrics_exporter.py: |
    #!/usr/bin/env python3
    """
    Enhanced AI Metrics Exporter for SutazAI Auto-scaling
    Exports comprehensive metrics for AI workloads and infrastructure
    """
    
    import asyncio
    import json
    import logging
    import os
    import time
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional
    
    import aiohttp
    import psutil
    import redis
    from prometheus_client import (
        CollectorRegistry, Counter, Gauge, Histogram, Info,
        generate_latest, start_http_server
    )
    from aiohttp import web
    import psycopg2
    from psycopg2.extras import DictCursor
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)
    
    # Prometheus metrics registry
    registry = CollectorRegistry()
    
    # AI Inference Metrics
    inference_requests_total = Counter(
        'sutazai_inference_requests_total',
        'Total AI inference requests',
        ['model', 'status', 'service'],
        registry=registry
    )
    
    inference_duration_seconds = Histogram(
        'sutazai_inference_duration_seconds',
        'AI inference duration in seconds',
        ['model', 'service'],
        buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
        registry=registry
    )
    
    inference_queue_depth = Gauge(
        'sutazai_inference_queue_depth',
        'Current AI inference queue depth',
        ['service'],
        registry=registry
    )
    
    model_memory_usage_bytes = Gauge(
        'sutazai_model_memory_usage_bytes',
        'Memory usage by AI models',
        ['model', 'service'],
        registry=registry
    )
    
    # Agent Metrics
    agent_task_queue_depth = Gauge(
        'sutazai_agent_task_queue_depth',
        'Agent task queue depth',
        ['agent_name', 'agent_type'],
        registry=registry
    )
    
    agent_active_tasks = Gauge(
        'sutazai_agent_active_tasks',
        'Number of active agent tasks',
        ['agent_name', 'agent_type'],
        registry=registry
    )
    
    agent_task_duration_seconds = Histogram(
        'sutazai_agent_task_duration_seconds',
        'Agent task execution duration',
        ['agent_name', 'agent_type', 'task_type'],
        buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0, 1800.0],
        registry=registry
    )
    
    # Vector Database Metrics
    vector_search_latency_seconds = Histogram(
        'sutazai_vector_search_latency_seconds',
        'Vector database search latency',
        ['database', 'collection'],
        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],
        registry=registry
    )
    
    vector_collection_size = Gauge(
        'sutazai_vector_collection_size',
        'Number of vectors in collection',
        ['database', 'collection'],
        registry=registry
    )
    
    vector_index_memory_bytes = Gauge(
        'sutazai_vector_index_memory_bytes',
        'Memory used by vector indexes',
        ['database', 'collection'],
        registry=registry
    )
    
    # System Resource Metrics
    system_cpu_usage_percent = Gauge(
        'sutazai_system_cpu_usage_percent',
        'System CPU usage percentage',
        ['instance'],
        registry=registry
    )
    
    system_memory_usage_bytes = Gauge(
        'sutazai_system_memory_usage_bytes',
        'System memory usage in bytes',
        ['instance', 'type'],
        registry=registry
    )
    
    system_disk_usage_bytes = Gauge(
        'sutazai_system_disk_usage_bytes',
        'System disk usage in bytes',
        ['instance', 'device', 'type'],
        registry=registry
    )
    
    # Custom Business Metrics
    user_sessions_active = Gauge(
        'sutazai_user_sessions_active',
        'Number of active user sessions',
        ['interface'],
        registry=registry
    )
    
    workflow_executions_total = Counter(
        'sutazai_workflow_executions_total',
        'Total workflow executions',
        ['workflow_type', 'status'],
        registry=registry
    )
    
    # Service Health Metrics
    service_up = Gauge(
        'sutazai_service_up',
        'Service availability (1 = up, 0 = down)',
        ['service_name', 'namespace'],
        registry=registry
    )
    
    service_response_time_seconds = Histogram(
        'sutazai_service_response_time_seconds',
        'Service response time',
        ['service_name', 'endpoint'],
        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],
        registry=registry
    )
    
    class AIMetricsExporter:
        def __init__(self):
            self.ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://ollama:10104')
            self.backend_url = os.getenv('BACKEND_URL', 'http://backend:8000')
            self.chromadb_url = os.getenv('CHROMADB_URL', 'http://chromadb:8000')
            self.qdrant_url = os.getenv('QDRANT_URL', 'http://qdrant:6333')
            self.postgres_url = os.getenv('POSTGRES_URL')
            self.redis_url = os.getenv('REDIS_URL')
            self.collection_interval = int(os.getenv('COLLECTION_INTERVAL', '30'))
            
            self.session = None
            self.redis_client = None
            self.postgres_conn = None
            
        async def initialize(self):
            """Initialize connections"""
            try:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=30)
                )
                
                if self.redis_url:
                    self.redis_client = redis.from_url(self.redis_url)
                
                if self.postgres_url:
                    self.postgres_conn = psycopg2.connect(self.postgres_url)
                    
                logger.info("AI Metrics Exporter initialized")
            except Exception as e:
                logger.error(f"Failed to initialize: {e}")
                raise
    
        async def collect_ollama_metrics(self):
            """Collect Ollama AI inference metrics"""
            try:
                # Get running models
                async with self.session.get(f"{self.ollama_url}/api/tags") as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        models = data.get('models', [])
                        
                        for model in models:
                            model_name = model.get('name', 'unknown')
                            size = model.get('size', 0)
                            model_memory_usage_bytes.labels(
                                model=model_name, 
                                service='ollama'
                            ).set(size)
                
                # Get model status and queue info (if available)
                async with self.session.get(f"{self.ollama_url}/api/show") as resp:
                    if resp.status == 200:
                        # This would need custom Ollama metrics endpoint
                        pass
                        
            except Exception as e:
                logger.error(f"Failed to collect Ollama metrics: {e}")
                service_up.labels(service_name='ollama', namespace='sutazai-core').set(0)
            else:
                service_up.labels(service_name='ollama', namespace='sutazai-core').set(1)
    
        async def collect_backend_metrics(self):
            """Collect backend API metrics"""
            try:
                start_time = time.time()
                async with self.session.get(f"{self.backend_url}/health") as resp:
                    response_time = time.time() - start_time
                    service_response_time_seconds.labels(
                        service_name='backend',
                        endpoint='/health'
                    ).observe(response_time)
                    
                    if resp.status == 200:
                        service_up.labels(
                            service_name='backend', 
                            namespace='sutazai-core'
                        ).set(1)
                        
                        # Get custom metrics if endpoint exists
                        try:
                            async with self.session.get(f"{self.backend_url}/metrics") as metrics_resp:
                                if metrics_resp.status == 200:
                                    metrics_data = await metrics_resp.json()
                                    # Process custom backend metrics
                                    self._process_backend_metrics(metrics_data)
                        except:
                            pass
                    else:
                        service_up.labels(
                            service_name='backend', 
                            namespace='sutazai-core'
                        ).set(0)
                        
            except Exception as e:
                logger.error(f"Failed to collect backend metrics: {e}")
                service_up.labels(service_name='backend', namespace='sutazai-core').set(0)
    
        async def collect_vector_db_metrics(self):
            """Collect vector database metrics"""
            # ChromaDB metrics
            try:
                start_time = time.time()
                async with self.session.get(f"{self.chromadb_url}/api/v1/heartbeat") as resp:
                    response_time = time.time() - start_time
                    service_response_time_seconds.labels(
                        service_name='chromadb',
                        endpoint='/heartbeat'
                    ).observe(response_time)
                    
                    if resp.status == 200:
                        service_up.labels(
                            service_name='chromadb', 
                            namespace='sutazai-infrastructure'
                        ).set(1)
                        
                        # Get collections info
                        async with self.session.get(f"{self.chromadb_url}/api/v1/collections") as coll_resp:
                            if coll_resp.status == 200:
                                collections = await coll_resp.json()
                                for collection in collections:
                                    name = collection.get('name', 'unknown')
                                    # Estimate collection size (would need custom endpoint)
                                    vector_collection_size.labels(
                                        database='chromadb',
                                        collection=name
                                    ).set(collection.get('count', 0))
                    else:
                        service_up.labels(
                            service_name='chromadb', 
                            namespace='sutazai-infrastructure'
                        ).set(0)
                        
            except Exception as e:
                logger.error(f"Failed to collect ChromaDB metrics: {e}")
                service_up.labels(service_name='chromadb', namespace='sutazai-infrastructure').set(0)
            
            # Qdrant metrics
            try:
                start_time = time.time()
                async with self.session.get(f"{self.qdrant_url}/") as resp:
                    response_time = time.time() - start_time
                    service_response_time_seconds.labels(
                        service_name='qdrant',
                        endpoint='/'
                    ).observe(response_time)
                    
                    if resp.status == 200:
                        service_up.labels(
                            service_name='qdrant', 
                            namespace='sutazai-infrastructure'
                        ).set(1)
                        
                        # Get collections info
                        async with self.session.get(f"{self.qdrant_url}/collections") as coll_resp:
                            if coll_resp.status == 200:
                                data = await coll_resp.json()
                                collections = data.get('result', {}).get('collections', [])
                                for collection in collections:
                                    name = collection.get('name', 'unknown')
                                    vectors_count = collection.get('vectors_count', 0)
                                    vector_collection_size.labels(
                                        database='qdrant',
                                        collection=name
                                    ).set(vectors_count)
                    else:
                        service_up.labels(
                            service_name='qdrant', 
                            namespace='sutazai-infrastructure'
                        ).set(0)
                        
            except Exception as e:
                logger.error(f"Failed to collect Qdrant metrics: {e}")
                service_up.labels(service_name='qdrant', namespace='sutazai-infrastructure').set(0)
    
        def collect_system_metrics(self):
            """Collect system resource metrics"""
            try:
                # CPU usage
                cpu_percent = psutil.cpu_percent(interval=1)
                system_cpu_usage_percent.labels(instance='localhost').set(cpu_percent)
                
                # Memory usage
                memory = psutil.virtual_memory()
                system_memory_usage_bytes.labels(
                    instance='localhost', 
                    type='used'
                ).set(memory.used)
                system_memory_usage_bytes.labels(
                    instance='localhost', 
                    type='total'
                ).set(memory.total)
                system_memory_usage_bytes.labels(
                    instance='localhost', 
                    type='available'
                ).set(memory.available)
                
                # Disk usage
                for partition in psutil.disk_partitions():
                    try:
                        usage = psutil.disk_usage(partition.mountpoint)
                        system_disk_usage_bytes.labels(
                            instance='localhost',
                            device=partition.device,
                            type='used'
                        ).set(usage.used)
                        system_disk_usage_bytes.labels(
                            instance='localhost',
                            device=partition.device,
                            type='total'
                        ).set(usage.total)
                        system_disk_usage_bytes.labels(
                            instance='localhost',
                            device=partition.device,
                            type='free'
                        ).set(usage.free)
                    except PermissionError:
                        continue
                        
            except Exception as e:
                logger.error(f"Failed to collect system metrics: {e}")
    
        def _process_backend_metrics(self, metrics_data: dict):
            """Process custom backend metrics"""
            try:
                if 'active_sessions' in metrics_data:
                    user_sessions_active.labels(interface='streamlit').set(
                        metrics_data['active_sessions']
                    )
                
                if 'agent_queues' in metrics_data:
                    for agent_name, queue_depth in metrics_data['agent_queues'].items():
                        agent_task_queue_depth.labels(
                            agent_name=agent_name,
                            agent_type='ai_agent'
                        ).set(queue_depth)
                
                if 'workflow_stats' in metrics_data:
                    for workflow_type, stats in metrics_data['workflow_stats'].items():
                        workflow_executions_total.labels(
                            workflow_type=workflow_type,
                            status='completed'
                        )._value._value = stats.get('completed', 0)
                        workflow_executions_total.labels(
                            workflow_type=workflow_type,
                            status='failed'
                        )._value._value = stats.get('failed', 0)
                        
            except Exception as e:
                logger.error(f"Failed to process backend metrics: {e}")
    
        async def collect_all_metrics(self):
            """Collect all metrics"""
            logger.info("Collecting AI metrics...")
            
            tasks = [
                self.collect_ollama_metrics(),
                self.collect_backend_metrics(),
                self.collect_vector_db_metrics(),
            ]
            
            # Run async collections concurrently
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # Run sync collections
            self.collect_system_metrics()
            
            logger.info("Metrics collection completed")
    
        async def metrics_collection_loop(self):
            """Main metrics collection loop"""
            while True:
                try:
                    await self.collect_all_metrics()
                    await asyncio.sleep(self.collection_interval)
                except Exception as e:
                    logger.error(f"Error in metrics collection loop: {e}")
                    await asyncio.sleep(self.collection_interval)
    
        async def start_http_server(self):
            """Start HTTP server for metrics endpoint"""
            app = web.Application()
            app.router.add_get('/metrics', self.metrics_handler)
            app.router.add_get('/health', self.health_handler)
            
            runner = web.AppRunner(app)
            await runner.setup()
            
            port = int(os.getenv('METRICS_PORT', '9200'))
            site = web.TCPSite(runner, '0.0.0.0', port)
            await site.start()
            
            logger.info(f"Metrics server started on port {port}")
            
        async def metrics_handler(self, request):
            """Handle metrics requests"""
            metrics_output = generate_latest(registry)
            return web.Response(text=metrics_output.decode('utf-8'), 
                              content_type='text/plain')
            
        async def health_handler(self, request):
            """Handle health check requests"""
            return web.json_response({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()})
    
        async def cleanup(self):
            """Cleanup resources"""
            if self.session:
                await self.session.close()
            if self.postgres_conn:
                self.postgres_conn.close()
    
    async def main():
        exporter = AIMetricsExporter()
        await exporter.initialize()
        
        try:
            # Start HTTP server
            await exporter.start_http_server()
            
            # Start metrics collection loop
            await exporter.metrics_collection_loop()
            
        except KeyboardInterrupt:
            logger.info("Shutting down...")
        finally:
            await exporter.cleanup()
    
    if __name__ == '__main__':
        asyncio.run(main())

---
# ConfigMap for AI Metrics Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-metrics-exporter-config
  namespace: sutazai-monitoring
data:
  config.yaml: |
    collection:
      interval: 15
      timeout: 30
      retry_attempts: 3
      
    services:
      ollama:
        url: "http://sutazai-ollama.sutazai-core:10104"
        endpoints:
          - "/api/tags"
          - "/api/show"
        metrics:
          - "model_memory_usage"
          - "inference_queue_depth"
          - "inference_latency"
          
      backend:
        url: "http://sutazai-backend.sutazai-core:8000"
        endpoints:
          - "/health"
          - "/metrics"
        metrics:
          - "active_sessions"
          - "agent_queues"
          - "response_time"
          
      chromadb:
        url: "http://sutazai-chromadb.sutazai-infrastructure:8000"
        endpoints:
          - "/api/v1/heartbeat"
          - "/api/v1/collections"
        metrics:
          - "collection_size"
          - "search_latency"
          
      qdrant:
        url: "http://sutazai-qdrant.sutazai-infrastructure:6333"
        endpoints:
          - "/"
          - "/collections"
        metrics:
          - "collection_size"
          - "search_latency"
    
    thresholds:
      cpu_warning: 70
      cpu_critical: 90
      memory_warning: 80
      memory_critical: 95
      inference_latency_warning: 5.0
      inference_latency_critical: 10.0
      queue_depth_warning: 10
      queue_depth_critical: 25