name: Jarvis Testing Suite

on:
  push:
    branches: [ main, develop, v59 ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'services/jarvis/**'
      - 'agents/jarvis-*/**'
      - 'docs/testing/**'
      - '.github/workflows/jarvis-tests.yml'
      - 'docker-compose.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'services/jarvis/**'
      - 'agents/jarvis-*/**'
      - 'docs/testing/**'
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'api'
        type: choice
        options:
          - api
          - e2e
          - load
          - all
      environment:
        description: 'Target environment'
        required: false
        default: 'local'
        type: choice
        options:
          - local
          - staging
          - production

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  # Pre-flight checks
  preflight:
    runs-on: ubuntu-latest
    outputs:
      should_run_tests: ${{ steps.check.outputs.should_run }}
      test_suite: ${{ steps.config.outputs.test_suite }}
      target_environment: ${{ steps.config.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Check if tests should run
        id: check
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]] || [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Set test configuration
        id: config
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "test_suite=${{ inputs.test_suite }}" >> $GITHUB_OUTPUT
            echo "environment=${{ inputs.environment }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "test_suite=api" >> $GITHUB_OUTPUT
            echo "environment=local" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "test_suite=all" >> $GITHUB_OUTPUT
            echo "environment=local" >> $GITHUB_OUTPUT
          else
            echo "test_suite=api" >> $GITHUB_OUTPUT
            echo "environment=local" >> $GITHUB_OUTPUT
          fi

  # API Tests with Newman
  api-tests:
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true' && (needs.preflight.outputs.test_suite == 'api' || needs.preflight.outputs.test_suite == 'all')
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          npm install -g newman newman-reporter-html newman-reporter-htmlextra

      - name: Create Docker network
        run: docker network create sutazai-network 2>/dev/null || true

      - name: Start services
        run: |
          docker-compose up -d postgres redis neo4j ollama chromadb qdrant faiss
          sleep 30
          
      - name: Start backend and jarvis services
        run: |
          docker-compose up -d backend
          docker-compose up -d jarvis-voice-interface jarvis-knowledge-management jarvis-automation-agent jarvis-multimodal-ai jarvis-hardware-resource-optimizer
          sleep 60

      - name: Wait for services to be ready
        run: |
          timeout 300 bash -c '
            until curl -f http://localhost:10010/health >/dev/null 2>&1; do
              echo "Waiting for backend..."
              sleep 5
            done
            echo "Backend is ready"
          '

      - name: Check service health
        run: |
          curl -f http://localhost:10010/health || exit 1
          curl -f http://localhost:11150/health || echo "Voice interface not ready"
          curl -f http://localhost:11101/health || echo "Knowledge management not ready"
          curl -f http://localhost:11102/health || echo "Automation agent not ready"
          curl -f http://localhost:11103/health || echo "Multimodal AI not ready"
          curl -f http://localhost:11104/health || echo "Hardware optimizer not ready"

      - name: Run API tests
        env:
          BASE_URL: http://localhost:10010
          JARVIS_VOICE_URL: http://localhost:11150
          JARVIS_KNOWLEDGE_URL: http://localhost:11101
          JARVIS_AUTOMATION_URL: http://localhost:11102
          JARVIS_MULTIMODAL_URL: http://localhost:11103
          JARVIS_HARDWARE_URL: http://localhost:11104
        run: |
          mkdir -p test-results
          node docs/testing/newman_ci_integration.js

      - name: Upload API test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: api-test-results-${{ github.run_number }}
          path: |
            test-results/
          retention-days: 30

      - name: Upload service logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: service-logs-api-${{ github.run_number }}
          path: |
            logs/
          retention-days: 7

      - name: Comment PR with API test results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const resultsPath = 'test-results/test-summary.json';
              if (fs.existsSync(resultsPath)) {
                const results = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));
                const body = `## 🧪 API Test Results
                
                **Total Tests**: ${results.total}
                **Passed**: ${results.passed} ✅
                **Failed**: ${results.failed} ${results.failed > 0 ? '❌' : '✅'}
                **Duration**: ${Math.round(results.duration / 1000)}s
                
                ${results.failed === 0 ? '🎉 All API tests passed!' : '⚠️ Some API tests failed. Please check the logs.'}
                
                [View detailed results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: body
                });
              }
            } catch (error) {
              console.log('Could not read test results:', error.message);
            }

      - name: Cleanup
        if: always()
        run: docker-compose down

  # E2E Tests with Cypress
  e2e-tests:
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true' && (needs.preflight.outputs.test_suite == 'e2e' || needs.preflight.outputs.test_suite == 'all')
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          npx cypress install

      - name: Create Docker network
        run: docker network create sutazai-network 2>/dev/null || true

      - name: Start services
        run: |
          docker-compose up -d
          sleep 90

      - name: Wait for frontend to be ready
        run: |
          timeout 300 bash -c '
            until curl -f http://localhost:10011 >/dev/null 2>&1; do
              echo "Waiting for frontend..."
              sleep 5
            done
            echo "Frontend is ready"
          '

      - name: Prepare Cypress tests
        run: |
          mkdir -p cypress/e2e
          cp docs/testing/cypress_e2e_tests.js cypress/e2e/jarvis_interface.cy.js

      - name: Run E2E tests
        uses: cypress-io/github-action@v6
        with:
          browser: chrome
          spec: cypress/e2e/jarvis_interface.cy.js
          config: baseUrl=http://localhost:10011
          wait-on: 'http://localhost:10011'
          wait-on-timeout: 300
        env:
          CYPRESS_baseUrl: http://localhost:10011

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results-${{ github.run_number }}
          path: |
            cypress/videos/
            cypress/screenshots/
          retention-days: 30

      - name: Upload service logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: service-logs-e2e-${{ github.run_number }}
          path: |
            logs/
          retention-days: 7

      - name: Cleanup
        if: always()
        run: docker-compose down

  # Load Tests with K6
  load-tests:
    runs-on: ubuntu-latest
    needs: preflight
    if: needs.preflight.outputs.should_run_tests == 'true' && (needs.preflight.outputs.test_suite == 'load' || needs.preflight.outputs.test_suite == 'all') && github.ref == 'refs/heads/main'
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install K6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Create Docker network
        run: docker network create sutazai-network 2>/dev/null || true

      - name: Start services with optimized resources
        run: |
          # Start core services first
          docker-compose up -d postgres redis ollama
          sleep 30
          
          # Start dependent services
          docker-compose up -d backend chromadb qdrant faiss neo4j
          sleep 60
          
          # Start Jarvis services
          docker-compose up -d jarvis-voice-interface jarvis-knowledge-management jarvis-automation-agent jarvis-multimodal-ai jarvis-hardware-resource-optimizer
          sleep 30

      - name: Verify system stability
        run: |
          echo "Checking system resources..."
          free -h
          df -h
          docker stats --no-stream
          
          echo "Checking service health..."
          timeout 180 bash -c '
            until curl -f http://localhost:10010/health >/dev/null 2>&1; do
              echo "Waiting for backend..."
              sleep 10
            done
          '

      - name: Run baseline load test
        env:
          BASE_URL: http://localhost:10010
          JARVIS_VOICE_URL: http://localhost:11150
          JARVIS_KNOWLEDGE_URL: http://localhost:11101
          JARVIS_AUTOMATION_URL: http://localhost:11102
          JARVIS_MULTIMODAL_URL: http://localhost:11103
          JARVIS_HARDWARE_URL: http://localhost:11104
        run: |
          k6 run --scenario baseline_load docs/testing/k6_load_tests.js

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results-${{ github.run_number }}
          path: |
            load-test-report.html
            load-test-results.json
          retention-days: 30

      - name: Upload system logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: system-logs-load-${{ github.run_number }}
          path: |
            logs/
          retention-days: 7

      - name: Cleanup
        if: always()
        run: docker-compose down

  # Test Summary and Reporting
  test-summary:
    runs-on: ubuntu-latest
    needs: [preflight, api-tests, e2e-tests, load-tests]
    if: always() && needs.preflight.outputs.should_run_tests == 'true'
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-test-results

      - name: Generate test summary
        run: |
          echo "# 🧪 Jarvis Testing Suite Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "**Workflow**: ${{ github.workflow }}" >> test-summary.md
          echo "**Run ID**: ${{ github.run_id }}" >> test-summary.md
          echo "**Commit**: ${{ github.sha }}" >> test-summary.md
          echo "**Branch**: ${{ github.ref_name }}" >> test-summary.md
          echo "**Event**: ${{ github.event_name }}" >> test-summary.md
          echo "**Timestamp**: $(date -u)" >> test-summary.md
          echo "" >> test-summary.md
          
          echo "## Test Results" >> test-summary.md
          echo "" >> test-summary.md
          
          # API Tests
          if [[ "${{ needs.api-tests.result }}" == "success" ]]; then
            echo "- ✅ **API Tests**: PASSED" >> test-summary.md
          elif [[ "${{ needs.api-tests.result }}" == "failure" ]]; then
            echo "- ❌ **API Tests**: FAILED" >> test-summary.md
          elif [[ "${{ needs.api-tests.result }}" == "skipped" ]]; then
            echo "- ⏭️ **API Tests**: SKIPPED" >> test-summary.md
          else
            echo "- ⚠️ **API Tests**: ${{ needs.api-tests.result }}" >> test-summary.md
          fi
          
          # E2E Tests
          if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
            echo "- ✅ **E2E Tests**: PASSED" >> test-summary.md
          elif [[ "${{ needs.e2e-tests.result }}" == "failure" ]]; then
            echo "- ❌ **E2E Tests**: FAILED" >> test-summary.md
          elif [[ "${{ needs.e2e-tests.result }}" == "skipped" ]]; then
            echo "- ⏭️ **E2E Tests**: SKIPPED" >> test-summary.md
          else
            echo "- ⚠️ **E2E Tests**: ${{ needs.e2e-tests.result }}" >> test-summary.md
          fi
          
          # Load Tests
          if [[ "${{ needs.load-tests.result }}" == "success" ]]; then
            echo "- ✅ **Load Tests**: PASSED" >> test-summary.md
          elif [[ "${{ needs.load-tests.result }}" == "failure" ]]; then
            echo "- ❌ **Load Tests**: FAILED" >> test-summary.md
          elif [[ "${{ needs.load-tests.result }}" == "skipped" ]]; then
            echo "- ⏭️ **Load Tests**: SKIPPED" >> test-summary.md
          else
            echo "- ⚠️ **Load Tests**: ${{ needs.load-tests.result }}" >> test-summary.md
          fi
          
          echo "" >> test-summary.md
          echo "## Artifacts" >> test-summary.md
          echo "" >> test-summary.md
          
          if [ -d "all-test-results" ]; then
            find all-test-results -type f -name "*.html" -o -name "*.json" -o -name "*.xml" | sort | sed 's/^/- /' >> test-summary.md
          fi
          
          cat test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary-${{ github.run_number }}
          path: test-summary.md
          retention-days: 90

      - name: Slack Notification
        if: github.ref == 'refs/heads/main' && env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          # Determine overall status
          if [[ "${{ needs.api-tests.result }}" == "success" ]] && [[ "${{ needs.e2e-tests.result }}" != "failure" ]] && [[ "${{ needs.load-tests.result }}" != "failure" ]]; then
            STATUS="success"
            EMOJI="✅"
            COLOR="good"
          else
            STATUS="failure"
            EMOJI="❌"
            COLOR="danger"
          fi
          
          # Send Slack notification
          curl -X POST -H 'Content-type: application/json' --data '{
            "attachments": [{
              "color": "'$COLOR'",
              "title": "'$EMOJI' Jarvis Testing Suite Results",
              "fields": [
                {
                  "title": "Branch",
                  "value": "${{ github.ref_name }}",
                  "short": true
                },
                {
                  "title": "Commit",
                  "value": "${{ github.sha }}",
                  "short": true
                },
                {
                  "title": "API Tests",
                  "value": "${{ needs.api-tests.result }}",
                  "short": true
                },
                {
                  "title": "E2E Tests", 
                  "value": "${{ needs.e2e-tests.result }}",
                  "short": true
                }
              ],
              "actions": [{
                "type": "button",
                "text": "View Details",
                "url": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
              }],
              "footer": "GitHub Actions",
              "ts": '$(($(date +%s)))'
            }]
          }' "$SLACK_WEBHOOK_URL" || echo "Slack notification failed"

  # Performance baseline check (only on main branch)
  performance-check:
    runs-on: ubuntu-latest
    needs: [load-tests]
    if: github.ref == 'refs/heads/main' && needs.load-tests.result == 'success'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download load test results
        uses: actions/download-artifact@v4
        with:
          pattern: load-test-results-*
          path: load-results

      - name: Check performance baselines
        run: |
          echo "Performance baseline check would be implemented here"
          echo "This would compare results against historical data"
          echo "and alert if performance has degraded significantly"
          
          # Future implementation:
          # - Parse load-test-results.json
          # - Compare against stored baselines  
          # - Alert if thresholds are exceeded
          # - Update baseline if performance improved