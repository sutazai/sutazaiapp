name: Ollama Integration CI

on:
  push:
    paths:
      - 'agents/ollama_integration/**'
      - 'schemas/ollama_schemas.py'
      - 'tests/test_ollama_integration.py'
  pull_request:
    paths:
      - 'agents/ollama_integration/**'
      - 'schemas/ollama_schemas.py'
      - 'tests/test_ollama_integration.py'

jobs:
  test-ollama-integration:
    runs-on: ubuntu-latest
    
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/tags || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio aiohttp pydantic fastapi uvicorn
    
    - name: Wait for Ollama to be ready
      run: |
        for i in {1..30}; do
          if curl -f http://localhost:11434/api/tags; then
            echo "Ollama is ready"
            break
          fi
          echo "Waiting for Ollama... ($i/30)"
          sleep 2
        done
    
    - name: Pull TinyLlama model
      run: |
        curl -X POST http://localhost:11434/api/pull \
          -d '{"name": "tinyllama:latest"}' \
          -H "Content-Type: application/json"
        
        # Wait for model to be fully loaded
        echo "Waiting for model to load..."
        sleep 10
    
    - name: Verify TinyLlama is loaded
      run: |
        echo "Checking loaded models..."
        MODELS=$(curl -s http://localhost:11434/api/tags | python3 -c "import json, sys; data=json.load(sys.stdin); print([m['name'] for m in data.get('models', [])])")
        echo "Available models: $MODELS"
        
        if [[ "$MODELS" != *"tinyllama"* ]]; then
          echo "ERROR: TinyLlama model not found!"
          exit 1
        fi
        echo "✅ TinyLlama model verified"
    
    - name: Test model generation
      run: |
        echo "Testing generation with TinyLlama..."
        RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
          -d '{"model": "tinyllama:latest", "prompt": "Hello", "stream": false}' \
          -H "Content-Type: application/json")
        
        echo "Response: $RESPONSE"
        
        # Check if response contains text
        if [[ "$RESPONSE" == *"response"* ]]; then
          echo "✅ Generation test passed"
        else
          echo "ERROR: Generation failed"
          exit 1
        fi
    
    - name: Run integration tests
      run: |
        cd /opt/sutazaiapp
        python -m pytest tests/test_ollama_integration.py -v --tb=short
    
    - name: Test CI prompt
      run: |
        python3 -c "
import asyncio
import sys
sys.path.append('/opt/sutazaiapp')

from agents.ollama_integration.app import OllamaIntegrationAgent

async def test_ci():
    async with OllamaIntegrationAgent() as agent:
        # CI test prompt
        result = await agent.generate(
            prompt='What is CI/CD?',
            temperature=0.5,
            max_tokens=100
        )
        
        print(f'Response: {result[\"response\"][:100]}...')
        print(f'Tokens: {result[\"tokens\"]}')
        print(f'Latency: {result[\"latency\"]}ms')
        
        # Assert tokens > 0
        assert result['tokens'] > 0, 'No tokens generated'
        assert len(result['response']) > 0, 'Empty response'
        
        print('✅ CI test prompt passed')
        return True

asyncio.run(test_ci())
"
    
    - name: Performance check
      run: |
        python3 -c "
import asyncio
import time
import sys
sys.path.append('/opt/sutazaiapp')

from agents.ollama_integration.app import OllamaIntegrationAgent

async def perf_test():
    async with OllamaIntegrationAgent() as agent:
        start = time.time()
        
        # Run 3 sequential requests
        for i in range(3):
            result = await agent.generate(
                prompt=f'Test prompt {i}',
                max_tokens=50
            )
            print(f'Request {i+1}: {result[\"latency\"]:.2f}ms')
        
        elapsed = time.time() - start
        print(f'Total time: {elapsed:.2f}s')
        
        # Should complete in reasonable time
        assert elapsed < 30, f'Too slow: {elapsed}s'
        print('✅ Performance check passed')

asyncio.run(perf_test())
"