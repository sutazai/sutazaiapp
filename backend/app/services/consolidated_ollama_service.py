"""
Consolidated Ollama Service - Production-Ready Implementation
Integrates all Ollama functionality: generation, embeddings, model management, caching, and streaming
Implements proper async patterns for LLM operations without blocking the event loop
"""

import asyncio
import json
import logging
import hashlib
import time
import pickle
from pathlib import Path
from typing import Dict, Any, Optional, List, AsyncGenerator, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor

from app.core.connection_pool import get_pool_manager
from app.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class ModelCacheEntry:
    """Model cache entry with metadata"""
    model_name: str
    load_time: datetime
    last_used: datetime
    usage_count: int
    warmup_completed: bool
    cache_size_mb: float
    performance_metrics: Dict[str, float]


@dataclass
class BatchRequest:
    """Batched request for processing"""
    request_id: str
    prompt: str
    model: str
    parameters: Dict[str, Any]
    timestamp: datetime
    future: asyncio.Future


@dataclass
class StreamingResponse:
    """Streaming response chunk"""
    request_id: str
    chunk: str
    is_final: bool
    metadata: Dict[str, Any]


class ConsolidatedOllamaService:
    """
    Consolidated Ollama Service with comprehensive functionality:
    - High-performance async operations with connection pooling
    - Text generation with caching and streaming
    - Embedding generation and similarity calculations
    - Model management (list, pull, load, unload)
    - Multi-model warm caching with LRU eviction
    - Request batching for high-concurrency scenarios
    - GPU acceleration with intelligent fallbacks
    - Performance monitoring and adaptive optimization
    """
    
    def __init__(self):
        # Core async infrastructure
        self._executor = ThreadPoolExecutor(max_workers=10)
        self._processing = False
        self._request_queue = asyncio.Queue(maxsize=100)
        
        # Configuration from settings
        self.ollama_host = getattr(settings, 'OLLAMA_HOST', 'http://sutazai-ollama:11434')
        self.default_model = getattr(settings, 'DEFAULT_MODEL', 'tinyllama')
        self.embedding_model = getattr(settings, 'EMBEDDING_MODEL', 'tinyllama')
        
        # Generation cache
        self._generation_cache = {}
        self._cache_ttl = 3600  # 1 hour cache
        self._max_cache_size = 1000
        
        # Multi-model cache management
        self.warm_cache: Dict[str, ModelCacheEntry] = {}
        self.cache_directory = Path(getattr(settings, 'DATA_DIR', '/tmp/sutazai')) / "model_cache"
        self.cache_directory.mkdir(parents=True, exist_ok=True)
        self.max_cache_size = getattr(settings, 'MODEL_CACHE_SIZE', 3)
        
        # Request batching
        self.batch_queue: Dict[str, List[BatchRequest]] = defaultdict(list)
        self.batch_size = getattr(settings, 'BATCH_SIZE', 8)
        self.batch_timeout_ms = getattr(settings, 'BATCH_TIMEOUT_MS', 100)
        self.batch_processors: Dict[str, asyncio.Task] = {}
        
        # GPU acceleration
        self.gpu_available = self._check_gpu_availability()
        self.device_preference = "cuda" if self.gpu_available else "cpu"
        
        # Performance monitoring
        self._stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0,
            'avg_response_time': 0
        }
        
        # Enhanced performance stats for advanced features
        self.performance_stats = defaultdict(lambda: {
            'total_requests': 0,
            'avg_latency': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'batch_efficiency': 0.0
        })
        
        # Model management
        self.loaded_models: Dict[str, Dict[str, Any]] = {}
        
        # Streaming management
        self.active_streams: Dict[str, asyncio.Queue] = {}
        
        logger.info(f"Consolidated Ollama service initialized - GPU: {self.gpu_available}, Cache: {self.max_cache_size} models")
    
    def _check_gpu_availability(self) -> bool:
        """Check if GPU acceleration is available"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                return result.returncode == 0
            except (subprocess.CalledProcessError, FileNotFoundError):
                return False
    
    async def initialize(self):
        """Initialize the consolidated service and start all workers"""
        self._processing = True
        
        # Initialize cache system
        await self._initialize_cache_system()
        
        # Check Ollama connectivity
        if await self._check_ollama_health():
            logger.info("Ollama connection established")
            
            # Load available models
            await self._discover_models()
            
            # Warm up cache with frequently used models
            await self._warmup_cache()
            
            # Start batch processors
            await self._start_batch_processors()
        else:
            logger.warning("Ollama service not available during initialization")
        
        # Start background processing workers
        for i in range(3):  # 3 concurrent workers
            asyncio.create_task(self._process_queue_worker(i))
        
        logger.info("Consolidated Ollama service fully initialized")
    
    async def _check_ollama_health(self) -> bool:
        """Check if Ollama service is healthy"""
        try:
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                response = await client.get('/api/tags', timeout=5.0)
                return response.status_code == 200
        except Exception as e:
            logger.error(f"Failed to connect to Ollama: {e}")
            return False
    
    async def _initialize_cache_system(self):
        """Initialize the multi-model cache system"""
        logger.info("Initializing multi-model cache system...")
        
        # Load existing cache metadata
        cache_metadata_file = self.cache_directory / "cache_metadata.json"
        if cache_metadata_file.exists():
            try:
                with open(cache_metadata_file, 'r') as f:
                    cache_data = json.load(f)
                    
                for model_name, entry_data in cache_data.items():
                    entry = ModelCacheEntry(
                        model_name=entry_data['model_name'],
                        load_time=datetime.fromisoformat(entry_data['load_time']),
                        last_used=datetime.fromisoformat(entry_data['last_used']),
                        usage_count=entry_data['usage_count'],
                        warmup_completed=entry_data['warmup_completed'],
                        cache_size_mb=entry_data['cache_size_mb'],
                        performance_metrics=entry_data['performance_metrics']
                    )
                    self.warm_cache[model_name] = entry
                    
                logger.info(f"Loaded {len(self.warm_cache)} cached models")
            except Exception as e:
                logger.error(f"Failed to load cache metadata: {e}")
    
    async def _discover_models(self):
        """Discover available models"""
        try:
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                response = await client.get('/api/tags')
                if response.status_code == 200:
                    data = response.json()
                    models = data.get("models", [])
                    
                    # Update cache entries for discovered models
                    for model in models:
                        model_name = model.get("name", "")
                        if model_name and model_name not in self.warm_cache:
                            self.warm_cache[model_name] = ModelCacheEntry(
                                model_name=model_name,
                                load_time=datetime.now(),
                                last_used=datetime.now(),
                                usage_count=0,
                                warmup_completed=False,
                                cache_size_mb=model.get("size", 0) / (1024 * 1024),
                                performance_metrics={}
                            )
                        
                        # Also update loaded_models for compatibility
                        self.loaded_models[model_name] = {
                            "name": model_name,
                            "size": model.get("size", 0),
                            "modified": model.get("modified_at", ""),
                            "loaded": True
                        }
                    
                    logger.info(f"Discovered {len(models)} models")
                    return models
        except Exception as e:
            logger.error(f"Failed to discover models: {e}")
        return []
    
    async def _warmup_cache(self):
        """Warm up cache with priority models"""
        priority_models = [
            self.default_model,
            getattr(settings, 'FALLBACK_MODEL', 'tinyllama'),
            self.embedding_model
        ]
        
        for model_name in priority_models:
            if model_name in self.warm_cache:
                success = await self._warm_model(model_name)
                if success:
                    self.warm_cache[model_name].warmup_completed = True
                    logger.info(f"âœ… Warmed up priority model: {model_name}")
    
    async def _warm_model(self, model_name: str) -> bool:
        """Warm up a specific model with advanced techniques"""
        try:
            logger.info(f"ðŸ”¥ Warming up model: {model_name}")
            
            # Use multiple warmup strategies for comprehensive warming
            warmup_prompts = [
                "Hello",  # Basic response
                "Write a short sentence.",  # Text generation
                "1+1=",  # Reasoning
            ]
            
            total_warmup_time = 0
            for i, prompt in enumerate(warmup_prompts):
                start_time = time.time()
                
                warmup_data = {
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 10 if i == 0 else 50,  # Progressive warming
                        "temperature": 0,
                        "num_ctx": 512 + (i * 256),  # Progressive context
                        "num_batch": 512,
                        "num_thread": 8 if not self.gpu_available else 4
                    }
                }
                
                if self.gpu_available:
                    warmup_data["options"]["num_gpu"] = 1
                
                pool_manager = await get_pool_manager()
                async with pool_manager.get_http_client('ollama') as client:
                    response = await client.post(
                        '/api/generate',
                        json=warmup_data,
                        timeout=120.0
                    )
                    
                    if response.status_code == 200:
                        await response.json()  # Consume response
                        warmup_time = time.time() - start_time
                        total_warmup_time += warmup_time
                        logger.info(f"  Warmup step {i+1}/3: {warmup_time:.2f}s")
                    else:
                        logger.warning(f"  Warmup step {i+1} failed: HTTP {response.status_code}")
                        return False
            
            # Save warmup performance metrics
            if model_name in self.warm_cache:
                self.warm_cache[model_name].performance_metrics.update({
                    'warmup_time': total_warmup_time,
                    'warmup_completed_at': datetime.now().isoformat(),
                    'warmup_steps': len(warmup_prompts)
                })
            
            logger.info(f"âœ… Model {model_name} warmed up in {total_warmup_time:.2f}s")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error warming model {model_name}: {e}")
            return False
    
    async def _start_batch_processors(self):
        """Start batch processors for different models"""
        logger.info("Starting batch processors...")
        
        # Create batch processors for commonly used models
        priority_models = [self.default_model, getattr(settings, 'FALLBACK_MODEL', 'tinyllama')]
        
        for model_name in priority_models:
            if model_name not in self.batch_processors:
                processor = asyncio.create_task(self._batch_processor(model_name))
                self.batch_processors[model_name] = processor
                logger.info(f"Started batch processor for {model_name}")
    
    async def _batch_processor(self, model_name: str):
        """Process batched requests for a specific model"""
        while self._processing:
            try:
                # Wait for requests to accumulate or timeout
                await asyncio.sleep(self.batch_timeout_ms / 1000)
                
                if model_name in self.batch_queue and self.batch_queue[model_name]:
                    batch = self.batch_queue[model_name][:self.batch_size]
                    self.batch_queue[model_name] = self.batch_queue[model_name][self.batch_size:]
                    
                    if batch:
                        await self._process_batch(model_name, batch)
                        
            except Exception as e:
                logger.error(f"Error in batch processor for {model_name}: {e}")
                await asyncio.sleep(1)  # Brief pause before retrying
    
    async def _process_batch(self, model_name: str, batch: List[BatchRequest]):
        """Process a batch of requests efficiently"""
        try:
            logger.debug(f"Processing batch of {len(batch)} requests for {model_name}")
            
            # For now, process requests individually but could be enhanced
            # to use true batched inference when Ollama supports it
            for request in batch:
                try:
                    response = await self._generate_single(
                        request.prompt, 
                        model_name, 
                        **request.parameters
                    )
                    request.future.set_result(response)
                    
                except Exception as e:
                    request.future.set_exception(e)
                    
            # Update batch efficiency metrics
            batch_time = (datetime.now() - batch[0].timestamp).total_seconds()
            efficiency = len(batch) / batch_time if batch_time > 0 else 0
            self.performance_stats[model_name]['batch_efficiency'] = efficiency
            
        except Exception as e:
            logger.error(f"Error processing batch: {e}")
            for request in batch:
                if not request.future.done():
                    request.future.set_exception(e)
    
    def _get_cache_key(self, model: str, prompt: str, options: Dict[str, Any]) -> str:
        """Generate cache key for prompts"""
        key_data = f"{model}:{prompt}:{json.dumps(options, sort_keys=True)}"
        return hashlib.sha256(key_data.encode()).hexdigest()
        
    def _is_cache_valid(self, cached_entry: Dict[str, Any]) -> bool:
        """Check if cached entry is still valid"""
        if 'timestamp' not in cached_entry:
            return False
        age = (datetime.now() - cached_entry['timestamp']).total_seconds()
        return age < self._cache_ttl
    
    # GENERATION METHODS
    async def generate(
        self,
        prompt: str,
        model: str = None,
        options: Optional[Dict[str, Any]] = None,
        use_cache: bool = True,
        stream: bool = False
    ) -> Dict[str, Any]:
        """Generate text using Ollama with caching and non-blocking execution"""
        
        model = model or self.default_model
        self._stats['total_requests'] += 1
        start_time = asyncio.get_event_loop().time()
        
        # Default options for performance
        if options is None:
            options = {
                'num_predict': 50,  # Reduced for faster generation
                'temperature': 0.7,
                'top_k': 40,
                'top_p': 0.9,
                'num_ctx': 512  # Reduced context for faster processing
            }
            
        # Check cache first
        if use_cache and not stream:
            cache_key = self._get_cache_key(model, prompt, options)
            if cache_key in self._generation_cache:
                cached = self._generation_cache[cache_key]
                if self._is_cache_valid(cached):
                    self._stats['cache_hits'] += 1
                    logger.debug(f"Cache hit for prompt: {prompt[:50]}...")
                    return cached['response']
                    
        self._stats['cache_misses'] += 1
        
        try:
            # Get connection pool manager
            pool_manager = await get_pool_manager()
            
            # Use the pooled Ollama client
            async with pool_manager.get_http_client('ollama') as client:
                # Prepare request
                request_data = {
                    'model': model,
                    'prompt': prompt,
                    'stream': stream,
                    'options': options
                }
                
                # Add GPU optimization if available
                if self.gpu_available:
                    request_data['options']['num_gpu'] = 1
                
                # Make async request with timeout
                response = await client.post(
                    '/api/generate',
                    json=request_data,
                    timeout=15.0
                )
                
                if response.status_code != 200:
                    raise Exception(f"Ollama error: {response.status_code} - {response.text}")
                    
                result = response.json()
                
                # Cache the result if not streaming
                if use_cache and not stream:
                    self._cache_result(cache_key, result)
                    
                # Update stats
                elapsed = asyncio.get_event_loop().time() - start_time
                self._update_avg_response_time(elapsed)
                
                # Update warm cache usage
                if model in self.warm_cache:
                    self.warm_cache[model].last_used = datetime.now()
                    self.warm_cache[model].usage_count += 1
                
                return result
                
        except Exception as e:
            self._stats['errors'] += 1
            logger.error(f"Ollama generation error: {e}")
            
            # Return fallback response
            return {
                'response': f"Error generating response: {str(e)}",
                'model': model,
                'done': True,
                'error': str(e)
            }
    
    async def _generate_single(self, prompt: str, model: str, **kwargs) -> str:
        """Generate text using a single model (internal method)"""
        try:
            start_time = time.time()
            
            # Prepare optimized generation parameters
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_thread": 8 if not self.gpu_available else 4,
                    "num_ctx": kwargs.get("num_ctx", 2048),
                    "num_batch": 512,
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_predict": kwargs.get("max_tokens", 2048)
                }
            }
            
            if self.gpu_available:
                data["options"]["num_gpu"] = 1
            
            # Add custom options
            data["options"].update(kwargs.get("options", {}))
            
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                response = await client.post(
                    '/api/generate',
                    json=data,
                    timeout=getattr(settings, 'MODEL_TIMEOUT', 60)
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Update performance metrics
                    latency = time.time() - start_time
                    stats = self.performance_stats[model]
                    stats['total_requests'] += 1
                    stats['avg_latency'] = (stats['avg_latency'] * (stats['total_requests'] - 1) + latency) / stats['total_requests']
                    
                    # Update cache metrics
                    if model in self.warm_cache:
                        self.warm_cache[model].last_used = datetime.now()
                        self.warm_cache[model].usage_count += 1
                        stats['cache_hits'] += 1
                    else:
                        stats['cache_misses'] += 1
                    
                    return result.get("response", "")
                else:
                    logger.error(f"Generation failed with status: {response.status_code}")
                    return ""
                    
        except Exception as e:
            logger.error(f"Error in single generation: {e}")
            return ""
    
    async def generate_text(self, prompt: str, model: Optional[str] = None, **kwargs) -> str:
        """
        Generate text with advanced optimizations
        - Uses warm cache when available
        - Batches requests for efficiency
        - Applies GPU acceleration
        """
        model = model or self.default_model
        
        # Check if model is warmed up
        if model in self.warm_cache and self.warm_cache[model].warmup_completed:
            # Model is warm, add to batch queue
            request_id = hashlib.md5(f"{prompt}{time.time()}".encode()).hexdigest()[:8]
            future = asyncio.Future()
            
            batch_request = BatchRequest(
                request_id=request_id,
                prompt=prompt,
                model=model,
                parameters=kwargs,
                timestamp=datetime.now(),
                future=future
            )
            
            self.batch_queue[model].append(batch_request)
            
            # Ensure batch processor is running
            if model not in self.batch_processors:
                processor = asyncio.create_task(self._batch_processor(model))
                self.batch_processors[model] = processor
            
            return await future
        else:
            # Model not warm, warm it up and process directly
            if model not in self.warm_cache:
                await self._discover_models()
            
            if not self.warm_cache.get(model, ModelCacheEntry("", datetime.now(), datetime.now(), 0, False, 0, {})).warmup_completed:
                await self._warm_model(model)
                if model in self.warm_cache:
                    self.warm_cache[model].warmup_completed = True
            
            return await self._generate_single(prompt, model, **kwargs)
    
    async def generate_streaming(
        self,
        prompt: str,
        model: str = None,
        options: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """Stream generation for real-time responses"""
        
        model = model or self.default_model
        if options is None:
            options = {
                'num_predict': 50,  # Reduced for faster generation
                'temperature': 0.7
            }
            
        try:
            pool_manager = await get_pool_manager()
            
            async with pool_manager.get_http_client('ollama') as client:
                request_data = {
                    'model': model,
                    'prompt': prompt,
                    'stream': True,
                    'options': options
                }
                
                if self.gpu_available:
                    request_data['options']['num_gpu'] = 1
                
                # Stream the response
                async with client.stream('POST', '/api/generate', json=request_data) as response:
                    async for line in response.aiter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                if 'response' in data:
                                    yield data['response']
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            logger.error(f"Streaming generation error: {e}")
            yield f"Error: {str(e)}"
    
    async def generate_streaming_advanced(self, prompt: str, model: Optional[str] = None, **kwargs) -> AsyncGenerator[StreamingResponse, None]:
        """
        Generate streaming text responses for real-time user experience
        """
        model = model or self.default_model
        request_id = hashlib.md5(f"{prompt}{time.time()}".encode()).hexdigest()[:8]
        
        try:
            # Prepare streaming generation parameters
            data = {
                "model": model,
                "prompt": prompt,
                "stream": True,  # Enable streaming
                "options": {
                    "num_thread": 8 if not self.gpu_available else 4,
                    "num_ctx": kwargs.get("num_ctx", 2048),
                    "num_batch": 512,
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_predict": kwargs.get("max_tokens", 2048)
                }
            }
            
            if self.gpu_available:
                data["options"]["num_gpu"] = 1
            
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                async with client.stream('POST', '/api/generate', json=data, timeout=getattr(settings, 'MODEL_TIMEOUT', 60)) as response:
                    if response.status_code == 200:
                        chunk_count = 0
                        async for line in response.aiter_lines():
                            if line:
                                try:
                                    chunk_data = json.loads(line)
                                    chunk_text = chunk_data.get("response", "")
                                    is_final = chunk_data.get("done", False)
                                    
                                    yield StreamingResponse(
                                        request_id=request_id,
                                        chunk=chunk_text,
                                        is_final=is_final,
                                        metadata={
                                            "chunk_count": chunk_count,
                                            "model": model,
                                            "timestamp": datetime.now().isoformat()
                                        }
                                    )
                                    
                                    chunk_count += 1
                                    
                                    if is_final:
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                    else:
                        yield StreamingResponse(
                            request_id=request_id,
                            chunk="",
                            is_final=True,
                            metadata={"error": f"HTTP {response.status_code}"}
                        )
                        
        except Exception as e:
            logger.error(f"Error in streaming generation: {e}")
            yield StreamingResponse(
                request_id=request_id,
                chunk="",
                is_final=True,
                metadata={"error": str(e)}
            )
    
    # CHAT METHODS
    async def chat(self, messages: List[Dict[str, str]], model: Optional[str] = None, **kwargs) -> str:
        """Chat with a model"""
        model = model or self.default_model
        
        try:
            # Prepare optimized chat parameters
            data = {
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "num_thread": 8 if not self.gpu_available else 4,
                    "num_ctx": kwargs.get("num_ctx", 2048),
                    "num_batch": 512,
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9)
                }
            }
            
            if self.gpu_available:
                data["options"]["num_gpu"] = 1
            
            # Add custom options
            data["options"].update(kwargs.get("options", {}))
            
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                response = await client.post(
                    '/api/chat',
                    json=data,
                    timeout=getattr(settings, 'MODEL_TIMEOUT', 60)
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("message", {}).get("content", "")
                else:
                    logger.error(f"Chat failed with status: {response.status_code}")
                    return ""
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            return ""
    
    async def chat_streaming(self, messages: List[Dict[str, str]], model: Optional[str] = None, **kwargs) -> AsyncGenerator[StreamingResponse, None]:
        """
        Streaming chat interface with conversation context
        """
        model = model or self.default_model
        request_id = hashlib.md5(f"{json.dumps(messages)}{time.time()}".encode()).hexdigest()[:8]
        
        try:
            data = {
                "model": model,
                "messages": messages,
                "stream": True,
                "options": {
                    "num_thread": 8 if not self.gpu_available else 4,
                    "num_ctx": kwargs.get("num_ctx", 4096),  # Larger context for chat
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9)
                }
            }
            
            if self.gpu_available:
                data["options"]["num_gpu"] = 1
            
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                async with client.stream('POST', '/api/chat', json=data, timeout=getattr(settings, 'MODEL_TIMEOUT', 60)) as response:
                    if response.status_code == 200:
                        chunk_count = 0
                        async for line in response.aiter_lines():
                            if line:
                                try:
                                    chunk_data = json.loads(line)
                                    message = chunk_data.get("message", {})
                                    chunk_text = message.get("content", "")
                                    is_final = chunk_data.get("done", False)
                                    
                                    yield StreamingResponse(
                                        request_id=request_id,
                                        chunk=chunk_text,
                                        is_final=is_final,
                                        metadata={
                                            "chunk_count": chunk_count,
                                            "model": model,
                                            "timestamp": datetime.now().isoformat(),
                                            "message_role": message.get("role", "assistant")
                                        }
                                    )
                                    
                                    chunk_count += 1
                                    
                                    if is_final:
                                        break
                                        
                                except json.JSONDecodeError:
                                    continue
                                
        except Exception as e:
            logger.error(f"Error in chat streaming: {e}")
            yield StreamingResponse(
                request_id=request_id,
                chunk="",
                is_final=True,
                metadata={"error": str(e)}
            )
    
    # EMBEDDING METHODS
    async def generate_embedding(self, text: str, model: Optional[str] = None) -> Optional[List[float]]:
        """
        Generate embedding for text using specified model
        
        Args:
            text: Input text to embed
            model: Model to use (defaults to embedding_model)
            
        Returns:
            List of float values representing the embedding, or None if failed
        """
        model = model or self.embedding_model
        
        if not text or not text.strip():
            logger.warning("Empty text provided for embedding")
            return None
        
        try:
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                # Use Ollama embeddings endpoint
                request_data = {
                    "model": model,
                    "prompt": text.strip()
                }
                
                response = await client.post(
                    '/api/embeddings',
                    json=request_data,
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    data = response.json()
                    embedding = data.get("embedding")
                    
                    if embedding and isinstance(embedding, list):
                        logger.debug(f"Generated embedding of size {len(embedding)}")
                        return embedding
                    else:
                        logger.error("Invalid embedding format from Ollama")
                        return None
                else:
                    logger.error(f"Ollama embedding failed: {response.status_code} - {response.text}")
                    return None
                    
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return None
    
    async def embed(self, text: str, model: Optional[str] = None) -> List[float]:
        """Generate embeddings for text (compatibility method)"""
        result = await self.generate_embedding(text, model)
        return result if result else []
    
    async def generate_embeddings_batch(self, texts: List[str], model: Optional[str] = None) -> List[Optional[List[float]]]:
        """
        Generate embeddings for multiple texts
        
        Args:
            texts: List of texts to embed
            model: Model to use for embeddings
            
        Returns:
            List of embeddings (or None for failed ones)
        """
        if not texts:
            return []
        
        # Process in parallel with semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(5)  # Max 5 concurrent requests
        
        async def embed_with_semaphore(text):
            async with semaphore:
                return await self.generate_embedding(text, model)
        
        tasks = [embed_with_semaphore(text) for text in texts]
        embeddings = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        results = []
        for i, result in enumerate(embeddings):
            if isinstance(result, Exception):
                logger.error(f"Error embedding text {i}: {result}")
                results.append(None)
            else:
                results.append(result)
        
        return results
    
    async def similarity(self, text1: str, text2: str, model: Optional[str] = None) -> Optional[float]:
        """
        Calculate cosine similarity between two texts
        
        Args:
            text1: First text
            text2: Second text
            model: Model to use for embeddings
            
        Returns:
            Similarity score between 0 and 1, or None if failed
        """
        embeddings = await self.generate_embeddings_batch([text1, text2], model)
        
        if len(embeddings) != 2 or None in embeddings:
            logger.error("Failed to generate embeddings for similarity calculation")
            return None
        
        try:
            import numpy as np
            
            # Convert to numpy arrays
            emb1 = np.array(embeddings[0])
            emb2 = np.array(embeddings[1])
            
            # Calculate cosine similarity
            dot_product = np.dot(emb1, emb2)
            norm1 = np.linalg.norm(emb1)
            norm2 = np.linalg.norm(emb2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            
            # Ensure result is between 0 and 1
            return max(0.0, min(1.0, (similarity + 1) / 2))
            
        except Exception as e:
            logger.error(f"Error calculating similarity: {e}")
            return None
    
    # MODEL MANAGEMENT METHODS
    async def list_models(self) -> List[Dict[str, Any]]:
        """List available models from Ollama"""
        try:
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                response = await client.get('/api/tags')
                if response.status_code == 200:
                    data = response.json()
                    models = data.get("models", [])
                    
                    # Update loaded models cache
                    for model in models:
                        model_name = model.get("name", "")
                        self.loaded_models[model_name] = {
                            "name": model_name,
                            "size": model.get("size", 0),
                            "modified": model.get("modified_at", ""),
                            "loaded": True
                        }
                    
                    logger.info(f"Found {len(models)} models in Ollama")
                    return models
                return []
        except Exception as e:
            logger.error(f"Failed to list models: {e}")
            return []
    
    async def pull_model(self, model_name: str) -> bool:
        """Pull a model from Ollama registry"""
        try:
            logger.info(f"Pulling model: {model_name}")
            
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                data = {"name": model_name}
                response = await client.post(
                    '/api/pull',
                    json=data,
                    timeout=3600.0  # 1 hour timeout
                )
                
                if response.status_code == 200:
                    # Stream the response to track progress
                    async for line in response.aiter_lines():
                        if line:
                            try:
                                progress = json.loads(line)
                                if "status" in progress:
                                    logger.info(f"Pull progress: {progress.get('status')}")
                            except Exception:
                                pass
                    
                    logger.info(f"Successfully pulled model: {model_name}")
                    return True
                else:
                    logger.error(f"Failed to pull model: {response.status_code}")
                    return False
        except Exception as e:
            logger.error(f"Error pulling model {model_name}: {e}")
            return False
    
    async def unload_model(self, model_name: str) -> bool:
        """Unload a model from memory"""
        try:
            pool_manager = await get_pool_manager()
            async with pool_manager.get_http_client('ollama') as client:
                data = {"name": model_name}
                response = await client.delete('/api/generate', json=data)
                
                if response.status_code == 200:
                    if model_name in self.loaded_models:
                        del self.loaded_models[model_name]
                    if model_name in self.warm_cache:
                        del self.warm_cache[model_name]
                    logger.info(f"Unloaded model: {model_name}")
                    return True
                return False
        except Exception as e:
            logger.error(f"Error unloading model {model_name}: {e}")
            return False
    
    # BATCH PROCESSING METHODS
    async def batch_generate(
        self,
        prompts: List[str],
        model: str = None,
        options: Optional[Dict[str, Any]] = None,
        max_concurrent: int = 5
    ) -> List[Dict[str, Any]]:
        """Process multiple prompts concurrently with rate limiting"""
        
        model = model or self.default_model
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_single(prompt: str) -> Dict[str, Any]:
            async with semaphore:
                return await self.generate(prompt, model, options)
                
        # Process all prompts concurrently
        tasks = [process_single(prompt) for prompt in prompts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle any exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Batch generation error for prompt {i}: {result}")
                processed_results.append({
                    'response': f"Error: {str(result)}",
                    'error': True
                })
            else:
                processed_results.append(result)
                
        return processed_results
    
    async def generate_with_retry(
        self,
        prompt: str,
        model: str = None,
        options: Optional[Dict[str, Any]] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0
    ) -> Dict[str, Any]:
        """Generate with automatic retry on failure"""
        
        model = model or self.default_model
        last_error = None
        
        for attempt in range(max_retries):
            try:
                return await self.generate(prompt, model, options, use_cache=attempt == 0)
            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay * (2 ** attempt))  # Exponential backoff
                    
        # All retries failed
        logger.error(f"All retries failed for prompt: {prompt[:50]}... Error: {last_error}")
        return {
            'response': f"Failed after {max_retries} attempts: {str(last_error)}",
            'error': True
        }
    
    # QUEUE PROCESSING METHODS
    async def _process_queue_worker(self, worker_id: int):
        """Background worker for processing queued requests"""
        logger.info(f"Queue worker {worker_id} started")
        
        while self._processing:
            try:
                # Get request from queue
                request = await asyncio.wait_for(
                    self._request_queue.get(),
                    timeout=1.0
                )
                
                # Process the request
                result = await self.generate(
                    request['prompt'],
                    request.get('model', self.default_model),
                    request.get('options')
                )
                
                # Store result if callback provided
                if 'callback' in request:
                    await request['callback'](result)
                    
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Queue worker {worker_id} error: {e}")
                
    async def queue_generation(
        self,
        prompt: str,
        model: str = None,
        options: Optional[Dict[str, Any]] = None,
        callback: Optional[Any] = None
    ) -> bool:
        """Queue a generation request for background processing"""
        
        model = model or self.default_model
        try:
            request = {
                'prompt': prompt,
                'model': model,
                'options': options,
                'callback': callback
            }
            
            # Try to add to queue (non-blocking)
            await asyncio.wait_for(
                self._request_queue.put(request),
                timeout=0.1
            )
            return True
            
        except asyncio.TimeoutError:
            logger.warning("Generation queue is full")
            return False
    
    # CACHE MANAGEMENT METHODS
    def _cache_result(self, key: str, result: Dict[str, Any]):
        """Cache generation result"""
        # Implement LRU eviction if cache is full
        if len(self._generation_cache) >= self._max_cache_size:
            # Remove oldest entry
            oldest_key = min(
                self._generation_cache.keys(),
                key=lambda k: self._generation_cache[k].get('timestamp', datetime.min)
            )
            del self._generation_cache[oldest_key]
            
        self._generation_cache[key] = {
            'response': result,
            'timestamp': datetime.now()
        }
    
    def clear_cache(self):
        """Clear the generation cache"""
        self._generation_cache.clear()
        logger.info("Ollama generation cache cleared")
    
    async def save_cache_artifacts(self) -> bytes:
        """
        Save portable cache artifacts using PyTorch-style caching
        Based on torch.compiler.save_cache_artifacts methodology
        """
        try:
            cache_data = {
                "cache_metadata": {
                    model_name: asdict(entry) for model_name, entry in self.warm_cache.items()
                },
                "performance_stats": dict(self.performance_stats),
                "system_info": {
                    "gpu_available": self.gpu_available,
                    "device_preference": self.device_preference,
                    "cache_size": self.max_cache_size,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            # Serialize cache data
            cache_bytes = pickle.dumps(cache_data)
            
            # Save to file as well
            cache_file = self.cache_directory / f"cache_artifacts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.bin"
            cache_file.write_bytes(cache_bytes)
            
            logger.info(f"Cache artifacts saved: {len(cache_bytes)} bytes")
            return cache_bytes
            
        except Exception as e:
            logger.error(f"Error saving cache artifacts: {e}")
            return b""
    
    async def load_cache_artifacts(self, cache_bytes: bytes) -> bool:
        """
        Load portable cache artifacts from another instance
        """
        try:
            cache_data = pickle.loads(cache_bytes)
            
            # Load cache metadata
            for model_name, entry_data in cache_data["cache_metadata"].items():
                # Convert datetime strings back to datetime objects
                if isinstance(entry_data["load_time"], str):
                    entry_data["load_time"] = datetime.fromisoformat(entry_data["load_time"])
                if isinstance(entry_data["last_used"], str):
                    entry_data["last_used"] = datetime.fromisoformat(entry_data["last_used"])
                
                self.warm_cache[model_name] = ModelCacheEntry(**entry_data)
            
            # Load performance stats
            self.performance_stats.update(cache_data["performance_stats"])
            
            logger.info(f"Loaded cache artifacts for {len(cache_data['cache_metadata'])} models")
            return True
            
        except Exception as e:
            logger.error(f"Error loading cache artifacts: {e}")
            return False
    
    async def _save_cache_metadata(self):
        """Save cache metadata to disk"""
        try:
            cache_metadata = {}
            for model_name, entry in self.warm_cache.items():
                cache_metadata[model_name] = {
                    "model_name": entry.model_name,
                    "load_time": entry.load_time.isoformat(),
                    "last_used": entry.last_used.isoformat(),
                    "usage_count": entry.usage_count,
                    "warmup_completed": entry.warmup_completed,
                    "cache_size_mb": entry.cache_size_mb,
                    "performance_metrics": entry.performance_metrics
                }
            
            cache_metadata_file = self.cache_directory / "cache_metadata.json"
            with open(cache_metadata_file, 'w') as f:
                json.dump(cache_metadata, f, indent=2)
                
            logger.info("Cache metadata saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving cache metadata: {e}")
    
    # PERFORMANCE AND MONITORING METHODS
    def _update_avg_response_time(self, elapsed: float):
        """Update average response time"""
        current_avg = self._stats['avg_response_time']
        total_requests = self._stats['total_requests']
        self._stats['avg_response_time'] = (
            (current_avg * (total_requests - 1) + elapsed) / total_requests
        )
    
    def _calculate_cache_hit_rate(self) -> float:
        """Calculate overall cache hit rate"""
        total_hits = sum(stats['cache_hits'] for stats in self.performance_stats.values())
        total_misses = sum(stats['cache_misses'] for stats in self.performance_stats.values())
        total_requests = total_hits + total_misses
        
        return (total_hits / total_requests * 100) if total_requests > 0 else 0.0
    
    def get_stats(self) -> Dict[str, Any]:
        """Get service statistics"""
        cache_hit_rate = 0
        if self._stats['total_requests'] > 0:
            cache_hit_rate = self._stats['cache_hits'] / self._stats['total_requests']
            
        return {
            **self._stats,
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self._generation_cache),
            'queue_size': self._request_queue.qsize()
        }
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        return {
            "cache_status": {
                "total_models": len(self.warm_cache),
                "warmed_models": sum(1 for entry in self.warm_cache.values() if entry.warmup_completed),
                "cache_hit_rate": self._calculate_cache_hit_rate(),
                "total_cache_size_mb": sum(entry.cache_size_mb for entry in self.warm_cache.values())
            },
            "performance_stats": dict(self.performance_stats),
            "system_info": {
                "gpu_available": self.gpu_available,
                "device_preference": self.device_preference,
                "active_batch_processors": len(self.batch_processors),
                "pending_batch_requests": sum(len(queue) for queue in self.batch_queue.values())
            },
            "basic_stats": self.get_stats()
        }
    
    def get_status(self) -> Dict[str, Any]:
        """Get model manager status"""
        return {
            "status": "active" if self._processing else "inactive",
            "loaded_count": len(self.loaded_models),
            "models": list(self.loaded_models.keys()),
            "default_model": self.default_model,
            "embedding_model": self.embedding_model,
            "gpu_available": self.gpu_available
        }
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get health status"""
        return {
            "status": "healthy" if self._processing else "unhealthy",
            "processing": self._processing,
            "total_requests": self._stats['total_requests'],
            "error_rate": self._stats['errors'] / max(1, self._stats['total_requests']),
            "cache_hit_rate": self._stats['cache_hits'] / max(1, self._stats['total_requests']),
            "warm_models": len([m for m in self.warm_cache.values() if m.warmup_completed])
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """
        Check health of the service
        
        Returns:
            Health status dictionary
        """
        status = {
            "service": "consolidated_ollama",
            "status": "unhealthy",
            "model": self.default_model,
            "embedding_model": self.embedding_model,
            "ollama_host": self.ollama_host,
            "details": {}
        }
        
        try:
            # Test basic connectivity
            if await self._check_ollama_health():
                models = await self.list_models()
                model_names = [m.get("name", "").split(":")[0] for m in models]
                
                status["details"]["available_models"] = model_names
                status["details"]["connectivity"] = "ok"
                
                if self.default_model in model_names:
                    # Test generation
                    test_result = await self.generate("Hello", self.default_model, {"num_predict": 5}, use_cache=False)
                    
                    if test_result and not test_result.get("error"):
                        status["status"] = "healthy"
                        status["details"]["test_generation"] = "success"
                        
                        # Test embedding if available
                        if self.embedding_model in model_names:
                            test_embedding = await self.generate_embedding("test")
                            if test_embedding:
                                status["details"]["embedding_size"] = len(test_embedding)
                                status["details"]["test_embedding"] = "success"
                            else:
                                status["details"]["test_embedding"] = "failed"
                    else:
                        status["details"]["test_generation"] = "failed"
                else:
                    status["details"]["model_available"] = False
            else:
                status["details"]["connectivity"] = "failed"
                
        except Exception as e:
            status["details"]["error"] = str(e)
        
        # Add performance metrics
        status["details"].update(await self.get_performance_metrics())
        
        return status
    
    # WARMUP AND MAINTENANCE METHODS
    async def warmup(self, num_requests: int = 5):
        """Warmup the service with test requests"""
        logger.info(f"Warming up consolidated Ollama service with {num_requests} requests")
        
        test_prompts = [
            "Hello",
            "Test",
            "System check",
            "What is 1+1?",
            "Ready"
        ]
        
        for i in range(min(num_requests, len(test_prompts))):
            await self.generate(test_prompts[i], use_cache=False)
            
        logger.info("Consolidated Ollama service warmup complete")
    
    async def shutdown(self):
        """Gracefully shutdown the service"""
        logger.info("Shutting down consolidated Ollama service...")
        
        self._processing = False
        
        # Stop batch processors
        for processor in self.batch_processors.values():
            processor.cancel()
        
        # Save cache metadata
        await self._save_cache_metadata()
        
        # Shutdown executor
        self._executor.shutdown(wait=True)
        
        logger.info("Consolidated Ollama service shutdown complete")


# Global service instance
_ollama_service: Optional[ConsolidatedOllamaService] = None


async def get_ollama_service() -> ConsolidatedOllamaService:
    """Get or create the global consolidated Ollama service"""
    global _ollama_service
    
    if _ollama_service is None:
        _ollama_service = ConsolidatedOllamaService()
        await _ollama_service.initialize()
        # Skip warmup - system responds quickly without it (200-400Âµs)
        # await _ollama_service.warmup(3)  # Removed - not needed for responsive system
        
    return _ollama_service


# Compatibility functions for existing code
async def get_ollama_embedding_service() -> ConsolidatedOllamaService:
    """Get the consolidated service (compatibility function for embeddings)"""
    return await get_ollama_service()


async def get_model_manager() -> ConsolidatedOllamaService:
    """Get the consolidated service (compatibility function for model management)"""
    return await get_ollama_service()


async def get_advanced_model_manager() -> ConsolidatedOllamaService:
    """Get the consolidated service (compatibility function for advanced features)"""
    return await get_ollama_service()