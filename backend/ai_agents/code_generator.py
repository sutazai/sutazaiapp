import logging

logger = logging.getLogger(__name__)

class CodeGenerator:
    def __init__(self, model_manager):
        self.model_manager = model_manager
        
    async def generate_code(self, requirements, language="python", params=None):
        try:
            # Using llama3-chatqa as it's confirmed to be available
            model_id = "llama3-chatqa"  
            
            # Ensure the model manager can access and potentially load the model
            # Depending on model_manager implementation, load_model might not be needed
            # if models are pre-loaded or loaded on demand by run_inference.
            # Adding a check here for robustness.
            # Consider if model_manager.load_model is synchronous or asynchronous
            # Assuming synchronous for now based on typical patterns
            # if hasattr(self.model_manager, 'load_model'):
            #     if not self.model_manager.load_model(model_id):
            #         logger.error(f"Failed to load model {model_id}")
            #         return {"error": f"Failed to load model {model_id}", "code": None}

            code_prompt = f"Generate {language} code based on the following requirements:\n{requirements}"
            
            # Default parameters for code generation
            default_params = {"temperature": 0.2, "max_tokens": 1024} 
            if params:
                default_params.update(params)

            # Assuming run_inference is an async method based on the 'await' keyword elsewhere
            result = await self.model_manager.run_inference(
                model_id=model_id,
                inputs=[{"role": "user", "content": code_prompt}], # Using chat-like input structure
                parameters=default_params
            )
            
            # Check the structure of the result from run_inference
            # Adapt this based on the actual return value of model_manager.run_inference
            if isinstance(result, dict) and "error" in result:
                logger.error(f"Inference error for model {model_id}: {result['error']}")
                return {"error": result["error"], "code": None}
            
            # Extract the generated text - adjust key based on actual model_manager response
            generated_code = result.get("text") or result.get("choices", [{}])[0].get("message", {}).get("content", "")

            if not generated_code:
                 logger.warning(f"No code generated by model {model_id} for prompt: {requirements[:100]}...")
                 return {"error": "Model did not generate any code.", "code": None}

            logger.info(f"Successfully generated code using model {model_id}")
            return {
                "code": generated_code.strip(),
                "language": language,
                "model": model_id
            }
            
        except Exception as e:
            logger.exception(f"Error during code generation: {str(e)}") # Use logger.exception for stack trace
            return {"error": "Code generation failed due to an internal error.", "code": None}
