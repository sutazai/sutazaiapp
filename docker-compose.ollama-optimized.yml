version: '3.8'

services:
  ollama:
    container_name: sutazai-ollama
    image: sutazai-ollama-secure:latest
    deploy:
      resources:
        limits:
          cpus: '8.0'           # Increased from 4.0
          memory: 8G            # Increased from 4G for better caching
        reservations:
          cpus: '4.0'           # Increased from 1.0
          memory: 4G            # Increased from 1G
    environment:
      # Core Ollama settings
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_MODELS: /home/ollama/.ollama/models
      OLLAMA_DEBUG: "false"
      
      # CRITICAL PERFORMANCE OPTIMIZATIONS
      OLLAMA_NUM_PARALLEL: '8'        # Increased from 1 for parallel processing
      OLLAMA_NUM_THREADS: '12'        # Increased from 8 to use all CPU threads
      OLLAMA_MAX_LOADED_MODELS: '2'   # Keep multiple models loaded
      OLLAMA_KEEP_ALIVE: '30m'        # Keep models loaded for 30 minutes
      
      # Memory optimizations
      OLLAMA_USE_MMAP: 'true'         # Memory-mapped files for faster loading
      OLLAMA_USE_MLOCK: 'true'        # Lock model in memory (requires privileges)
      OLLAMA_FLASH_ATTENTION: '1'     # Enable flash attention (was 0)
      OLLAMA_USE_NUMA: 'false'        # Disable NUMA for single-node setup
      
      # Context and batch optimization
      OLLAMA_NUM_CTX: '2048'          # Optimized context size for TinyLlama
      OLLAMA_BATCH_SIZE: '64'         # Increased batch size
      OLLAMA_NUM_BATCH: '512'         # Number of batches
      
      # GPU acceleration (auto-detect)
      OLLAMA_NUM_GPU: '-1'            # Auto-detect GPU layers
      CUDA_VISIBLE_DEVICES: '0'       # Use first GPU if available
      
      # Connection pooling
      OLLAMA_MAX_QUEUE: '50'          # Increased from 10
      OLLAMA_CONNECTION_POOL: '50'    # Increased from 10
      OLLAMA_TIMEOUT: '30s'           # Reduced from 300s for faster failures
      OLLAMA_REQUEST_TIMEOUT: '30'    # Reduced from 300
      
      # Model loading optimization
      OLLAMA_LOAD_TIMEOUT: '60s'      # Timeout for model loading
      OLLAMA_RUNNERS_DIR: /tmp/ollama-runners  # Fast SSD/RAM disk
      OLLAMA_TMPDIR: /tmp/ollama
      
      # Additional optimizations
      OLLAMA_SCHEDULE_SPREAD: 'false'  # Don't spread load (single instance)
      OLLAMA_LLM_LIBRARY_OVERRIDE: ''  # Use optimized libraries if available
      
      # Monitoring
      OLLAMA_METRICS_PORT: '11435'     # Separate metrics port
      
    # Add tmpfs for faster temporary file operations
    tmpfs:
      - /tmp/ollama:size=2G,mode=1777
      - /tmp/ollama-runners:size=1G,mode=1777
      
    # System optimizations
    sysctls:
      - net.core.somaxconn=65535
      - net.ipv4.tcp_fin_timeout=30
      - net.ipv4.tcp_tw_reuse=1
      - net.ipv4.tcp_keepalive_time=60
      - net.ipv4.tcp_keepalive_probes=3
      - net.ipv4.tcp_keepalive_intvl=10
      
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      memlock:
        soft: -1
        hard: -1
      
    # Add capabilities for memory locking
    cap_add:
      - IPC_LOCK
      
    # Health check optimized for performance
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f --max-time 2 http://localhost:11434/api/tags || exit 1
      interval: 60s          # Less frequent checks
      timeout: 5s
      retries: 2
      start_period: 30s      # Faster startup
      
    # Logging optimization
    logging:
      driver: json-file
      options:
        max-size: 50m        # Reduced from 100m
        max-file: '3'        # Reduced from 5
        
    volumes:
      - ollama_data:/home/ollama/.ollama
      - models_data:/models
      - /opt/sutazaiapp/CLAUDE.md:/app/CLAUDE.md:ro
      - /opt/sutazaiapp/config/ollama-optimized.yaml:/app/config/ollama.yaml:ro
      
    networks:
      - sutazai-network
      
    ports:
      - 10104:11434
      - 11435:11435  # Metrics port
      
    restart: unless-stopped

volumes:
  ollama_data:
  models_data:

networks:
  sutazai-network:
    external: true