version: '3.8'

# Distributed SutazAI Architecture
# This configuration implements a highly available, horizontally scalable system

networks:
  ai-mesh:
    driver: overlay
    attachable: true
    driver_opts:
      encrypted: "true"
  data-tier:
    driver: overlay
    attachable: true
  cache-tier:
    driver: overlay
    attachable: true
  monitoring:
    driver: overlay
    attachable: true

volumes:
  consul-data-1:
  consul-data-2:
  consul-data-3:
  rabbitmq-data-1:
  rabbitmq-data-2:
  rabbitmq-data-3:
  redis-data-1:
  redis-data-2:
  redis-data-3:
  redis-data-4:
  redis-data-5:
  redis-data-6:
  postgres-primary:
  postgres-replica-1:
  postgres-replica-2:
  shared-models:
  prometheus-data:
  grafana-data:
  jaeger-data:

# Base configurations
x-healthcheck: &default-healthcheck
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    labels: "service,replica"

x-deploy-replicated: &deploy-replicated
  replicas: 2
  update_config:
    parallelism: 1
    delay: 10s
    failure_action: rollback
    monitor: 60s
    max_failure_ratio: 0.3
  restart_policy:
    condition: any
    delay: 5s
    max_attempts: 3
    window: 120s

services:
  # ======================
  # LAYER 1: Load Balancers
  # ======================
  
  haproxy:
    image: haproxy:2.9-alpine
    networks:
      - ai-mesh
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # Stats page
    volumes:
      - ./config/haproxy:/usr/local/etc/haproxy:ro
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 10s
    logging: *default-logging
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      <<: *default-healthcheck

  # ======================
  # LAYER 2: Service Discovery
  # ======================
  
  consul-server-1:
    image: consul:1.17
    networks:
      - ai-mesh
    ports:
      - target: 8500
        published: 8500
        mode: host
    volumes:
      - consul-data-1:/consul/data
      - ./config/consul:/consul/config
    environment:
      CONSUL_BIND_INTERFACE: eth0
      CONSUL_CLIENT_INTERFACE: eth0
    command: |
      agent -server -ui 
      -node=consul-server-1 
      -bootstrap-expect=3 
      -client=0.0.0.0 
      -datacenter=dc1
      -encrypt=${CONSUL_ENCRYPT_KEY:-Luj2FZWwlt8475wD1WtwUQ==}
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.labels.consul == server1
    logging: *default-logging
    healthcheck:
      test: ["CMD", "consul", "operator", "raft", "list-peers"]
      <<: *default-healthcheck

  consul-server-2:
    image: consul:1.17
    networks:
      - ai-mesh
    volumes:
      - consul-data-2:/consul/data
      - ./config/consul:/consul/config
    environment:
      CONSUL_BIND_INTERFACE: eth0
      CONSUL_CLIENT_INTERFACE: eth0
    command: |
      agent -server 
      -node=consul-server-2 
      -bootstrap-expect=3 
      -retry-join=consul-server-1 
      -client=0.0.0.0 
      -datacenter=dc1
      -encrypt=${CONSUL_ENCRYPT_KEY:-Luj2FZWwlt8475wD1WtwUQ==}
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.labels.consul == server2
    logging: *default-logging

  consul-server-3:
    image: consul:1.17
    networks:
      - ai-mesh
    volumes:
      - consul-data-3:/consul/data
      - ./config/consul:/consul/config
    environment:
      CONSUL_BIND_INTERFACE: eth0
      CONSUL_CLIENT_INTERFACE: eth0
    command: |
      agent -server 
      -node=consul-server-3 
      -bootstrap-expect=3 
      -retry-join=consul-server-1 
      -client=0.0.0.0 
      -datacenter=dc1
      -encrypt=${CONSUL_ENCRYPT_KEY:-Luj2FZWwlt8475wD1WtwUQ==}
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.labels.consul == server3
    logging: *default-logging

  # Consul Template for dynamic configuration
  consul-template:
    image: hashicorp/consul-template:0.35-alpine
    networks:
      - ai-mesh
    volumes:
      - ./config/consul-template:/consul-template
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      CONSUL_HTTP_ADDR: consul-server-1:8500
    command: |
      -config=/consul-template/config.hcl
      -log-level=info
    deploy:
      <<: *deploy-replicated
    depends_on:
      - consul-server-1
    logging: *default-logging

  # ======================
  # LAYER 3: Message Queue Cluster
  # ======================
  
  rabbitmq-1:
    image: rabbitmq:3.12-management-alpine
    hostname: rabbitmq-1
    networks:
      - ai-mesh
    ports:
      - target: 5672
        published: 5672
        mode: host
      - target: 15672
        published: 15672
        mode: host
    volumes:
      - rabbitmq-data-1:/var/lib/rabbitmq
      - ./config/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./config/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json
    environment:
      RABBITMQ_ERLANG_COOKIE: ${RABBITMQ_ERLANG_COOKIE:-secret-cookie}
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-admin}
      RABBITMQ_USE_LONGNAME: "true"
      RABBITMQ_NODENAME: rabbit@rabbitmq-1
    deploy:
      placement:
        constraints:
          - node.labels.rabbitmq == node1
    logging: *default-logging
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      <<: *default-healthcheck

  rabbitmq-2:
    image: rabbitmq:3.12-management-alpine
    hostname: rabbitmq-2
    networks:
      - ai-mesh
    volumes:
      - rabbitmq-data-2:/var/lib/rabbitmq
      - ./config/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    environment:
      RABBITMQ_ERLANG_COOKIE: ${RABBITMQ_ERLANG_COOKIE:-secret-cookie}
      RABBITMQ_USE_LONGNAME: "true"
      RABBITMQ_NODENAME: rabbit@rabbitmq-2
    command: |
      bash -c "
        sleep 30;
        rabbitmqctl stop_app;
        rabbitmqctl reset;
        rabbitmqctl join_cluster rabbit@rabbitmq-1;
        rabbitmqctl start_app;
        tail -f /dev/null
      "
    deploy:
      placement:
        constraints:
          - node.labels.rabbitmq == node2
    depends_on:
      - rabbitmq-1
    logging: *default-logging

  rabbitmq-3:
    image: rabbitmq:3.12-management-alpine
    hostname: rabbitmq-3
    networks:
      - ai-mesh
    volumes:
      - rabbitmq-data-3:/var/lib/rabbitmq
      - ./config/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    environment:
      RABBITMQ_ERLANG_COOKIE: ${RABBITMQ_ERLANG_COOKIE:-secret-cookie}
      RABBITMQ_USE_LONGNAME: "true"
      RABBITMQ_NODENAME: rabbit@rabbitmq-3
    command: |
      bash -c "
        sleep 30;
        rabbitmqctl stop_app;
        rabbitmqctl reset;
        rabbitmqctl join_cluster rabbit@rabbitmq-1;
        rabbitmqctl start_app;
        tail -f /dev/null
      "
    deploy:
      placement:
        constraints:
          - node.labels.rabbitmq == node3
    depends_on:
      - rabbitmq-1
    logging: *default-logging

  # ======================
  # LAYER 4: Redis Cluster
  # ======================
  
  redis-master-1:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-1:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      placement:
        constraints:
          - node.labels.redis == master1
    logging: *default-logging
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      <<: *default-healthcheck

  redis-slave-1:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-2:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis-master-1 6379
    deploy:
      placement:
        constraints:
          - node.labels.redis == slave1
    depends_on:
      - redis-master-1
    logging: *default-logging

  redis-master-2:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-3:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      placement:
        constraints:
          - node.labels.redis == master2
    logging: *default-logging

  redis-slave-2:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-4:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis-master-2 6379
    deploy:
      placement:
        constraints:
          - node.labels.redis == slave2
    depends_on:
      - redis-master-2
    logging: *default-logging

  redis-master-3:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-5:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      placement:
        constraints:
          - node.labels.redis == master3
    logging: *default-logging

  redis-slave-3:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - redis-data-6:/data
      - ./config/redis/redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf --slaveof redis-master-3 6379
    deploy:
      placement:
        constraints:
          - node.labels.redis == slave3
    depends_on:
      - redis-master-3
    logging: *default-logging

  # Redis Sentinel for automatic failover
  redis-sentinel:
    image: redis:7-alpine
    networks:
      - cache-tier
      - ai-mesh
    volumes:
      - ./config/redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    deploy:
      replicas: 3
      placement:
        preferences:
          - spread: node.id
    depends_on:
      - redis-master-1
      - redis-master-2
      - redis-master-3
    logging: *default-logging

  # ======================
  # LAYER 5: Database Layer
  # ======================
  
  postgres-primary:
    image: postgres:16-alpine
    networks:
      - data-tier
      - ai-mesh
    volumes:
      - postgres-primary:/var/lib/postgresql/data
      - ./config/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./config/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf
    environment:
      POSTGRES_DB: sutazai
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai}
      POSTGRES_REPLICATION_MODE: master
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator}
    command: |
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf 
      -c hba_file=/etc/postgresql/pg_hba.conf
    deploy:
      placement:
        constraints:
          - node.labels.postgres == primary
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sutazai"]
      <<: *default-healthcheck

  postgres-replica-1:
    image: postgres:16-alpine
    networks:
      - data-tier
      - ai-mesh
    volumes:
      - postgres-replica-1:/var/lib/postgresql/data
    environment:
      POSTGRES_REPLICATION_MODE: slave
      POSTGRES_MASTER_SERVICE: postgres-primary
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator}
    deploy:
      placement:
        constraints:
          - node.labels.postgres == replica1
    depends_on:
      - postgres-primary
    logging: *default-logging

  postgres-replica-2:
    image: postgres:16-alpine
    networks:
      - data-tier
      - ai-mesh
    volumes:
      - postgres-replica-2:/var/lib/postgresql/data
    environment:
      POSTGRES_REPLICATION_MODE: slave
      POSTGRES_MASTER_SERVICE: postgres-primary
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD:-replicator}
    deploy:
      placement:
        constraints:
          - node.labels.postgres == replica2
    depends_on:
      - postgres-primary
    logging: *default-logging

  # PgBouncer for connection pooling
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    networks:
      - data-tier
      - ai-mesh
    volumes:
      - ./config/pgbouncer:/etc/pgbouncer
    environment:
      DATABASES_HOST: postgres-primary
      DATABASES_PORT: 5432
      DATABASES_USER: sutazai
      DATABASES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai}
      DATABASES_DBNAME: sutazai
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 25
    deploy:
      <<: *deploy-replicated
    depends_on:
      - postgres-primary
    logging: *default-logging

  # ======================
  # LAYER 6: AI Services (Scalable)
  # ======================
  
  # Ollama with multiple instances
  ollama:
    image: ollama/ollama:latest
    networks:
      - ai-mesh
    volumes:
      - shared-models:/root/.ollama
      - ./config/ollama:/config
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_NUM_PARALLEL: 4
      OLLAMA_MAX_LOADED_MODELS: 3
      OLLAMA_KEEP_ALIVE: 5m
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.labels.gpu == true
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          memory: 4G
          cpus: '2'
          generic_resources:
            - discrete_resource_spec:
                kind: 'gpu'
                value: 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      <<: *default-healthcheck
    logging: *default-logging

  # AI Agent Pool (Auto-scaled)
  ai-agent:
    image: sutazai/ai-agent:latest
    networks:
      - ai-mesh
    environment:
      CONSUL_SERVICE_NAME: ai-agent
      CONSUL_SERVICE_PORT: 8080
      CONSUL_HTTP_ADDR: consul-server-1:8500
      REDIS_CLUSTER: redis-master-1:6379,redis-master-2:6379,redis-master-3:6379
      RABBITMQ_CLUSTER: rabbitmq-1:5672,rabbitmq-2:5672,rabbitmq-3:5672
      OLLAMA_HOSTS: ollama:11434
      DB_CONNECTION_POOL: pgbouncer:6432
      JAEGER_AGENT_HOST: jaeger
      JAEGER_AGENT_PORT: 6831
    deploy:
      replicas: 10
      update_config:
        parallelism: 2
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      <<: *default-healthcheck
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ai-agent.rule=PathPrefix(`/api/ai`)"
      - "traefik.http.services.ai-agent.loadbalancer.server.port=8080"
      - "consul.service.name=ai-agent"
      - "consul.service.port=8080"
      - "consul.service.check.http=/health"
      - "consul.service.check.interval=10s"

  # Celery Workers for distributed tasks
  celery-worker:
    image: sutazai/celery-worker:latest
    networks:
      - ai-mesh
    environment:
      CELERY_BROKER_URL: amqp://admin:${RABBITMQ_PASSWORD:-admin}@rabbitmq-1:5672//
      CELERY_RESULT_BACKEND: redis://redis-master-1:6379/0
      CELERY_TASK_ROUTES: |
        {
          'ai.inference.*': {'queue': 'inference', 'routing_key': 'ai.inference'},
          'ai.training.*': {'queue': 'training', 'routing_key': 'ai.training'},
          'data.processing.*': {'queue': 'processing', 'routing_key': 'data.proc'}
        }
    command: celery -A tasks worker --loglevel=info --concurrency=4
    deploy:
      replicas: 5
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    depends_on:
      - rabbitmq-1
      - redis-master-1
    logging: *default-logging

  # Celery Beat for scheduled tasks
  celery-beat:
    image: sutazai/celery-worker:latest
    networks:
      - ai-mesh
    environment:
      CELERY_BROKER_URL: amqp://admin:${RABBITMQ_PASSWORD:-admin}@rabbitmq-1:5672//
      CELERY_RESULT_BACKEND: redis://redis-master-1:6379/0
    command: celery -A tasks beat --loglevel=info
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    depends_on:
      - rabbitmq-1
      - redis-master-1
    logging: *default-logging

  # Flower for task monitoring
  flower:
    image: mher/flower:latest
    networks:
      - ai-mesh
    ports:
      - "5555:5555"
    environment:
      CELERY_BROKER_URL: amqp://admin:${RABBITMQ_PASSWORD:-admin}@rabbitmq-1:5672//
      FLOWER_BASIC_AUTH: admin:${FLOWER_PASSWORD:-admin}
    command: flower --broker_api=amqp://admin:${RABBITMQ_PASSWORD:-admin}@rabbitmq-1:15672/api/
    deploy:
      replicas: 1
    depends_on:
      - rabbitmq-1
    logging: *default-logging

  # ======================
  # LAYER 7: Monitoring & Observability
  # ======================
  
  prometheus:
    image: prom/prometheus:latest
    networks:
      - monitoring
      - ai-mesh
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./config/prometheus:/etc/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    deploy:
      placement:
        constraints:
          - node.role == manager
    logging: *default-logging

  grafana:
    image: grafana/grafana:latest
    networks:
      - monitoring
      - ai-mesh
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: redis-datasource,redis-app
    deploy:
      placement:
        constraints:
          - node.role == manager
    depends_on:
      - prometheus
    logging: *default-logging

  jaeger:
    image: jaegertracing/all-in-one:latest
    networks:
      - monitoring
      - ai-mesh
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"  # UI
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: ":9411"
      SPAN_STORAGE_TYPE: badger
      BADGER_EPHEMERAL: "false"
      BADGER_DIRECTORY_VALUE: /badger/data
      BADGER_DIRECTORY_KEY: /badger/key
    volumes:
      - jaeger-data:/badger
    deploy:
      placement:
        constraints:
          - node.role == manager
    logging: *default-logging

  # Alertmanager for alert routing
  alertmanager:
    image: prom/alertmanager:latest
    networks:
      - monitoring
      - ai-mesh
    ports:
      - "11016:11016"
    volumes:
      - ./config/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == manager
    logging: *default-logging

  # Node Exporter for host metrics
  node-exporter:
    image: prom/node-exporter:latest
    networks:
      - monitoring
    ports:
      - target: 9100
        published: 9100
        mode: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    deploy:
      mode: global
    logging: *default-logging

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    networks:
      - monitoring
    ports:
      - target: 8080
        published: 8090
        mode: host
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    deploy:
      mode: global
    logging: *default-logging

  # ======================
  # LAYER 8: Service Mesh & Auto-scaling
  # ======================
  
  # Traefik as dynamic reverse proxy
  traefik:
    image: traefik:v3.0
    networks:
      - ai-mesh
    ports:
      - "8080:8080"  # Dashboard
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/traefik:/etc/traefik
    command:
      - --api.dashboard=true
      - --providers.docker=true
      - --providers.docker.swarmMode=true
      - --providers.consul=true
      - --providers.consul.endpoint.address=consul-server-1:8500
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --metrics.prometheus=true
      - --tracing.jaeger=true
      - --tracing.jaeger.samplingType=const
      - --tracing.jaeger.samplingParam=1.0
      - --tracing.jaeger.localAgentHostPort=jaeger:6831
    deploy:
      placement:
        constraints:
          - node.role == manager
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.api.rule=Host(`traefik.localhost`)"
        - "traefik.http.routers.api.service=api@internal"
    depends_on:
      - consul-server-1
      - jaeger
    logging: *default-logging

  # Auto-scaler service
  autoscaler:
    image: sutazai/autoscaler:latest
    networks:
      - ai-mesh
      - monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./config/autoscaler:/config
    environment:
      DOCKER_HOST: unix:///var/run/docker.sock
      PROMETHEUS_URL: http://prometheus:9090
      CONSUL_HTTP_ADDR: consul-server-1:8500
      SCALING_RULES_PATH: /config/scaling-rules.yaml
      CHECK_INTERVAL: 30s
      COOLDOWN_SCALE_UP: 60s
      COOLDOWN_SCALE_DOWN: 300s
    deploy:
      placement:
        constraints:
          - node.role == manager
    depends_on:
      - prometheus
      - consul-server-1
    logging: *default-logging