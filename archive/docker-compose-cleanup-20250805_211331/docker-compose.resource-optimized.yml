version: '3.8'

# Optimized Resource Allocation Configuration for SutazAI System
# Total System Resources: 12 CPU cores, 29.38GB RAM
# Target Utilization: 40-60% CPU, 80% Memory

x-resource-pools: &resource-pools
  # Tier 1: Critical Infrastructure Services
  tier1-infrastructure: &tier1-resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 60s

  # Tier 2: Active AI Agents
  tier2-agents: &tier2-resources
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 5
        window: 120s

  # Tier 3: On-Demand Services
  tier3-ondemand: &tier3-resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.25'
          memory: 256M
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
        window: 300s

x-health-checks: &standard-health
  healthcheck:
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
    
x-optimized-health: &optimized-health
  healthcheck:
    interval: 60s
    timeout: 15s
    retries: 5
    start_period: 120s

services:
  # TIER 1: CRITICAL INFRASTRUCTURE SERVICES
  
  backend:
    <<: *tier1-resources
    <<: *standard-health
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      placement:
        constraints:
          - node.labels.pool == infrastructure
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
    environment:
      - CPU_AFFINITY=2,3
      - WORKER_PROCESSES=2
      - MAX_CONNECTIONS=1000
    ulimits:
      nofile:
        soft: 65536
        hard: 65536

  frontend:
    <<: *tier1-resources
    <<: *standard-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - CPU_AFFINITY=2,3

  postgres:
    <<: *tier1-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      placement:
        constraints:
          - node.labels.pool == database
    environment:
      - CPU_AFFINITY=4,5
      - POSTGRES_SHARED_BUFFERS=512MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
      - POSTGRES_WORK_MEM=64MB
      - POSTGRES_MAINTENANCE_WORK_MEM=256MB
      - POSTGRES_MAX_CONNECTIONS=200
    command: >
      postgres
      -c shared_buffers=512MB
      -c effective_cache_size=1GB
      -c work_mem=64MB
      -c maintenance_work_mem=256MB
      -c max_connections=200
      -c random_page_cost=1.1
      -c checkpoint_completion_target=0.9

  redis:
    <<: *tier2-resources
    <<: *standard-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    environment:
      - CPU_AFFINITY=4,5
    command: >
      redis-server
      --maxmemory 400mb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --timeout 300

  ollama:
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
      placement:
        constraints:
          - node.labels.pool == llm-compute
    environment:
      - CPU_AFFINITY=6,7,8,9
      - OLLAMA_NUM_THREADS=4
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    sysctls:
      - net.core.somaxconn=32768
    ulimits:
      nofile:
        soft: 32768
        hard: 32768

  # TIER 2: ACTIVE AI AGENTS

  hardware-resource-optimizer:
    <<: *tier2-resources
    <<: *standard-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      placement:
        constraints:
          - node.labels.pool == active-agents
    environment:
      - CPU_AFFINITY=10,11
      - OPTIMIZATION_INTERVAL=60
    privileged: true
    pid: host

  health-monitor:
    <<: *tier2-resources
    <<: *standard-health
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    environment:
      - CPU_AFFINITY=10,11
      - MONITOR_INTERVAL=30
      - BATCH_SIZE=10

  # MONITORING STACK

  prometheus:
    <<: *tier2-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - CPU_AFFINITY=10,11
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--storage.tsdb.retention.size=8GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.max-concurrency=4'

  grafana:
    <<: *tier2-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - CPU_AFFINITY=10,11
      - GF_DATABASE_MAX_OPEN_CONN=10
      - GF_DATABASE_MAX_IDLE_CONN=5

  # VECTOR DATABASES

  chromadb:
    <<: *tier2-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - CPU_AFFINITY=4,5
      - CHROMA_SERVER_HTTP_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]

  qdrant:
    <<: *tier2-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - CPU_AFFINITY=4,5
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334

  neo4j:
    <<: *tier1-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    environment:
      - CPU_AFFINITY=4,5
      - NEO4J_dbms_memory_heap_max__size=1G
      - NEO4J_dbms_memory_pagecache_size=512M
      - NEO4J_dbms_memory_transaction_max__size=256M

  # TIER 3: ON-DEMAND ML/AI SERVICES

  pytorch:
    <<: *tier3-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '0.5'
          memory: 512M
      replicas: 0  # Start with 0 replicas, scale on demand
    environment:
      - CPU_AFFINITY=8,9,10,11
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4

  tensorflow:
    <<: *tier3-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '0.5'
          memory: 512M
      replicas: 0  # Start with 0 replicas, scale on demand
    environment:
      - CPU_AFFINITY=8,9,10,11
      - TF_NUM_INTEROP_THREADS=2
      - TF_NUM_INTRAOP_THREADS=4

  jax:
    <<: *tier3-resources
    <<: *optimized-health
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '0.5'
          memory: 512M
      replicas: 0  # Start with 0 replicas, scale on demand
    environment:
      - CPU_AFFINITY=8,9,10,11
      - JAX_PLATFORM_NAME=cpu

  # DEVELOPMENT TOOLS

  aider:
    <<: *tier3-resources
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      replicas: 0
    environment:
      - CPU_AFFINITY=11

  gpt-engineer:
    <<: *tier3-resources
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      replicas: 0
    environment:
      - CPU_AFFINITY=11

  # BATCH/BACKGROUND SERVICES

  code-improver:
    <<: *tier3-resources
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 60s
        max_attempts: 2
        window: 600s
    environment:
      - CPU_AFFINITY=11
      - BATCH_MODE=true

networks:
  sutazai-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: sutazai-br0
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.driver.mtu: 1500
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/postgres
  
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/redis
      
  ollama_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/ollama
      
  models_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/models
      
  chromadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/chromadb
      
  qdrant_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/qdrant
      
  neo4j_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/sutazaiapp/data/neo4j

# Resource monitoring and alerting
x-monitoring-config:
  resource_limits:
    cpu_threshold: 80
    memory_threshold: 85
    disk_threshold: 90
  
  scaling_policies:
    scale_up_threshold: 70
    scale_down_threshold: 30
    cooldown_period: 300
    
  health_check_policy:
    stagger_interval: 5s
    max_concurrent_checks: 10
    failure_escalation: exponential