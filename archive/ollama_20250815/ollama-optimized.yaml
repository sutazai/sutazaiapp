# ULTRA-OPTIMIZED Ollama Configuration for <2s Response Time
# Optimized for TinyLlama (637MB) with aggressive performance tuning

model:
  name: tinyllama
  version: latest
  
# CRITICAL PERFORMANCE SETTINGS
performance:
  # Model preloading for instant response
  preload_models:
    - tinyllama
  keep_models_loaded: true
  model_memory_lock: true  # Pin model in memory
  
  # Context optimization for TinyLlama
  context_size: 2048       # Reduced from default 4096 for faster processing
  batch_size: 64           # Increased batch processing
  
  # Threading optimization
  num_threads: 12          # Use all available CPU threads
  num_parallel: 8          # Parallel request processing
  
  # GPU acceleration (if available)
  use_gpu: auto            # Auto-detect and use GPU if available
  gpu_layers: 32           # Load all layers to GPU if available
  
# Response optimization
inference:
  temperature: 0.1         # Lower temperature for faster, deterministic responses
  max_tokens: 256          # Limit response length for speed
  top_k: 10               # Reduced from 20 for faster sampling
  top_p: 0.85             # Slightly reduced for faster processing
  repeat_penalty: 1.05    #   penalty for speed
  
  # Streaming settings
  stream_buffer_size: 16   # Smaller buffer for faster first token
  chunk_size: 32          # Optimal chunk size for streaming
  
# Memory optimization
memory:
  model_cache_size: 1024MB  # Dedicated cache for model
  kv_cache_size: 512MB      # Key-value cache
  use_mmap: true            # Memory-mapped file for faster loading
  use_mlock: true           # Lock model in RAM
  
# Connection pooling
connection:
  max_connections: 50       # High connection pool
  keepalive_ms: 60000      # 60 second keepalive
  timeout_ms: 10000        # 10 second timeout
  
  # Request queuing
  queue_size: 100          # Large queue for burst handling
  queue_timeout_ms: 5000   # 5 second queue timeout
  
# Caching strategy
cache:
  response_cache_enabled: true
  cache_size: 1000         # Cache last 1000 responses
  cache_ttl_seconds: 3600  # 1 hour TTL
  
  # Semantic cache for similar prompts
  semantic_cache_enabled: true
  similarity_threshold: 0.95
  
# Load balancing (for multiple instances)
load_balancing:
  enabled: false           # Disable for single instance optimization
  strategy: least_loaded
  
# Monitoring
monitoring:
  metrics_enabled: true
  log_level: WARN         # Reduce logging overhead
  performance_tracking: true
  latency_histogram_buckets: [0.1, 0.2, 0.5, 1.0, 2.0, 5.0]