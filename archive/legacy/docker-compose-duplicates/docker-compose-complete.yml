version: '3.8'

networks:
  sutazai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  models-data:
  vector-data:
  chroma-data:
  qdrant-data:
  postgres-data:
  redis-data:
  grafana-data:
  prometheus-data:
  ollama-data:
  workspace-data:
  logs-data:
  neuromorphic-data:
  self-improvement-data:
  web-learning-data:

services:
  # Core Infrastructure
  postgres:
    image: postgres:15
    container_name: sutazai-postgres
    environment:
      POSTGRES_DB: sutazai
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: sutazai_password
      POSTGRES_MULTIPLE_DATABASES: sutazai,vector_store,agent_memory
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.10
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sutazai"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: sutazai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.11
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Vector Databases & Memory Systems
  qdrant:
    image: qdrant/qdrant:latest
    container_name: sutazai-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
      - ./config/qdrant.yaml:/qdrant/config/production.yaml
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__LOG_LEVEL: INFO
      QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS: 8
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.12
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  chromadb:
    image: chromadb/chroma:latest
    container_name: sutazai-chromadb
    ports:
      - "8001:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
      CHROMA_SERVER_CORS_ALLOW_ORIGINS: '["*"]'
      CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER: "chromadb.auth.simple_rbac_authz.SimpleRBACAuthzConfigurationProvider"
      CHROMA_SERVER_AUTH_PROVIDER: "chromadb.auth.basic.BasicAuthenticationServerProvider"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.13
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # FAISS Vector Similarity Search
  faiss:
    build:
      context: ./docker/faiss
      dockerfile: Dockerfile
    container_name: sutazai-faiss
    ports:
      - "8002:8000"
    environment:
      FAISS_DATA_PATH: /data/faiss_indexes
      FAISS_MAX_MEMORY_GB: 16
      FAISS_INDEX_TYPE: IVFFlat
    volumes:
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.14
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Model Management & Inference
  ollama:
    image: ollama/ollama:latest
    container_name: sutazai-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./scripts/init-ollama.sh:/init-ollama.sh
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_ORIGINS: "*"
      OLLAMA_KEEP_ALIVE: "24h"
      OLLAMA_MAX_LOADED_MODELS: 4
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.15
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    command: >
      sh -c "ollama serve &
             sleep 10 &&
             ollama pull deepseek-r1:8b &&
             ollama pull qwen3:8b &&
             ollama pull deepseek-coder:33b &&
             ollama pull llama3.2:1b &&
             ollama pull codellama:7b &&
             wait"

  # Enhanced Model Manager
  enhanced-model-manager:
    build:
      context: ./docker/enhanced-model-manager
      dockerfile: Dockerfile
    container_name: sutazai-enhanced-model-manager
    ports:
      - "8003:8000"
    environment:
      MODEL_CACHE_PATH: /data/models
      DEEPSEEK_MODEL_PATH: /data/models/deepseek-coder
      LLAMA_MODEL_PATH: /data/models/llama2
      QWEN_MODEL_PATH: /data/models/qwen3
      OLLAMA_URL: http://ollama:11434
      GPU_MEMORY_LIMIT: 24GB
      QUANTIZATION_ENABLED: "true"
    volumes:
      - models-data:/data/models
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.16
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Core AI Agents
  autogpt:
    build:
      context: ./docker/autogpt
      dockerfile: Dockerfile
    container_name: sutazai-autogpt
    ports:
      - "8010:8000"
    volumes:
      - workspace-data:/app/autogpt/auto_gpt_workspace
      - logs-data:/app/logs
    environment:
      OPENAI_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
      MEMORY_BACKEND: "qdrant"
      WEAVIATE_URL: "http://qdrant:6333"
      AUTOGPT_WORKSPACE: "/app/autogpt/auto_gpt_workspace"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.20
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped

  # LocalAGI service removed
    build:
      context: ./docker/localagi
      dockerfile: Dockerfile
    container_name: sutazai-localagi
    ports:
      - "8011:8080"
    volumes:
      - models-data:/models
      - workspace-data:/workspace
    environment:
      LOCALAI_ADDRESS: "0.0.0.0:8080"
      LOCALAI_MODELS_PATH: "/models"
      LOCALAI_CONTEXT_SIZE: 4096
      LOCALAI_THREADS: 8
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.21
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  tabbyml:
    build:
      context: ./docker/tabbyml
      dockerfile: Dockerfile
    container_name: sutazai-tabbyml
    ports:
      - "8012:8080"
    volumes:
      - models-data:/data
    environment:
      TABBY_MODEL: "CodeLlama-7B"
      TABBY_DEVICE: "cuda"
      TABBY_HOST: "0.0.0.0"
      TABBY_PORT: 8080
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.22
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  semgrep:
    build:
      context: ./docker/semgrep
      dockerfile: Dockerfile
    container_name: sutazai-semgrep
    ports:
      - "8013:8000"
    volumes:
      - workspace-data:/src
      - logs-data:/logs
    environment:
      SEMGREP_APP_TOKEN: "local"
      SEMGREP_BASELINE_REF: "main"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.23
    restart: unless-stopped

  # LangChain & AutoGen Agents
  langchain-agents:
    build:
      context: ./docker/langchain-agents
      dockerfile: Dockerfile
    container_name: sutazai-langchain-agents
    ports:
      - "8014:8000"
    volumes:
      - workspace-data:/workspace
      - logs-data:/logs
    environment:
      OPENAI_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
      LANGCHAIN_TRACING_V2: "true"
      LANGCHAIN_PROJECT: "sutazai"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.24
    depends_on:
      - ollama
    restart: unless-stopped

  autogen-agents:
    build:
      context: ./docker/autogen
      dockerfile: Dockerfile
    container_name: sutazai-autogen
    ports:
      - "8015:8000"
    environment:
      AUTOGEN_USE_DOCKER: "False"
      OPENAI_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
      AUTOGEN_CONFIG_PATH: "/config"
    volumes:
      - workspace-data:/data
      - logs-data:/logs
      - ./config/autogen:/config
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.25
    depends_on:
      - ollama
    restart: unless-stopped

  agentzero:
    build:
      context: ./docker/agentzero
      dockerfile: Dockerfile
    container_name: sutazai-agentzero
    ports:
      - "8016:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      AGENTZERO_API_KEY: "local"
      AGENTZERO_MODEL_URL: "http://ollama:11434"
      AGENTZERO_VECTOR_DB: "http://qdrant:6333"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.26
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped

  # BigAGI service removed
    build:
      context: ./docker/bigagi
      dockerfile: Dockerfile
    container_name: sutazai-bigagi
    ports:
      - "8017:3000"
    volumes:
      - workspace-data:/workspace
    environment:
      NEXT_PUBLIC_BACKEND_URL: "http://sutazai-backend:8000"
      OPENAI_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
      BIGAGI_MEMORY_STORE: "qdrant"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.27
    depends_on:
      - ollama
    restart: unless-stopped

  # Web Automation & Browser Agents
  browser-use:
    build:
      context: ./docker/browser-use
      dockerfile: Dockerfile
    container_name: sutazai-browser-use
    ports:
      - "8018:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      DISPLAY: ":99"
      BROWSER_HEADLESS: "true"
      BROWSER_USE_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.28
    depends_on:
      - ollama
    restart: unless-stopped

  skyvern:
    build:
      context: ./docker/skyvern
      dockerfile: Dockerfile
    container_name: sutazai-skyvern
    ports:
      - "8019:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      SKYVERN_API_KEY: "local"
      SKYVERN_BASE_URL: "http://localhost:8019"
      DATABASE_URL: "postgresql://sutazai:sutazai_password@postgres:5432/sutazai"
      REDIS_URL: "redis://redis:6379"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.29
    depends_on:
      - postgres
      - redis
      - ollama
    restart: unless-stopped

  # Document & Financial Processing
  documind:
    build:
      context: ./docker/documind
      dockerfile: Dockerfile
    container_name: sutazai-documind
    ports:
      - "8020:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      DOCUMIND_API_KEY: "local"
      DOCUMIND_STORAGE_PATH: "/workspace/documents"
      OPENAI_API_BASE: "http://ollama:11434/v1"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.30
    depends_on:
      - ollama
    restart: unless-stopped

  finrobot:
    build:
      context: ./docker/finrobot
      dockerfile: Dockerfile
    container_name: sutazai-finrobot
    ports:
      - "8021:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      FINROBOT_DATA_PATH: "/workspace/financial_data"
      DATABASE_URL: "postgresql://sutazai:sutazai_password@postgres:5432/sutazai"
      REDIS_URL: "redis://redis:6379"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.31
    depends_on:
      - postgres
      - redis
      - ollama
    restart: unless-stopped

  # Code Generation & Development
  gpt-engineer:
    build:
      context: ./docker/gpt-engineer
      dockerfile: Dockerfile
    container_name: sutazai-gpt-engineer
    ports:
      - "8022:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      OPENAI_API_KEY: "local"
      OPENAI_API_BASE: "http://ollama:11434/v1"
      GPT_ENGINEER_WORKSPACE: "/workspace/projects"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.32
    depends_on:
      - ollama
    restart: unless-stopped

  aider:
    build:
      context: ./docker/aider
      dockerfile: Dockerfile
    container_name: sutazai-aider
    ports:
      - "8023:8000"
    volumes:
      - workspace-data:/workspace
    environment:
      AIDER_OPENAI_API_KEY: "local"
      AIDER_OPENAI_API_BASE: "http://ollama:11434/v1"
      AIDER_AUTO_COMMITS: "true"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.33
    depends_on:
      - ollama
    restart: unless-stopped

  # Advanced UI Interfaces
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: sutazai-open-webui
    ports:
      - "8030:8080"
    volumes:
      - workspace-data:/app/backend/data
    environment:
      OLLAMA_BASE_URL: "http://ollama:11434"
      WEBUI_SECRET_KEY: "sutazai-secret-key"
      WEBUI_AUTH: "False"
      ENABLE_RAG_HYBRID_SEARCH: "True"
      RAG_EMBEDDING_ENGINE: "ollama"
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.40
    depends_on:
      - ollama
    restart: unless-stopped

  langflow:
    build:
      context: ./docker/langflow
      dockerfile: Dockerfile
    container_name: sutazai-langflow
    ports:
      - "7860:7860"
    environment:
      LANGFLOW_DATABASE_URL: "sqlite:///./langflow.db"
      LANGFLOW_HOST: "0.0.0.0"
      LANGFLOW_PORT: 7860
      LANGFLOW_WORKERS: 4
    volumes:
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.41
    depends_on:
      - ollama
    restart: unless-stopped

  dify:
    build:
      context: ./docker/dify
      dockerfile: Dockerfile
    container_name: sutazai-dify
    ports:
      - "5001:5001"
    environment:
      EDITION: "COMMUNITY"
      DEPLOY_ENV: "PRODUCTION"
      DATABASE_URL: "postgresql://sutazai:sutazai_password@postgres:5432/sutazai"
      REDIS_URL: "redis://redis:6379"
      SECRET_KEY: "sutazai-dify-secret"
    volumes:
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.42
    depends_on:
      - postgres
      - redis
      - ollama
    restart: unless-stopped

  # Machine Learning Frameworks
  pytorch:
    build:
      context: ./docker/pytorch
      dockerfile: Dockerfile
    container_name: sutazai-pytorch
    ports:
      - "8040:8000"
    environment:
      TRANSFORMERS_CACHE: "/data/transformers"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      TORCH_HOME: "/data/torch"
    volumes:
      - models-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.50
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  tensorflow:
    build:
      context: ./docker/tensorflow
      dockerfile: Dockerfile
    container_name: sutazai-tensorflow
    ports:
      - "8041:8000"
    environment:
      TF_CPP_MIN_LOG_LEVEL: 2
      TF_FORCE_GPU_ALLOW_GROWTH: "true"
      TENSORFLOW_HOME: "/data/tensorflow"
    volumes:
      - models-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.51
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  jax:
    build:
      context: ./docker/jax
      dockerfile: Dockerfile
    container_name: sutazai-jax
    ports:
      - "8042:8000"
    environment:
      JAX_PLATFORM_NAME: "gpu"
      JAX_ENABLE_X64: "True"
      XLA_PYTHON_CLIENT_PREALLOCATE: "false"
    volumes:
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.52
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # Additional AI Services
  awesome-code-ai:
    build:
      context: ./docker/awesome-code-ai
      dockerfile: Dockerfile
    container_name: sutazai-awesome-code-ai
    ports:
      - "8043:8000"
    environment:
      CODE_AI_DATA_PATH: "/data/code_analysis"
      OPENAI_API_BASE: "http://ollama:11434/v1"
    volumes:
      - workspace-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.53
    depends_on:
      - ollama
    restart: unless-stopped

  # Self-Improvement & processing networks
  neuromorphic-engine:
    build:
      context: ./docker/neuromorphic
      dockerfile: Dockerfile
    container_name: sutazai-neuromorphic
    ports:
      - "8050:8000"
    environment:
      NEUROMORPHIC_DATA_PATH: "/data/neural_networks"
      STDP_LEARNING_ENABLED: "true"
      BIOLOGICAL_MODELING: "true"
    volumes:
      - neuromorphic-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.60
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  self-improvement-engine:
    build:
      context: ./docker/self-improvement
      dockerfile: Dockerfile
    container_name: sutazai-self-improvement
    ports:
      - "8051:8000"
    environment:
      IMPROVEMENT_DATA_PATH: "/data/improvements"
      CODE_GENERATION_ENABLED: "true"
      SAFETY_CHECKS_ENABLED: "true"
      HUMAN_APPROVAL_REQUIRED: "true"
    volumes:
      - self-improvement-data:/data
      - workspace-data:/workspace
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.61
    depends_on:
      - ollama
      - gpt-engineer
      - aider
    restart: unless-stopped

  web-learning-engine:
    build:
      context: ./docker/web-learning
      dockerfile: Dockerfile
    container_name: sutazai-web-learning
    ports:
      - "8052:8000"
    environment:
      WEB_DATA_PATH: "/data/web_content"
      SCRAPING_ENABLED: "true"
      LEARNING_RATE: "0.001"
      MAX_CONCURRENT_SCRAPERS: 10
    volumes:
      - web-learning-data:/data
      - logs-data:/logs
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.62
    depends_on:
      - browser-use
      - ollama
    restart: unless-stopped

  # Main SutazAI Backend
  sutazai-backend:
    build:
      context: ./backend
      dockerfile: ../docker/backend.Dockerfile
    container_name: sutazai-backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - workspace-data:/workspace
      - logs-data:/logs
      - models-data:/models
    environment:
      # Database Configuration
      - DATABASE_URL=postgresql://sutazai:sutazai_password@postgres:5432/sutazai
      - REDIS_URL=redis://redis:6379

      # Vector Databases
      - QDRANT_URL=http://qdrant:6333
      - CHROMADB_URL=http://chromadb:8000
      - FAISS_URL=http://faiss:8000

      # Model Management
      - OLLAMA_URL=http://ollama:11434
      - ENHANCED_MODEL_MANAGER_URL=http://enhanced-model-manager:8000

      # Core AI Agents
      - AUTOGPT_URL=http://autogpt:8000
      - LOCALAGI_URL=http://# LocalAGI service removed

    networks:
      sutazai-network:
        ipv4_address: 172.20.0.100
    depends_on:
      - postgres
      - redis
      - qdrant
      - chromadb
      - faiss
      - ollama
      - enhanced-model-manager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Streamlit Web UI
  sutazai-streamlit:
    build:
      context: ./frontend
      dockerfile: ../docker/streamlit.Dockerfile
    container_name: sutazai-streamlit
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app
      - workspace-data:/workspace
    environment:
      - BACKEND_URL=http://sutazai-backend:8000
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_THEME_BASE=dark
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.101
    depends_on:
      - sutazai-backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Monitoring & Observability
  prometheus:
    image: prom/prometheus:latest
    container_name: sutazai-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.110
    depends_on:
      - sutazai-backend
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: sutazai-grafana
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana:/etc/grafana
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel
      - GF_FEATURE_TOGGLES_ENABLE=ngalert
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.111
    depends_on:
      - prometheus
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: sutazai-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--collector.cpu'
      - '--collector.meminfo'
      - '--collector.diskstats'
      - '--collector.netdev'
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.112
    restart: unless-stopped

  # Reverse Proxy & Load Balancer
  nginx:
    image: nginx:alpine
    container_name: sutazai-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./nginx/logs:/var/log/nginx
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.120
    depends_on:
      - sutazai-backend
      - sutazai-streamlit
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Health Check & System Monitor
  health-check:
    build:
      context: ./docker/health-check
      dockerfile: Dockerfile
    container_name: sutazai-health-check
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - CHECK_INTERVAL=30
      - ALERT_WEBHOOK_URL=http://sutazai-backend:8000/api/alerts
      - SERVICES_TO_CHECK=sutazai-backend,sutazai-streamlit,postgres,redis,qdrant,chromadb,ollama,enhanced-model-manager,autogpt,localagi,tabbyml,semgrep,langchain-agents,autogen-agents,agentzero,bigagi,browser-use,skyvern,documind,finrobot,gpt-engineer,aider,open-webui,langflow,dify,pytorch,tensorflow,jax,awesome-code-ai,neuromorphic-engine,self-improvement-engine,web-learning-engine
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.130
    depends_on:
      - sutazai-backend
    restart: unless-stopped

  # System Orchestrator & Automation
  sutazai-orchestrator:
    build:
      context: ./docker/orchestrator
      dockerfile: Dockerfile
    container_name: sutazai-orchestrator
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - workspace-data:/workspace
      - logs-data:/logs
    environment:
      - ORCHESTRATOR_MODE=production
      - AUTO_SCALING_ENABLED=true
      - LOAD_BALANCING_ENABLED=true
      - SELF_HEALING_ENABLED=true
      - DEPLOYMENT_AUTOMATION=true
    networks:
      sutazai-network:
        ipv4_address: 172.20.0.131
    depends_on:
      - sutazai-backend
    restart: unless-stopped