version: '3.8'

x-resource-limits: &resource-limits
  deploy:
    resources:
      limits:
        memory: 1G
      reservations:
        memory: 512M

x-small-resource-limits: &small-resource-limits
  deploy:
    resources:
      limits:
        memory: 512M
      reservations:
        memory: 256M

x-ollama-resource-limits: &ollama-resource-limits
  deploy:
    resources:
      limits:
        memory: 4G
      reservations:
        memory: 2G

x-healthcheck: &default-healthcheck
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

services:
  # Core Infrastructure with Memory Limits
  nginx:
    image: nginx:alpine
    <<: *small-resource-limits
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
    depends_on:
      - fastapi-backend
      - streamlit-frontend
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "nginx", "-t"]

  postgresql:
    image: postgres:15-alpine
    <<: *resource-limits
    environment:
      POSTGRES_DB: sutazai
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sutazai2024}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=C --lc-ctype=C"
      POSTGRES_HOST_AUTH_METHOD: md5
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD-SHELL", "pg_isready -U sutazai"]
    command: >
      postgres
      -c shared_buffers=256MB
      -c work_mem=4MB
      -c maintenance_work_mem=64MB
      -c effective_cache_size=1GB
      -c max_connections=100

  redis:
    image: redis:7-alpine
    <<: *small-resource-limits
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "redis-cli", "ping"]
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes

  # Vector Databases with Memory Optimization
  chromadb:
    image: chromadb/chroma:latest
    <<: *resource-limits
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      CHROMA_SERVER_HOST: 0.0.0.0
      CHROMA_SERVER_HTTP_PORT: 8000
      ANONYMIZED_TELEMETRY: "false"
      ALLOW_RESET: "false"
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]

  qdrant:
    image: qdrant/qdrant:latest
    <<: *resource-limits
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__LOG_LEVEL: INFO
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]

  # Ollama with Strict Memory Limits and Model Management
  ollama:
    image: ollama/ollama:latest
    <<: *ollama-resource-limits
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-startup.sh:/startup.sh:ro
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_NUM_PARALLEL: 1
      OLLAMA_MAX_LOADED_MODELS: 1
      OLLAMA_FLASH_ATTENTION: 1
      OLLAMA_KEEP_ALIVE: 5m
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
    command: ["/bin/bash", "/startup.sh"]
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G

  # Core Backend with Memory Optimization
  fastapi-backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        - INSTALL_DEV_DEPS=false
    <<: *resource-limits
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://sutazai:${POSTGRES_PASSWORD:-sutazai2024}@postgresql:5432/sutazai
      REDIS_URL: redis://redis:6379
      CHROMADB_URL: http://chromadb:8000
      QDRANT_URL: http://qdrant:6333
      OLLAMA_URL: http://ollama:11434
      PYTHONUNBUFFERED: 1
      LOG_LEVEL: INFO
      WORKERS: 2
      MAX_REQUESTS: 1000
      MAX_REQUESTS_JITTER: 50
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
      chromadb:
        condition: service_started
      qdrant:
        condition: service_started
      ollama:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./backend:/app
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    command: >
      uvicorn app.main:app
      --host 0.0.0.0
      --port 8000
      --workers 2
      --limit-max-requests 1000
      --limit-max-requests-jitter 50

  # Streamlit Frontend with Memory Optimization
  streamlit-frontend:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    <<: *resource-limits
    ports:
      - "8501:8501"
    environment:
      BACKEND_URL: http://fastapi-backend:8000
      STREAMLIT_SERVER_MAX_MESSAGE_SIZE: 200
      STREAMLIT_SERVER_MAX_UPLOAD_SIZE: 200
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: false
      STREAMLIT_THEME_BASE: dark
      PYTHONUNBUFFERED: 1
    depends_on:
      fastapi-backend:
        condition: service_healthy
    volumes:
      - ./intelligent_chat_app_fixed.py:/app/app.py:ro
      - ./frontend:/app/frontend
    restart: unless-stopped
    healthcheck:
      <<: *default-healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8501/healthz"]
    command: >
      streamlit run app.py
      --server.port=8501
      --server.address=0.0.0.0
      --server.maxMessageSize=200
      --server.maxUploadSize=200
      --browser.gatherUsageStats=false

  # Lightweight Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    <<: *small-resource-limits
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=1GB'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    <<: *small-resource-limits
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: ""
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped

  # Health Check Service
  health-check:
    build:
      context: .
      dockerfile: Dockerfile.healthcheck
    <<: *small-resource-limits
    environment:
      CHECK_INTERVAL: 60
      ALERT_WEBHOOK: ${ALERT_WEBHOOK:-}
    depends_on:
      - fastapi-backend
      - ollama
    restart: unless-stopped

  # Optional Services (disabled by default to save memory)
  # Uncomment services as needed

  # tabbyml:
  #   image: tabbyml/tabby:latest
  #   <<: *resource-limits
  #   ports:
  #     - "8081:8080"
  #   volumes:
  #     - tabby_data:/data
  #   command: serve --model TabbyML/StarCoder-1B --device cpu
  #   restart: unless-stopped
  #   profiles:
  #     - full

  # documind:
  #   build:
  #     context: ./services/documind
  #     dockerfile: Dockerfile
  #   <<: *resource-limits
  #   ports:
  #     - "8002:8000"
  #   volumes:
  #     - document_storage:/app/storage
  #   restart: unless-stopped
  #   profiles:
  #     - full

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  chromadb_data:
    driver: local
  qdrant_data:
    driver: local
  ollama_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  tabby_data:
    driver: local
  document_storage:
    driver: local

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16