# ============================================================================
# SUTAZAI MASTER TEMPLATE: Testing Service Base
# ============================================================================
# Purpose: Production-ready testing service with comprehensive frameworks
# Security: Non-root user, secure test execution environment
# Performance: Optimized for CI/CD and automated testing
# Compatibility: pytest, unittest, integration tests, load testing
# Author: ULTRA DEPLOYMENT ENGINEER
# Date: August 10, 2025
# Version: v1.0.0
# ============================================================================

FROM python:3.12.8-slim-bookworm as base

# ============================================================================
# TESTING ENVIRONMENT CONFIGURATION
# ============================================================================

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    DEBIAN_FRONTEND=noninteractive \
    PYTEST_ADDOPTS="--tb=short --strict-markers"

# Install testing system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        ca-certificates \
        git \
        build-essential \
        libpq-dev \
        chromium \
        chromium-driver \
        && rm -rf /var/lib/apt/lists/*

# Create testing user
RUN groupadd --gid 1000 tester && \
    useradd --uid 1000 --gid 1000 --create-home --shell /bin/bash tester

# ============================================================================
# TESTING FRAMEWORK INSTALLATION
# ============================================================================

WORKDIR /app

# Install comprehensive testing dependencies
RUN pip install --upgrade pip setuptools wheel

RUN pip install --no-cache-dir \
        pytest>=7.4.3 \
        pytest-asyncio>=0.21.1 \
        pytest-cov>=4.1.0 \
        pytest-html>=4.1.1 \
        pytest-xdist>=3.5.0 \
        pytest-mock>=3.12.0 \
        pytest-benchmark>=4.0.0 \
        pytest-timeout>=2.2.0 \
        pytest-dependency>=0.5.1 \
        pytest-repeat>=0.9.3 \
        pytest-rerunfailures>=12.0 \
        requests>=2.31.0 \
        httpx>=0.25.2 \
        selenium>=4.15.0 \
        locust>=2.17.0 \
        factory-boy>=3.3.0 \
        faker>=20.1.0 \
        unittest-xml-reporting>=3.2.0 \
        coverage>=7.3.0 \
        allure-pytest>=2.13.2 \
        black>=23.11.0 \
        isort>=5.12.0 \
        flake8>=6.1.0 \
        mypy>=1.7.0 \
        bandit>=1.7.5 \
        safety>=2.3.5

# ============================================================================
# TESTING FRAMEWORK SETUP
# ============================================================================

# Create pytest configuration
RUN cat > /app/pytest.ini << 'EOF'
[tool:pytest]
addopts = 
    --strict-markers
    --strict-config
    --cov=app
    --cov-report=html:reports/coverage
    --cov-report=xml:reports/coverage.xml
    --cov-report=term-missing
    --html=reports/pytest_report.html
    --self-contained-html
    --junitxml=reports/junit.xml
    --maxfail=3
    --tb=short
    -v

testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    smoke: Smoke tests
    performance: Performance tests
    security: Security tests
    slow: Slow running tests
    fast: Fast running tests

filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
EOF

# Create test suite framework
RUN cat > /app/test_framework.py << 'EOF'
"""Comprehensive Test Framework for SutazAI"""
import pytest
import asyncio
import time
import logging
import json
from typing import Dict, Any, List
from pathlib import Path
from unittest.mock import Mock, patch
from faker import Faker
from factory import Factory, Faker as FactoryFaker
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from locust import HttpUser, task, between

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BaseTestClass:
    """Base test class with common utilities"""
    
    @pytest.fixture(autouse=True)
    def setup_method(self):
        """Setup method for each test"""
        self.start_time = time.time()
        logger.info(f"Starting test: {self.__class__.__name__}")
    
    def teardown_method(self):
        """Teardown method for each test"""
        duration = time.time() - self.start_time
        logger.info(f"Test completed in {duration:.3f}s")

class APITestClient:
    """HTTP API test client"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
    
    async def get(self, endpoint: str) -> Dict[str, Any]:
        """GET request to API"""
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{self.base_url}{endpoint}")
            return {
                "status_code": response.status_code,
                "data": response.json() if response.status_code < 400 else None,
                "error": response.text if response.status_code >= 400 else None
            }
    
    async def post(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """POST request to API"""
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(f"{self.base_url}{endpoint}", json=data)
            return {
                "status_code": response.status_code,
                "data": response.json() if response.status_code < 400 else None,
                "error": response.text if response.status_code >= 400 else None
            }

class WebDriverManager:
    """Selenium WebDriver manager"""
    
    @staticmethod
    def get_chrome_driver(headless: bool = True) -> webdriver.Chrome:
        """Get Chrome WebDriver instance"""
        options = Options()
        if headless:
            options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        return webdriver.Chrome(options=options)

class TestDataFactory:
    """Test data factory for generating test data"""
    
    def __init__(self):
        self.fake = Faker()
    
    def user_data(self) -> Dict[str, Any]:
        """Generate user test data"""
        return {
            "id": self.fake.uuid4(),
            "username": self.fake.user_name(),
            "email": self.fake.email(),
            "first_name": self.fake.first_name(),
            "last_name": self.fake.last_name(),
            "created_at": self.fake.date_time().isoformat()
        }
    
    def api_response_data(self) -> Dict[str, Any]:
        """Generate API response test data"""
        return {
            "status": self.fake.random_element(["success", "error"]),
            "message": self.fake.sentence(),
            "data": {
                "id": self.fake.random_int(1, 1000),
                "value": self.fake.random_number(digits=5)
            },
            "timestamp": self.fake.date_time().isoformat()
        }

class PerformanceTestUser(HttpUser):
    """Locust performance test user"""
    
    wait_time = between(1, 3)
    
    @task(3)
    def test_health_endpoint(self):
        """Test health endpoint performance"""
        self.client.get("/health")
    
    @task(2)
    def test_api_endpoint(self):
        """Test main API endpoint performance"""
        self.client.get("/api/v1/status")
    
    @task(1)
    def test_post_endpoint(self):
        """Test POST endpoint performance"""
        self.client.post("/api/v1/data", json={"test": "data"})

class TestReporter:
    """Test result reporter"""
    
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_summary_report(self, test_results: Dict[str, Any]):
        """Generate test summary report"""
        report = {
            "test_run": {
                "timestamp": time.time(),
                "total_tests": test_results.get("total", 0),
                "passed": test_results.get("passed", 0),
                "failed": test_results.get("failed", 0),
                "skipped": test_results.get("skipped", 0),
                "duration": test_results.get("duration", 0.0)
            },
            "coverage": test_results.get("coverage", {}),
            "performance": test_results.get("performance", {})
        }
        
        report_file = self.output_dir / "test_summary.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)
        
        return report_file

# Sample test cases
class TestHealthEndpoint(BaseTestClass):
    """Health endpoint tests"""
    
    @pytest.mark.unit
    @pytest.mark.fast
    async def test_health_endpoint_returns_200(self):
        """Test health endpoint returns 200"""
        client = APITestClient()
        result = await client.get("/health")
        assert result["status_code"] == 200
        assert "status" in result["data"]
    
    @pytest.mark.integration
    async def test_health_endpoint_structure(self):
        """Test health endpoint response structure"""
        client = APITestClient()
        result = await client.get("/health")
        data = result["data"]
        required_fields = ["status", "timestamp"]
        for field in required_fields:
            assert field in data

class TestAPIEndpoints(BaseTestClass):
    """API endpoints tests"""
    
    @pytest.mark.unit
    @pytest.mark.parametrize("endpoint", ["/api/v1/status", "/metrics"])
    async def test_endpoints_accessible(self, endpoint):
        """Test API endpoints are accessible"""
        client = APITestClient()
        result = await client.get(endpoint)
        assert result["status_code"] in [200, 404]  # 404 is acceptable if not implemented
    
    @pytest.mark.slow
    @pytest.mark.performance
    async def test_api_response_time(self):
        """Test API response time"""
        client = APITestClient()
        start_time = time.time()
        result = await client.get("/health")
        duration = time.time() - start_time
        assert duration < 1.0  # Should respond within 1 second
EOF

# Create test runner script
RUN cat > /app/run_tests.py << 'EOF'
"""Test Runner Script"""
import subprocess
import sys
import os
import json
from pathlib import Path

def run_unit_tests():
    """Run unit tests"""
    cmd = ["pytest", "-m", "unit", "--tb=short"]
    return subprocess.run(cmd, capture_output=True, text=True)

def run_integration_tests():
    """Run integration tests"""
    cmd = ["pytest", "-m", "integration", "--tb=short"]
    return subprocess.run(cmd, capture_output=True, text=True)

def run_performance_tests():
    """Run performance tests with Locust"""
    cmd = ["locust", "-f", "test_framework.py", "--headless", "-u", "10", "-r", "2", "-t", "30s"]
    return subprocess.run(cmd, capture_output=True, text=True)

def run_security_tests():
    """Run security tests"""
    results = {}
    
    # Bandit security scan
    bandit_cmd = ["bandit", "-r", ".", "-f", "json"]
    bandit_result = subprocess.run(bandit_cmd, capture_output=True, text=True)
    results["bandit"] = bandit_result.returncode == 0
    
    # Safety check
    safety_cmd = ["safety", "check", "--json"]
    safety_result = subprocess.run(safety_cmd, capture_output=True, text=True)
    results["safety"] = safety_result.returncode == 0
    
    return results

def run_all_tests():
    """Run complete test suite"""
    print("🚀 Starting SutazAI Test Suite...")
    
    # Create reports directory
    Path("reports").mkdir(exist_ok=True)
    
    results = {
        "unit_tests": None,
        "integration_tests": None,
        "performance_tests": None,
        "security_tests": None
    }
    
    # Run unit tests
    print("📝 Running unit tests...")
    unit_result = run_unit_tests()
    results["unit_tests"] = unit_result.returncode == 0
    
    # Run integration tests
    print("🔗 Running integration tests...")
    integration_result = run_integration_tests()
    results["integration_tests"] = integration_result.returncode == 0
    
    # Run security tests
    print("🔒 Running security tests...")
    security_results = run_security_tests()
    results["security_tests"] = security_results
    
    # Generate summary report
    with open("reports/test_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print("✅ Test suite completed!")
    print(f"Results: {results}")
    
    return all([
        results["unit_tests"],
        results["integration_tests"],
        all(results["security_tests"].values())
    ])

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
EOF

# ============================================================================
# APPLICATION SETUP
# ============================================================================

# Create directories and set permissions
RUN mkdir -p /app/tests /app/reports /app/config && \
    chown -R tester:tester /app

# Copy application code
COPY --chown=tester:tester . /app/

# Switch to testing user
USER tester

# ============================================================================
# HEALTH CHECK & MONITORING
# ============================================================================

# Create health check for testing service
RUN cat > /app/health_check.py << 'EOF'
"""Testing Service Health Check"""
from flask import Flask, jsonify
import subprocess
import time

app = Flask(__name__)

@app.route("/health")
def health():
    try:
        # Quick pytest dry-run to check if tests can be discovered
        result = subprocess.run(
            ["pytest", "--collect-only", "-q"],
            capture_output=True, text=True, timeout=10
        )
        
        return jsonify({
            "status": "healthy",
            "service": "testing-service",
            "pytest_available": result.returncode == 0,
            "timestamp": time.time()
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }), 503

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080)
EOF

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python health_check.py || exit 1

# ============================================================================
# RUNTIME CONFIGURATION
# ============================================================================

EXPOSE 8080

ENV TEST_ENV=development \
    PYTEST_WORKERS=auto \
    COVERAGE_THRESHOLD=80 \
    TEST_TIMEOUT=300

VOLUME ["/app/tests", "/app/reports"]

# Default command
CMD ["python", "run_tests.py"]

# ============================================================================
# TEMPLATE USAGE INSTRUCTIONS
# ============================================================================
#
# To use this template:
#
# Basic test execution:
# docker build -t testing-service .
# docker run -v $(pwd)/tests:/app/tests testing-service
#
# With custom test configuration:
# docker run -e PYTEST_WORKERS=4 \
#            -e COVERAGE_THRESHOLD=90 \
#            -v $(pwd)/reports:/app/reports \
#            testing-service
#
# Interactive test development:
# docker run -it -v $(pwd):/app testing-service bash
#
# Features:
# - Comprehensive testing framework (pytest, selenium, locust)
# - Unit, integration, performance, and security testing
# - Automated test discovery and execution
# - Code coverage reporting with HTML/XML output
# - Performance testing with Locust
# - Security testing with Bandit and Safety
# - Test data generation with Faker and Factory Boy
# - WebDriver support for E2E testing
# - CI/CD integration with JUnit XML output
#
# Test Commands:
# - pytest -m unit - Run unit tests only
# - pytest -m integration - Run integration tests
# - pytest -m performance - Run performance tests
# - pytest --cov=app - Run with coverage
# - python run_tests.py - Run complete test suite
#
# Report Generation:
# - HTML coverage report: reports/coverage/index.html
# - JUnit XML: reports/junit.xml
# - Test results JSON: reports/test_results.json
# ============================================================================