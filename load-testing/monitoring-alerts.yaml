# SutazAI Production Monitoring Alerts
# Based on load testing results and performance baselines

groups:
  - name: sutazai_critical_alerts
    interval: 15s
    rules:
      - alert: SystemDown
        expr: up{job=~"sutazai.*"} == 0
        for: 30s
        labels:
          severity: critical
          team: platform
          runbook: "https://docs.sutazai.com/runbooks/system-down"
        annotations:
          summary: "SutazAI service {{ $labels.instance }} is down"
          description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 30 seconds"
          
      - alert: HighErrorRate
        expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) * 100 > 5
        for: 2m
        labels:
          severity: critical
          team: platform
          runbook: "https://docs.sutazai.com/runbooks/high-error-rate"
        annotations:
          summary: "High error rate detected: {{ $value | humanizePercentage }}"
          description: "Error rate for {{ $labels.service }} is {{ $value | humanizePercentage }}, exceeding 5% critical threshold"
          
      - alert: ExtremeResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 10
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Extreme response time: {{ $value }}s"
          description: "95th percentile response time is {{ $value }}s, indicating severe performance issues"

  - name: sutazai_performance_alerts
    interval: 30s
    rules:
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 3
        for: 5m
        labels:
          severity: warning
          team: platform
          component: api
        annotations:
          summary: "High response time: {{ $value }}s"
          description: "95th percentile response time is {{ $value }}s, exceeding 3s SLA threshold"
          
      - alert: ModerateErrorRate
        expr: (rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])) * 100 > 1
        for: 3m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Elevated error rate: {{ $value | humanizePercentage }}"
          description: "Error rate is {{ $value | humanizePercentage }}, exceeding 1% warning threshold"
          
      - alert: LowThroughput
        expr: rate(http_requests_total[5m]) < 10
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Low request throughput: {{ $value }} req/s"
          description: "Request throughput is {{ $value }} req/s, below expected baseline of 50 req/s"

  - name: sutazai_agent_alerts
    interval: 30s
    rules:
      - alert: AgentUnavailable
        expr: up{job="sutazai-agents"} == 0
        for: 1m
        labels:
          severity: critical
          team: ai-platform
          component: agent
        annotations:
          summary: "Agent {{ $labels.agent_name }} is unavailable"
          description: "Agent {{ $labels.agent_name }} on {{ $labels.instance }} has been down for more than 1 minute"
          
      - alert: AgentHighLatency
        expr: histogram_quantile(0.95, rate(agent_request_duration_seconds_bucket{agent_name!=""}[5m])) > 8
        for: 5m
        labels:
          severity: warning
          team: ai-platform
          component: agent
        annotations:
          summary: "High agent latency: {{ $labels.agent_name }}"
          description: "Agent {{ $labels.agent_name }} 95th percentile latency is {{ $value }}s, exceeding 8s threshold"
          
      - alert: AgentQueueBacklog
        expr: agent_queue_length > 100
        for: 3m
        labels:
          severity: warning
          team: ai-platform
          component: agent
        annotations:
          summary: "Agent queue backlog: {{ $labels.agent_name }}"
          description: "Agent {{ $labels.agent_name }} has {{ $value }} queued requests, indicating overload"
          
      - alert: AgentMemoryHigh
        expr: (agent_memory_usage_bytes / agent_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          team: ai-platform
          component: agent
        annotations:
          summary: "High agent memory usage: {{ $labels.agent_name }}"
          description: "Agent {{ $labels.agent_name }} memory usage is {{ $value | humanizePercentage }}"
          
      - alert: AgentCPUHigh
        expr: rate(agent_cpu_usage_seconds_total[5m]) > 0.8
        for: 10m
        labels:
          severity: warning
          team: ai-platform
          component: agent
        annotations:
          summary: "High agent CPU usage: {{ $labels.agent_name }}"
          description: "Agent {{ $labels.agent_name }} CPU usage is {{ $value | humanizePercentage }}"

  - name: sutazai_database_alerts
    interval: 30s
    rules:
      - alert: DatabaseConnectionsHigh
        expr: (pg_stat_database_numbackends / pg_settings_max_connections) > 0.8
        for: 5m
        labels:
          severity: warning
          team: platform
          component: database
        annotations:
          summary: "High database connection usage: {{ $value | humanizePercentage }}"
          description: "PostgreSQL connection usage is {{ $value | humanizePercentage }}, approaching limit"
          
      - alert: DatabaseConnectionsExhausted
        expr: (pg_stat_database_numbackends / pg_settings_max_connections) > 0.95
        for: 1m
        labels:
          severity: critical
          team: platform
          component: database
        annotations:
          summary: "Database connections nearly exhausted"
          description: "PostgreSQL connection usage is {{ $value | humanizePercentage }}, immediate action required"
          
      - alert: SlowDatabaseQueries
        expr: histogram_quantile(0.95, rate(pg_stat_statements_exec_time_bucket[5m])) > 1000
        for: 5m
        labels:
          severity: warning
          team: platform
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "95th percentile query execution time is {{ $value }}ms, exceeding 1s threshold"
          
      - alert: DatabaseReplicationLag
        expr: (pg_stat_replication_replay_lag > 30)
        for: 2m
        labels:
          severity: warning
          team: platform
          component: database
        annotations:
          summary: "Database replication lag: {{ $value }}s"
          description: "PostgreSQL replication lag is {{ $value }}s, may impact read performance"
          
      - alert: RedisMemoryHigh
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          team: platform
          component: cache
        annotations:
          summary: "High Redis memory usage: {{ $value | humanizePercentage }}"
          description: "Redis instance {{ $labels.instance }} memory usage is {{ $value | humanizePercentage }}"
          
      - alert: RedisConnectionsHigh
        expr: redis_connected_clients > 500
        for: 5m
        labels:
          severity: warning
          team: platform
          component: cache
        annotations:
          summary: "High Redis connection count: {{ $value }}"
          description: "Redis has {{ $value }} connected clients, may indicate connection leak"

  - name: sutazai_infrastructure_alerts
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: (100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 10m
        labels:
          severity: warning
          team: platform
          component: infrastructure
        annotations:
          summary: "High CPU usage: {{ $value }}%"
          description: "CPU usage on {{ $labels.instance }} is {{ $value }}%, may impact performance"
          
      - alert: CriticalCPUUsage
        expr: (100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 95
        for: 2m
        labels:
          severity: critical
          team: platform
          component: infrastructure
        annotations:
          summary: "Critical CPU usage: {{ $value }}%"
          description: "CPU usage on {{ $labels.instance }} is {{ $value }}%, immediate attention required"
          
      - alert: HighMemoryUsage
        expr: ((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) > 0.85
        for: 10m
        labels:
          severity: warning
          team: platform
          component: infrastructure
        annotations:
          summary: "High memory usage: {{ $value | humanizePercentage }}"
          description: "Memory usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}"
          
      - alert: CriticalMemoryUsage
        expr: ((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) > 0.95
        for: 2m
        labels:
          severity: critical
          team: platform
          component: infrastructure
        annotations:
          summary: "Critical memory usage: {{ $value | humanizePercentage }}"
          description: "Memory usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}, may cause OOM"
          
      - alert: DiskSpaceLow
        expr: ((node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) < 0.1)
        for: 5m
        labels:
          severity: warning
          team: platform
          component: infrastructure
        annotations:
          summary: "Low disk space: {{ $value | humanizePercentage }} available"
          description: "Disk space on {{ $labels.instance }}:{{ $labels.mountpoint }} is {{ $value | humanizePercentage }} available"
          
      - alert: DiskSpaceCritical
        expr: ((node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) < 0.05)
        for: 1m
        labels:
          severity: critical
          team: platform
          component: infrastructure
        annotations:
          summary: "Critical disk space: {{ $value | humanizePercentage }} available"
          description: "Disk space on {{ $labels.instance }}:{{ $labels.mountpoint }} is critically low at {{ $value | humanizePercentage }}"

  - name: sutazai_jarvis_alerts
    interval: 30s
    rules:
      - alert: JarvisHighLatency
        expr: histogram_quantile(0.95, rate(jarvis_request_duration_seconds_bucket[5m])) > 2
        for: 3m
        labels:
          severity: warning
          team: ai-platform
          component: jarvis
        annotations:
          summary: "High Jarvis response time: {{ $value }}s"
          description: "Jarvis 95th percentile response time is {{ $value }}s, exceeding 2s threshold"
          
      - alert: JarvisSessionBacklog
        expr: jarvis_active_sessions > 1000
        for: 5m
        labels:
          severity: warning
          team: ai-platform
          component: jarvis
        annotations:
          summary: "High Jarvis session count: {{ $value }}"
          description: "Jarvis has {{ $value }} active sessions, may impact performance"
          
      - alert: JarvisErrorRate
        expr: (rate(jarvis_requests_total{status=~"5.."}[5m]) / rate(jarvis_requests_total[5m])) * 100 > 2
        for: 3m
        labels:
          severity: warning
          team: ai-platform
          component: jarvis
        annotations:
          summary: "High Jarvis error rate: {{ $value | humanizePercentage }}"
          description: "Jarvis error rate is {{ $value | humanizePercentage }}, exceeding 2% threshold"

  - name: sutazai_ollama_alerts
    interval: 30s
    rules:
      - alert: OllamaServiceDown
        expr: up{job="ollama"} == 0
        for: 1m
        labels:
          severity: critical
          team: ai-platform
          component: ollama
        annotations:
          summary: "Ollama service is down"
          description: "Ollama service on {{ $labels.instance }} has been down for more than 1 minute"
          
      - alert: OllamaHighMemory
        expr: (ollama_memory_usage_bytes / ollama_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          team: ai-platform
          component: ollama
        annotations:
          summary: "High Ollama memory usage: {{ $value | humanizePercentage }}"
          description: "Ollama memory usage is {{ $value | humanizePercentage }}, may impact model performance"
          
      - alert: OllamaModelLoadTime
        expr: histogram_quantile(0.95, rate(ollama_model_load_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
          team: ai-platform
          component: ollama
        annotations:
          summary: "Slow Ollama model loading: {{ $value }}s"
          description: "Ollama model loading time is {{ $value }}s, exceeding 30s threshold"

  - name: sutazai_api_gateway_alerts
    interval: 30s
    rules:
      - alert: APIGatewayRateLimitHit
        expr: increase(api_gateway_rate_limit_exceeded_total[5m]) > 100
        for: 1m
        labels:
          severity: warning
          team: platform
          component: api_gateway
        annotations:
          summary: "API Gateway rate limit frequently exceeded"
          description: "Rate limit exceeded {{ $value }} times in the last 5 minutes"
          
      - alert: APIGatewayHighLatency
        expr: histogram_quantile(0.95, rate(api_gateway_request_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          team: platform
          component: api_gateway
        annotations:
          summary: "High API Gateway latency: {{ $value }}s"
          description: "API Gateway 95th percentile latency is {{ $value }}s, adding overhead to requests"

  - name: sutazai_business_kpis
    interval: 60s
    rules:
      - alert: LowUserEngagement
        expr: rate(user_interactions_total[1h]) < 10
        for: 30m
        labels:
          severity: warning
          team: product
          component: business
        annotations:
          summary: "Low user engagement: {{ $value }} interactions/hour"
          description: "User interaction rate is {{ $value }} per hour, below expected baseline"
          
      - alert: HighUserChurn
        expr: (user_sessions_ended_total / user_sessions_started_total) > 0.8
        for: 15m
        labels:
          severity: warning
          team: product
          component: business
        annotations:
          summary: "High user churn rate: {{ $value | humanizePercentage }}"
          description: "User churn rate is {{ $value | humanizePercentage }}, indicating user experience issues"

# Alert routing and notification configuration
alertmanager_config: |
  global:
    smtp_smarthost: 'localhost:587'
    smtp_from: 'alerts@sutazai.com'
    
  route:
    group_by: ['alertname', 'severity']
    group_wait: 10s
    group_interval: 30s
    repeat_interval: 12h
    receiver: 'web.hook'
    routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 5m
    - match:
        team: ai-platform
      receiver: 'ai-team'
    - match:
        team: platform
      receiver: 'platform-team'
      
  receivers:
  - name: 'web.hook'
    webhook_configs:
    - url: 'http://localhost:5001/webhook'
      
  - name: 'critical-alerts'
    email_configs:
    - to: 'oncall@sutazai.com'
      subject: 'CRITICAL: SutazAI Alert - {{ .GroupLabels.alertname }}'
      body: |
        {{ range .Alerts }}
        Alert: {{ .Annotations.summary }}
        Description: {{ .Annotations.description }}
        Severity: {{ .Labels.severity }}
        Instance: {{ .Labels.instance }}
        {{ end }}
    slack_configs:
    - api_url: 'YOUR_SLACK_WEBHOOK_URL'
      channel: '#critical-alerts'
      title: 'CRITICAL: {{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      
  - name: 'ai-team'
    email_configs:
    - to: 'ai-team@sutazai.com'
      subject: 'SutazAI AI Platform Alert - {{ .GroupLabels.alertname }}'
      
  - name: 'platform-team'
    email_configs:
    - to: 'platform-team@sutazai.com'
      subject: 'SutazAI Platform Alert - {{ .GroupLabels.alertname }}'

# Silencing rules for maintenance windows
silencing_rules:
  - comment: "Maintenance window - Database upgrades"
    matchers:
      - name: "component"
        value: "database"
    startsAt: "2024-01-01T02:00:00Z"
    endsAt: "2024-01-01T04:00:00Z"