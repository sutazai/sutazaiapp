# ============================================================================
# SUTAZAI MASTER TEMPLATE: AI/ML Service Base
# ============================================================================
# Purpose: Production-ready AI/ML service with GPU support
# Security: Non-root user, hardened ML environment
# Performance: Optimized for AI/ML workloads with CUDA support
# Compatibility: Python 3.12.8 with ML/AI frameworks
# Author: ULTRA DEPLOYMENT ENGINEER
# Date: August 10, 2025
# Version: v1.0.0
# ============================================================================

# Multi-stage build for GPU and CPU variants
ARG CUDA_VERSION=12.1
ARG PYTHON_VERSION=3.12.8

# Base stage - choose between CUDA and CPU
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu22.04 as gpu-base
FROM python:${PYTHON_VERSION}-slim-bookworm as cpu-base

# Select runtime based on GPU availability
ARG ENABLE_GPU=false
FROM ${ENABLE_GPU:+gpu-base}${ENABLE_GPU:-cpu-base} as runtime

# ============================================================================
# SYSTEM CONFIGURATION & SECURITY HARDENING
# ============================================================================

# Set environment variables for ML optimization
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    DEBIAN_FRONTEND=noninteractive \
    CUDA_VISIBLE_DEVICES=all \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Install system dependencies for AI/ML
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        ca-certificates \
        git \
        wget \
        libsm6 \
        libxext6 \
        libxrender-dev \
        libglib2.0-0 \
        libgomp1 \
        && rm -rf /var/lib/apt/lists/* \
        && apt-get clean

# Install Python if using GPU base (Ubuntu doesn't include Python)
RUN if [ -f /etc/lsb-release ]; then \
        apt-get update && \
        apt-get install -y --no-install-recommends \
            python3.12 \
            python3.12-dev \
            python3-pip \
            && ln -s /usr/bin/python3.12 /usr/bin/python \
            && rm -rf /var/lib/apt/lists/*; \
    fi

# Create non-root user for security
RUN groupadd --gid 1000 mluser && \
    useradd --uid 1000 --gid 1000 --create-home --shell /bin/bash mluser

# ============================================================================
# APPLICATION DIRECTORY SETUP
# ============================================================================

# Create application directory with proper ownership
RUN mkdir -p /app /app/models /app/data /app/logs /app/cache && \
    chown -R mluser:mluser /app

# Set working directory
WORKDIR /app

# ============================================================================
# AI/ML DEPENDENCIES INSTALLATION
# ============================================================================

# Copy AI/ML requirements file
COPY docker/base/base-requirements.txt /tmp/base-requirements.txt

# Create AI/ML specific requirements
RUN cat > /tmp/ml-requirements.txt << 'EOF'
# Core ML/AI frameworks
torch>=2.2.0
torchvision>=0.17.0
torchaudio>=2.2.0
transformers>=4.36.0
accelerate>=0.26.0
datasets>=2.16.0

# Deep Learning utilities
pytorch-lightning>=2.1.0
torchmetrics>=1.2.0
huggingface-hub>=0.20.0

# Scientific computing
numpy>=1.26.0
scipy>=1.11.0
scikit-learn>=1.4.0
pandas>=2.1.0
matplotlib>=3.8.0
seaborn>=0.13.0

# Computer Vision
opencv-python-headless>=4.9.0
pillow>=10.2.0

# Natural Language Processing
nltk>=3.8.1
spacy>=3.7.0
tokenizers>=0.15.0

# Vector databases
faiss-cpu>=1.7.4
chromadb>=0.4.18
qdrant-client>=1.6.9

# Model serving
fastapi>=0.104.1
uvicorn[standard]>=0.24.0
gradio>=4.0.0

# Monitoring and observability
tensorboard>=2.15.0
wandb>=0.16.0
mlflow>=2.9.0
EOF

# Install Python dependencies
RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir -r /tmp/base-requirements.txt && \
    pip install --no-cache-dir -r /tmp/ml-requirements.txt && \
    rm -rf /tmp/*requirements.txt

# ============================================================================
# APPLICATION CODE SETUP
# ============================================================================

# Copy application code and set ownership
COPY --chown=mluser:mluser . /app/

# Switch to non-root user
USER mluser

# Create ML-specific directories
RUN mkdir -p /app/models/pretrained \
             /app/models/finetuned \
             /app/data/input \
             /app/data/output \
             /app/logs \
             /app/cache \
             /app/checkpoints

# ============================================================================
# HEALTH CHECK & MONITORING
# ============================================================================

# Health check for ML service
HEALTHCHECK --interval=60s --timeout=30s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ============================================================================
# RUNTIME CONFIGURATION
# ============================================================================

# Expose ML service port
EXPOSE 8000

# Set ML environment variables
ENV ML_SERVICE_PORT=8000 \
    ML_SERVICE_HOST=0.0.0.0 \
    MODEL_CACHE_DIR=/app/models \
    DATA_DIR=/app/data \
    LOG_LEVEL=INFO \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
    TOKENIZERS_PARALLELISM=false \
    TRANSFORMERS_CACHE=/app/cache/transformers \
    HF_HOME=/app/cache/huggingface

# Default entrypoint for ML services
ENTRYPOINT ["python", "-m", "uvicorn"]
CMD ["app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]

# ============================================================================
# TEMPLATE USAGE INSTRUCTIONS
# ============================================================================
#
# To use this template:
# 1. Build with GPU support:
#    docker build --build-arg ENABLE_GPU=true -t ml-service:gpu .
# 2. Build CPU-only version:
#    docker build --build-arg ENABLE_GPU=false -t ml-service:cpu .
# 3. Run with GPU access:
#    docker run --gpus all -p 8000:8000 ml-service:gpu
#
# Features:
# - Dual CPU/GPU support via multi-stage builds
# - Pre-installed ML/AI frameworks (PyTorch, Transformers, etc.)
# - Model caching and persistence
# - Non-root execution for security
# - Health checks optimized for ML workloads
# - Memory management for large models
#
# Directory Structure:
# /app/models/     - Model storage (pretrained/finetuned)
# /app/data/       - Input/output data
# /app/cache/      - Framework caches (HuggingFace, etc.)
# /app/checkpoints/- Training checkpoints
# /app/logs/       - Application logs
# ============================================================================