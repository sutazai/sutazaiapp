# ============================================================================
# SUTAZAI MASTER TEMPLATE: Worker Service Base
# ============================================================================
# Purpose: Production-ready background worker service
# Security: Non-root user, secure task processing
# Performance: Optimized for high-throughput background jobs
# Compatibility: Celery, RQ, asyncio workers
# Author: ULTRA DEPLOYMENT ENGINEER
# Date: August 10, 2025
# Version: v1.0.0
# ============================================================================

FROM python:3.12.8-slim-bookworm as base

# ============================================================================
# SYSTEM CONFIGURATION & SECURITY HARDENING
# ============================================================================

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    DEBIAN_FRONTEND=noninteractive \
    C_FORCE_ROOT=1

# Install system dependencies for worker services
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        ca-certificates \
        git \
        postgresql-client \
        redis-tools \
        && rm -rf /var/lib/apt/lists/*

# Create worker user
RUN groupadd --gid 1000 worker && \
    useradd --uid 1000 --gid 1000 --create-home --shell /bin/bash worker

# ============================================================================
# WORKER DEPENDENCIES INSTALLATION
# ============================================================================

WORKDIR /app

# Install worker dependencies
RUN pip install --upgrade pip setuptools wheel

# Copy base requirements
COPY docker/base/base-requirements.txt /tmp/base-requirements.txt

# Install worker-specific dependencies
RUN cat > /tmp/worker-requirements.txt << 'EOF'
# Task Queue Systems
celery[redis]>=5.3.4
rq>=1.15.1
kombu>=5.3.4

# Async processing
aiofiles>=23.2.1
aiohttp>=3.9.1
aioredis>=2.0.1

# Database connectivity
sqlalchemy>=2.0.23
asyncpg>=0.29.0
psycopg2-binary>=2.9.7

# Message brokers
pika>=1.3.2
kafka-python>=2.0.2

# Monitoring
prometheus-client>=0.19.0
flower>=2.0.1

# Utilities
requests>=2.31.0
click>=8.1.7
schedule>=1.2.0
EOF

RUN pip install --no-cache-dir -r /tmp/base-requirements.txt && \
    pip install --no-cache-dir -r /tmp/worker-requirements.txt && \
    rm -rf /tmp/*requirements.txt

# ============================================================================
# WORKER CONFIGURATION
# ============================================================================

# Create Celery configuration
RUN cat > /app/celeryconfig.py << 'EOF'
"""Celery Configuration for Worker Services"""
import os

# Broker settings
broker_url = os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0')
result_backend = os.getenv('CELERY_RESULT_BACKEND', 'redis://localhost:6379/0')

# Task settings
task_serializer = 'json'
accept_content = ['json']
result_serializer = 'json'
timezone = 'UTC'
enable_utc = True

# Worker settings
worker_prefetch_multiplier = 1
task_acks_late = True
worker_max_tasks_per_child = 1000
worker_disable_rate_limits = False

# Monitoring
worker_send_task_events = True
task_send_sent_event = True

# Task routing
task_routes = {
    'tasks.high_priority.*': {'queue': 'high_priority'},
    'tasks.normal.*': {'queue': 'normal'},
    'tasks.low_priority.*': {'queue': 'low_priority'},
}

# Result expiration
result_expires = 3600  # 1 hour
EOF

# Create worker application
RUN cat > /app/worker.py << 'EOF'
"""Worker Service Application"""
from celery import Celery
from celery.signals import worker_ready, worker_shutdown
import logging
import time
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create Celery app
app = Celery('sutazai_worker')
app.config_from_object('celeryconfig')

# Health check flag
worker_ready_flag = False

@worker_ready.connect
def worker_ready_handler(sender=None, **kwargs):
    """Worker ready signal handler"""
    global worker_ready_flag
    worker_ready_flag = True
    logger.info("Worker is ready to process tasks")

@worker_shutdown.connect
def worker_shutdown_handler(sender=None, **kwargs):
    """Worker shutdown signal handler"""
    global worker_ready_flag
    worker_ready_flag = False
    logger.info("Worker is shutting down")

# Sample tasks
@app.task(bind=True)
def process_data(self, data):
    """Sample data processing task"""
    try:
        logger.info(f"Processing data: {data}")
        # Simulate processing
        time.sleep(2)
        return {"status": "completed", "processed_data": data}
    except Exception as exc:
        logger.error(f"Task failed: {exc}")
        self.retry(countdown=60, max_retries=3)

@app.task
def send_notification(message, recipient):
    """Sample notification task"""
    logger.info(f"Sending notification to {recipient}: {message}")
    # Implement notification logic
    return {"status": "sent", "recipient": recipient}

@app.task
def cleanup_old_data():
    """Sample cleanup task"""
    logger.info("Running data cleanup task")
    # Implement cleanup logic
    return {"status": "cleaned", "timestamp": time.time()}

# Health check endpoint
def health_check():
    """Worker health check"""
    return {
        "status": "healthy" if worker_ready_flag else "starting",
        "worker_ready": worker_ready_flag,
        "timestamp": time.time()
    }

if __name__ == '__main__':
    app.start()
EOF

# Create RQ worker alternative
RUN cat > /app/rq_worker.py << 'EOF'
"""RQ Worker Alternative"""
import os
import time
import logging
from rq import Worker, Queue, Connection
from redis import Redis

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Redis connection
redis_conn = Redis.from_url(os.getenv('REDIS_URL', 'redis://localhost:6379/0'))

def sample_job(data):
    """Sample RQ job"""
    logger.info(f"Processing job with data: {data}")
    time.sleep(2)
    return f"Processed: {data}"

def run_worker():
    """Run RQ worker"""
    with Connection(redis_conn):
        worker = Worker(['high', 'normal', 'low'])
        logger.info("Starting RQ worker...")
        worker.work()

if __name__ == '__main__':
    run_worker()
EOF

# Create health check script
RUN cat > /app/health_check.py << 'EOF'
"""Worker Health Check Service"""
import json
import time
from flask import Flask, jsonify
from celery import Celery
import redis

app = Flask(__name__)

@app.route('/health')
def health():
    """Health check endpoint"""
    try:
        # Check Redis connectivity
        r = redis.Redis.from_url('redis://localhost:6379/0')
        r.ping()
        
        # Check Celery worker status
        celery_app = Celery('sutazai_worker')
        celery_app.config_from_object('celeryconfig')
        
        inspect = celery_app.control.inspect()
        stats = inspect.stats()
        
        return jsonify({
            "status": "healthy",
            "redis_connected": True,
            "workers_active": len(stats) if stats else 0,
            "timestamp": time.time()
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }), 503

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
EOF

# ============================================================================
# APPLICATION SETUP
# ============================================================================

# Create directories and set permissions
RUN mkdir -p /app/logs /app/data /app/temp && \
    chown -R worker:worker /app

# Copy application code
COPY --chown=worker:worker . /app/

# Switch to worker user
USER worker

# ============================================================================
# HEALTH CHECK & MONITORING
# ============================================================================

HEALTHCHECK --interval=30s --timeout=10s --start-period=15s --retries=3 \
    CMD python /app/health_check.py || exit 1

# ============================================================================
# RUNTIME CONFIGURATION
# ============================================================================

EXPOSE 8080 5555

ENV WORKER_TYPE=celery \
    CELERY_BROKER_URL=redis://redis:6379/0 \
    CELERY_RESULT_BACKEND=redis://redis:6379/0 \
    WORKER_CONCURRENCY=2 \
    WORKER_LOGLEVEL=INFO

VOLUME ["/app/logs", "/app/data"]

# Default entrypoint script
RUN cat > /app/entrypoint.sh << 'EOF'
#!/bin/bash
set -e

WORKER_TYPE=${WORKER_TYPE:-celery}

case "$WORKER_TYPE" in
    celery)
        exec celery -A worker worker \
            --loglevel=${WORKER_LOGLEVEL:-INFO} \
            --concurrency=${WORKER_CONCURRENCY:-2} \
            "$@"
        ;;
    rq)
        exec python rq_worker.py "$@"
        ;;
    health)
        exec python health_check.py "$@"
        ;;
    *)
        echo "Unknown worker type: $WORKER_TYPE"
        echo "Supported types: celery, rq, health"
        exit 1
        ;;
esac
EOF

RUN chmod +x /app/entrypoint.sh

ENTRYPOINT ["/app/entrypoint.sh"]
CMD []

# ============================================================================
# TEMPLATE USAGE INSTRUCTIONS
# ============================================================================
#
# To use this template:
#
# Celery Worker:
# docker run -e WORKER_TYPE=celery worker-service
#
# RQ Worker:
# docker run -e WORKER_TYPE=rq worker-service
#
# Health Check Service:
# docker run -e WORKER_TYPE=health -p 8080:8080 worker-service
#
# With custom configuration:
# docker run -e CELERY_BROKER_URL=redis://broker:6379/0 \
#            -e WORKER_CONCURRENCY=4 \
#            -e WORKER_LOGLEVEL=DEBUG \
#            worker-service
#
# Features:
# - Celery and RQ worker support
# - Configurable task queues and routing
# - Health check service with monitoring
# - Non-root execution for security
# - Redis and PostgreSQL connectivity
# - Task retry and error handling
# - Prometheus metrics integration
# - Comprehensive logging
#
# Queue Management:
# - High priority queue for urgent tasks
# - Normal priority for standard tasks
# - Low priority for background tasks
#
# Monitoring:
# - Health check endpoint on port 8080
# - Flower monitoring on port 5555 (Celery)
# - Worker metrics and statistics
# ============================================================================