input {
  # Receive logs from Filebeat
  beats {
    port => 5044
    host => "0.0.0.0"
    tags => ["filebeat"]
  }
  
  # TCP input for direct log forwarding
  tcp {
    port => 5000
    codec => json
    tags => ["direct"]
  }
  
  # Direct JSON file input for testing
  file {
    path => "/opt/sutazaiapp/logs/*.json.log"
    start_position => "beginning"
    sincedb_path => "/opt/sutazaiapp/monitoring/elk/logstash/sincedb"
    codec => json
    tags => ["file"]
  }
}

filter {
  # Add host information
  mutate {
    add_field => { "[@metadata][host]" => "%{host}" }
  }
  
  # Process structured JSON logs
  if [message] and [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "log_data"
    }
    
    # Extract fields from parsed JSON
    if [log_data] {
      mutate {
        rename => { 
          "[log_data][timestamp]" => "@timestamp"
          "[log_data][logger]" => "logger_name"
          "[log_data][level]" => "log_level"
          "[log_data][message]" => "log_message"
          "[log_data][host]" => "host"
          "[log_data][environment]" => "environment"
        }
      }
      
      # Remove the original parsed log_data to avoid duplication
      mutate {
        remove_field => [ "log_data" ]
      }
    }
  }
  
  # Handle application logs
  if [tags] and "app" in [tags] {
    # Parse timestamp if it exists
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS" ]
      target => "@timestamp"
      remove_field => [ "timestamp" ]
    }
    
    # Add type for easier filtering
    mutate {
      add_field => { "type" => "application" }
    }
  }
  
  # Handle model logs
  if [logger_name] and [logger_name] =~ /^model\./ {
    # Extract model name from logger
    grok {
      match => { "logger_name" => "model\.%{GREEDYDATA:model_name}" }
    }
    
    # Add type for easier filtering
    mutate {
      add_field => { "type" => "model" }
    }
  }
  
  # Handle API logs
  if [logger_name] == "sutazai_api" {
    # Add type for easier filtering
    mutate {
      add_field => { "type" => "api" }
    }
    
    # Extract endpoint and method if they exist
    if [extra] and [extra][endpoint] {
      mutate {
        add_field => { "endpoint" => "%{[extra][endpoint]}" }
      }
    }
    
    if [extra] and [extra][method] {
      mutate {
        add_field => { "http_method" => "%{[extra][method]}" }
      }
    }
  }
  
  # Handle security logs
  if [logger_name] == "sutazai_security" {
    # Add type for easier filtering
    mutate {
      add_field => { "type" => "security" }
    }
    
    # Increase priority for security logs
    mutate {
      add_field => { "priority" => "high" }
    }
  }
  
  # Convert log level to lowercase for consistency
  if [log_level] {
    mutate {
      lowercase => [ "log_level" ]
    }
  }
  
  # Add environment-specific tag
  if [environment] {
    mutate {
      add_tag => [ "%{environment}" ]
    }
  } else {
    mutate {
      add_field => { "environment" => "unknown" }
      add_tag => [ "unknown_env" ]
    }
  }
  
  # Add timestamp if missing
  if ![@timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS" ]
      target => "@timestamp"
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "message" ]
  }
}

output {
  # Send everything to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "sutazai-%{+YYYY.MM.dd}"
    user => "${ELASTIC_USERNAME:elastic}"
    password => "${ELASTIC_PASSWORD:sutazaisecure}"
  }
  
  # Send critical logs to a separate index
  if [log_level] == "critical" or [log_level] == "error" or [priority] == "high" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "sutazai-critical-%{+YYYY.MM.dd}"
      user => "${ELASTIC_USERNAME:elastic}"
      password => "${ELASTIC_PASSWORD:sutazaisecure}"
    }
  }
  
  # Optional stdout output for debugging
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }
}
