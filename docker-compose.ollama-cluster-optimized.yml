# Docker Compose for High-Concurrency Ollama Cluster
# Optimized for 174+ concurrent AI agent connections with intelligent load balancing

version: '3.8'

networks:
  ollama-cluster:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
    enable_ipv6: false

volumes:
  ollama-models-primary:
    driver: local
  ollama-models-secondary:
    driver: local
  ollama-models-tertiary:
    driver: local
  nginx-logs:
    driver: local
  redis-data:
    driver: local
  prometheus-data:
    driver: local

services:
  # Primary Ollama instance (main workload)
  ollama-primary:
    image: ollama/ollama:latest
    container_name: ollama-primary
    hostname: ollama-primary
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.10
    ports:
      - "11434:11434"
    environment:
      # High-concurrency configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=50
      - OLLAMA_MAX_LOADED_MODELS=3
      - OLLAMA_KEEP_ALIVE=10m
      - OLLAMA_CPU_THREADS=8
      - OLLAMA_GPU_LAYERS=0
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_MEMORY_MAPPING=1
      - OLLAMA_MAX_MEMORY=16384  # 16GB
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama
      # Queue and connection settings
      - OLLAMA_MAX_CONNECTIONS=100
      - OLLAMA_REQUEST_TIMEOUT=120
      - OLLAMA_QUEUE_SIZE=200
      - OLLAMA_CONCURRENT_REQUESTS=50
      - OLLAMA_BATCH_SIZE=32
      - OLLAMA_CACHE_SIZE=2048
      # System optimizations
      - OLLAMA_NOPRUNE=false
      - OLLAMA_DEBUG=false
      - OLLAMA_LOG_LEVEL=info
      - OLLAMA_METRICS=true
    volumes:
      - ollama-models-primary:/root/.ollama
      - /opt/sutazaiapp/config/ollama.yaml:/etc/ollama/config.yaml:ro
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8.0'
        reservations:
          memory: 8G
          cpus: '4.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Secondary Ollama instance (overflow handling)
  ollama-secondary:
    image: ollama/ollama:latest
    container_name: ollama-secondary
    hostname: ollama-secondary
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.11
    ports:
      - "11435:11434"
    environment:
      # Moderate-concurrency configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=30
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=8m
      - OLLAMA_CPU_THREADS=6
      - OLLAMA_GPU_LAYERS=0
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_MEMORY_MAPPING=1
      - OLLAMA_MAX_MEMORY=10240  # 10GB
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama
      # Queue and connection settings
      - OLLAMA_MAX_CONNECTIONS=60
      - OLLAMA_REQUEST_TIMEOUT=120
      - OLLAMA_QUEUE_SIZE=150
      - OLLAMA_CONCURRENT_REQUESTS=30
      - OLLAMA_BATCH_SIZE=24
      - OLLAMA_CACHE_SIZE=1536
      # System optimizations
      - OLLAMA_NOPRUNE=false
      - OLLAMA_DEBUG=false
      - OLLAMA_LOG_LEVEL=info
      - OLLAMA_METRICS=true
    volumes:
      - ollama-models-secondary:/root/.ollama
      - /opt/sutazaiapp/config/ollama.yaml:/etc/ollama/config.yaml:ro
    deploy:
      resources:
        limits:
          memory: 10G
          cpus: '6.0'
        reservations:
          memory: 5G
          cpus: '3.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      - ollama-primary
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Tertiary Ollama instance (peak load handling)
  ollama-tertiary:
    image: ollama/ollama:latest
    container_name: ollama-tertiary
    hostname: ollama-tertiary
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.12
    ports:
      - "11436:11434"
    environment:
      # Basic-concurrency configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=20
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_CPU_THREADS=4
      - OLLAMA_GPU_LAYERS=0
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_MEMORY_MAPPING=1
      - OLLAMA_MAX_MEMORY=6144  # 6GB
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama
      # Queue and connection settings
      - OLLAMA_MAX_CONNECTIONS=40
      - OLLAMA_REQUEST_TIMEOUT=120
      - OLLAMA_QUEUE_SIZE=100
      - OLLAMA_CONCURRENT_REQUESTS=20
      - OLLAMA_BATCH_SIZE=16
      - OLLAMA_CACHE_SIZE=1024
      # System optimizations
      - OLLAMA_NOPRUNE=false
      - OLLAMA_DEBUG=false
      - OLLAMA_LOG_LEVEL=info
      - OLLAMA_METRICS=true
    volumes:
      - ollama-models-tertiary:/root/.ollama
      - /opt/sutazaiapp/config/ollama.yaml:/etc/ollama/config.yaml:ro
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '4.0'
        reservations:
          memory: 3G
          cpus: '2.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      - ollama-primary
      - ollama-secondary
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # NGINX Load Balancer
  nginx-lb:
    image: nginx:alpine
    container_name: ollama-nginx-lb
    hostname: nginx-lb
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.20
    ports:
      - "8080:80"
      - "8443:443"
      - "8404:8404"  # Status page
    environment:
      - NGINX_ENVSUBST_TEMPLATE_DIR=/etc/nginx/templates
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx/conf.d
    volumes:
      - /opt/sutazaiapp/config/nginx/ollama-high-concurrency.conf:/etc/nginx/conf.d/default.conf:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - ollama-primary
      - ollama-secondary
      - ollama-tertiary
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8404/status"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # Redis for coordination and caching
  redis-cluster:
    image: redis:7-alpine
    container_name: ollama-redis
    hostname: redis-cluster
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.30
    ports:
      - "6379:6379"
    command: >
      redis-server
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
      --appendfsync everysec
      --tcp-backlog 511
      --timeout 0
      --tcp-keepalive 300
      --databases 16
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: ollama-prometheus
    hostname: prometheus
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.40
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    volumes:
      - /opt/sutazaiapp/monitoring/prometheus/ollama-cluster.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    depends_on:
      - ollama-primary
      - nginx-lb
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # Connection Pool Manager
  pool-manager:
    build:
      context: /opt/sutazaiapp
      dockerfile: docker/Dockerfile.pool-manager
    image: ollama-pool-manager:latest
    container_name: ollama-pool-manager
    hostname: pool-manager
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.50
    ports:
      - "8081:8080"  # Pool management API
    environment:
      - OLLAMA_INSTANCES=ollama-primary:11434,ollama-secondary:11434,ollama-tertiary:11434
      - REDIS_URL=redis://redis-cluster:6379
      - MAX_CONNECTIONS_PER_INSTANCE=50,30,20
      - MAX_QUEUE_SIZE=500
      - REQUEST_TIMEOUT=120
      - HEALTH_CHECK_INTERVAL=30
      - LOG_LEVEL=INFO
      - METRICS_ENABLED=true
    volumes:
      - /opt/sutazaiapp/agents/core:/app/core:ro
      - /opt/sutazaiapp/logs:/app/logs
    depends_on:
      - redis-cluster
      - ollama-primary
      - ollama-secondary
      - ollama-tertiary
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # Auto-scaler service
  autoscaler:
    build:
      context: /opt/sutazaiapp
      dockerfile: docker/Dockerfile.autoscaler
    image: ollama-autoscaler:latest
    container_name: ollama-autoscaler
    hostname: autoscaler
    restart: unless-stopped
    networks:
      ollama-cluster:
        ipv4_address: 172.28.1.60
    environment:
      - PROMETHEUS_URL=http://prometheus:9090
      - POOL_MANAGER_URL=http://pool-manager:8080
      - REDIS_URL=redis://redis-cluster:6379
      - MIN_INSTANCES=1
      - MAX_INSTANCES=3
      - CPU_THRESHOLD=80
      - MEMORY_THRESHOLD=85
      - QUEUE_THRESHOLD=100
      - SCALE_UP_COOLDOWN=300
      - SCALE_DOWN_COOLDOWN=600
      - CHECK_INTERVAL=30
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /opt/sutazaiapp/logs:/app/logs
    depends_on:
      - prometheus
      - pool-manager
      - redis-cluster
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

# Resource limits summary
# Total Memory Allocation: 16G + 10G + 6G + 4G + 2G + 1G + 0.5G + 0.5G = ~40GB (within 29GB physical + swap)
# Total CPU Allocation: 8 + 6 + 4 + 2 + 1 + 1 + 0.5 + 0.5 = 23 cores (distributed across 12 physical cores)
# 
# Capacity Summary:
# - Primary: 50 concurrent connections
# - Secondary: 30 concurrent connections  
# - Tertiary: 20 concurrent connections
# - Total Capacity: 100 simultaneous connections
# - Queue Capacity: 450 total queued requests (200+150+100)
# - Total Theoretical Capacity: 550 concurrent requests (100 active + 450 queued)