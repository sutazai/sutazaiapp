#!/usr/bin/env python3
"""
MCP Artifact Cleanup Service

Intelligent cleanup of temporary files, downloads, staging areas, and other
artifacts generated by MCP operations. Provides safe cleanup with comprehensive
safety validation and configurable retention policies.

Author: Claude AI Assistant (garbage-collector.md)
Created: 2025-08-15 21:00:00 UTC
Version: 1.0.0
"""

import asyncio
import json
import shutil
import os
import stat
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import fnmatch

# Import parent automation components
import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import get_config, MCPAutomationConfig

from .retention_policies import RetentionPolicy
from .audit_logger import AuditLogger, AuditEvent, AuditEventType


class ArtifactType(Enum):
    """Types of artifacts that can be cleaned up."""
    TEMPORARY_FILES = "temporary_files"
    DOWNLOAD_CACHE = "download_cache"
    STAGING_DIRECTORIES = "staging_directories"
    LOG_FILES = "log_files"
    BACKUP_FILES = "backup_files"
    BUILD_ARTIFACTS = "build_artifacts"
    INSTALLATION_PACKAGES = "installation_packages"
    QUARANTINE_FILES = "quarantine_files"


class ArtifactCleanupStatus(Enum):
    """Status of artifact cleanup operations."""
    PENDING = "pending"
    SCANNING = "scanning"
    ANALYZING = "analyzing"
    VALIDATING = "validating"
    CLEANING = "cleaning"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class ArtifactInfo:
    """Information about a file or directory artifact."""
    path: Path
    artifact_type: ArtifactType
    size_bytes: int
    created_at: datetime
    modified_at: datetime
    accessed_at: Optional[datetime]
    owner: str
    permissions: str
    is_directory: bool
    checksum: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = asdict(self)
        result['path'] = str(self.path)
        result['artifact_type'] = self.artifact_type.value
        result['created_at'] = self.created_at.isoformat()
        result['modified_at'] = self.modified_at.isoformat()
        if self.accessed_at:
            result['accessed_at'] = self.accessed_at.isoformat()
        return result
    
    @classmethod
    def from_path(cls, path: Path, artifact_type: ArtifactType) -> 'ArtifactInfo':
        """Create ArtifactInfo from a file path."""
        try:
            stat_info = path.stat()
            
            return cls(
                path=path,
                artifact_type=artifact_type,
                size_bytes=stat_info.st_size,
                created_at=datetime.fromtimestamp(stat_info.st_ctime, tz=timezone.utc),
                modified_at=datetime.fromtimestamp(stat_info.st_mtime, tz=timezone.utc),
                accessed_at=datetime.fromtimestamp(stat_info.st_atime, tz=timezone.utc),
                owner=path.owner() if hasattr(path, 'owner') else 'unknown',
                permissions=stat.filemode(stat_info.st_mode),
                is_directory=path.is_dir()
            )
        except Exception as e:
            # Return minimal info if stat fails
            return cls(
                path=path,
                artifact_type=artifact_type,
                size_bytes=0,
                created_at=datetime.now(timezone.utc),
                modified_at=datetime.now(timezone.utc),
                accessed_at=None,
                owner='unknown',
                permissions='unknown',
                is_directory=path.is_dir() if path.exists() else False
            )


@dataclass
class ArtifactCleanupCandidate:
    """Information about an artifact candidate for cleanup."""
    artifact_info: ArtifactInfo
    cleanup_safety: str  # 'safe', 'caution', 'unsafe'
    cleanup_reason: str
    cleanup_priority: int  # 1-10, higher = more urgent
    age_days: int
    last_access_days: Optional[int]
    related_processes: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = asdict(self)
        result['artifact_info'] = self.artifact_info.to_dict()
        return result


@dataclass
class ArtifactCleanupResult:
    """Results from artifact cleanup execution."""
    server_name: str
    artifact_types: List[ArtifactType]
    status: ArtifactCleanupStatus
    started_at: datetime
    completed_at: Optional[datetime]
    cleaned_artifacts: List[ArtifactInfo]
    skipped_artifacts: List[ArtifactInfo]
    bytes_freed: int
    errors: List[str]
    warnings: List[str]
    dry_run: bool
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = asdict(self)
        result['artifact_types'] = [t.value for t in self.artifact_types]
        result['status'] = self.status.value
        result['started_at'] = self.started_at.isoformat()
        if self.completed_at:
            result['completed_at'] = self.completed_at.isoformat()
        result['cleaned_artifacts'] = [a.to_dict() for a in self.cleaned_artifacts]
        result['skipped_artifacts'] = [a.to_dict() for a in self.skipped_artifacts]
        return result


class ArtifactCleanupService:
    """
    Service for intelligent MCP artifact cleanup.
    
    Provides safe cleanup of temporary files, downloads, staging areas,
    and other artifacts while preserving critical files.
    """
    
    def __init__(self, config: MCPAutomationConfig, audit_logger: AuditLogger):
        """Initialize artifact cleanup service."""
        self.config = config
        self.audit_logger = audit_logger
        self.logger = logging.getLogger(__name__)
        
        # Cleanup patterns for different artifact types
        self.artifact_patterns = {
            ArtifactType.TEMPORARY_FILES: [
                "*.tmp", "*.temp", "*.tmpfile",
                ".DS_Store", "Thumbs.db",
                "*.lock", "*.pid"
            ],
            ArtifactType.DOWNLOAD_CACHE: [
                "*.download", "*.partial",
                "*.crdownload", "*.part"
            ],
            ArtifactType.LOG_FILES: [
                "*.log", "*.log.*",
                "*.out", "*.err"
            ],
            ArtifactType.BACKUP_FILES: [
                "*.bak", "*.backup", "*.old",
                "*~", "#*#"
            ],
            ArtifactType.BUILD_ARTIFACTS: [
                "*.pyc", "*.pyo", "__pycache__",
                "node_modules/.cache",
                "*.build", "dist/"
            ]
        }
        
        # Critical files that should never be deleted
        self.critical_file_patterns = [
            "*.mcp.json", ".mcp.json",
            "package.json", "requirements.txt",
            "*.py", "*.js", "*.sh",
            "README*", "LICENSE*",
            "Dockerfile*", "docker-compose*"
        ]
        
        # Age thresholds for different artifact types (in days)
        self.age_thresholds = {
            ArtifactType.TEMPORARY_FILES: 1,  # 1 day
            ArtifactType.DOWNLOAD_CACHE: 7,   # 1 week
            ArtifactType.LOG_FILES: 30,       # 1 month
            ArtifactType.BACKUP_FILES: 7,     # 1 week
            ArtifactType.BUILD_ARTIFACTS: 3,  # 3 days
            ArtifactType.STAGING_DIRECTORIES: 3,  # 3 days
            ArtifactType.INSTALLATION_PACKAGES: 14,  # 2 weeks
            ArtifactType.QUARANTINE_FILES: 30  # 1 month
        }
        
        self.logger.info("ArtifactCleanupService initialized", extra={
            'artifact_types': len(self.artifact_patterns),
            'critical_patterns': len(self.critical_file_patterns)
        })
    
    async def analyze_cleanup_candidates(
        self,
        server_name: str,
        artifact_types: Optional[List[ArtifactType]] = None,
        retention_policy: Optional[RetentionPolicy] = None
    ) -> Dict[str, Any]:
        """
        Analyze artifact cleanup candidates for a specific server.
        
        Args:
            server_name: Name of the MCP server
            artifact_types: Types of artifacts to analyze (None for all)
            retention_policy: Custom retention policy (None for default)
            
        Returns:
            Analysis results with cleanup candidates and recommendations
        """
        analysis_start = datetime.now(timezone.utc)
        
        try:
            if artifact_types is None:
                artifact_types = list(ArtifactType)
            
            self.logger.info(f"Analyzing artifact cleanup candidates for: {server_name}", extra={
                'artifact_types': [t.value for t in artifact_types]
            })
            
            # Get server-specific paths
            server_paths = await self._get_server_paths(server_name)
            
            analysis_results = {
                'server_name': server_name,
                'analysis_timestamp': analysis_start.isoformat(),
                'analyzed_paths': [str(p) for p in server_paths],
                'artifact_types': [t.value for t in artifact_types],
                'candidates_by_type': {},
                'total_candidates': 0,
                'estimated_space_freed': 0,
                'recommendations': []
            }
            
            # Analyze each artifact type
            for artifact_type in artifact_types:
                type_candidates = await self._analyze_artifact_type(
                    server_paths, artifact_type, retention_policy
                )
                
                analysis_results['candidates_by_type'][artifact_type.value] = [
                    candidate.to_dict() for candidate in type_candidates
                ]
                analysis_results['total_candidates'] += len(type_candidates)
                
                # Calculate estimated space freed
                for candidate in type_candidates:
                    if candidate.cleanup_safety in ['safe', 'caution']:
                        analysis_results['estimated_space_freed'] += candidate.artifact_info.size_bytes
            
            # Generate recommendations
            analysis_results['recommendations'] = await self._generate_artifact_recommendations(
                server_name, analysis_results
            )
            
            # Log analysis completion
            await self.audit_logger.log_event(AuditEvent(
                event_type=AuditEventType.ANALYSIS_COMPLETED,
                timestamp=analysis_start,
                component='artifact_cleanup',
                user='system',
                action='analyze_cleanup_candidates',
                resource=server_name,
                details={
                    'artifact_types': [t.value for t in artifact_types],
                    'total_candidates': analysis_results['total_candidates'],
                    'estimated_space_freed': analysis_results['estimated_space_freed']
                }
            ))
            
            self.logger.info(f"Artifact analysis completed for {server_name}", extra={
                'total_candidates': analysis_results['total_candidates'],
                'estimated_space_freed': analysis_results['estimated_space_freed']
            })
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"Error analyzing artifacts for {server_name}: {e}", exc_info=True)
            raise
    
    async def execute_cleanup(
        self,
        server_name: str,
        retention_policy: RetentionPolicy,
        dry_run: bool = False,
        artifact_types: Optional[List[ArtifactType]] = None
    ) -> ArtifactCleanupResult:
        """
        Execute artifact cleanup for a specific server.
        
        Args:
            server_name: Name of the MCP server
            retention_policy: Retention policy to apply
            dry_run: If True, simulate cleanup without making changes
            artifact_types: Types of artifacts to clean (None for all)
            
        Returns:
            Cleanup execution results
        """
        cleanup_start = datetime.now(timezone.utc)
        
        if artifact_types is None:
            artifact_types = list(ArtifactType)
        
        result = ArtifactCleanupResult(
            server_name=server_name,
            artifact_types=artifact_types,
            status=ArtifactCleanupStatus.SCANNING,
            started_at=cleanup_start,
            completed_at=None,
            cleaned_artifacts=[],
            skipped_artifacts=[],
            bytes_freed=0,
            errors=[],
            warnings=[],
            dry_run=dry_run
        )
        
        try:
            self.logger.info(f"Starting artifact cleanup for {server_name}", extra={
                'dry_run': dry_run,
                'artifact_types': [t.value for t in artifact_types]
            })
            
            # Log cleanup start
            await self.audit_logger.log_event(AuditEvent(
                event_type=AuditEventType.CLEANUP_STARTED,
                timestamp=cleanup_start,
                component='artifact_cleanup',
                user='system',
                action='execute_cleanup',
                resource=server_name,
                details={
                    'dry_run': dry_run,
                    'artifact_types': [t.value for t in artifact_types]
                }
            ))
            
            # Analyze cleanup candidates
            result.status = ArtifactCleanupStatus.ANALYZING
            analysis = await self.analyze_cleanup_candidates(
                server_name, artifact_types, retention_policy
            )
            
            # Validate cleanup safety
            result.status = ArtifactCleanupStatus.VALIDATING
            safety_validation = await self._validate_artifact_cleanup_safety(server_name, analysis)
            
            if not safety_validation['safe'] and not dry_run:
                result.status = ArtifactCleanupStatus.FAILED
                result.errors.extend(safety_validation['errors'])
                result.warnings.extend(safety_validation['warnings'])
                return result
            
            result.warnings.extend(safety_validation['warnings'])
            
            # Execute cleanup
            result.status = ArtifactCleanupStatus.CLEANING
            
            for artifact_type in artifact_types:
                type_candidates_data = analysis['candidates_by_type'].get(artifact_type.value, [])
                
                for candidate_data in type_candidates_data:
                    try:
                        # Reconstruct candidate from dict
                        artifact_path = Path(candidate_data['artifact_info']['path'])
                        artifact_info = ArtifactInfo.from_path(artifact_path, artifact_type)
                        
                        # Check if this artifact should be skipped
                        if candidate_data['cleanup_safety'] == 'unsafe':
                            result.skipped_artifacts.append(artifact_info)
                            result.warnings.append(f"Skipped unsafe artifact: {artifact_path}")
                            continue
                        
                        # Perform cleanup
                        if dry_run:
                            # Simulate cleanup
                            result.cleaned_artifacts.append(artifact_info)
                            result.bytes_freed += artifact_info.size_bytes
                            self.logger.info(f"[DRY RUN] Would remove artifact: {artifact_path}")
                        else:
                            # Actual cleanup
                            cleanup_success = await self._remove_artifact(artifact_info)
                            
                            if cleanup_success:
                                result.cleaned_artifacts.append(artifact_info)
                                result.bytes_freed += artifact_info.size_bytes
                                
                                # Log artifact removal
                                await self.audit_logger.log_event(AuditEvent(
                                    event_type=AuditEventType.ARTIFACT_REMOVED,
                                    timestamp=datetime.now(timezone.utc),
                                    component='artifact_cleanup',
                                    user='system',
                                    action='remove_artifact',
                                    resource=str(artifact_path),
                                    details={
                                        'artifact_type': artifact_type.value,
                                        'size_bytes': artifact_info.size_bytes,
                                        'server_name': server_name
                                    }
                                ))
                                
                                self.logger.info(f"Removed artifact: {artifact_path}")
                            else:
                                result.skipped_artifacts.append(artifact_info)
                                result.warnings.append(f"Failed to remove artifact: {artifact_path}")
                    
                    except Exception as e:
                        error_msg = f"Error processing artifact {candidate_data.get('artifact_info', {}).get('path', 'unknown')}: {e}"
                        result.errors.append(error_msg)
                        self.logger.error(error_msg, exc_info=True)
            
            # Complete cleanup
            cleanup_end = datetime.now(timezone.utc)
            result.completed_at = cleanup_end
            result.status = ArtifactCleanupStatus.COMPLETED
            
            # Log cleanup completion
            await self.audit_logger.log_event(AuditEvent(
                event_type=AuditEventType.CLEANUP_COMPLETED,
                timestamp=cleanup_end,
                component='artifact_cleanup',
                user='system',
                action='execute_cleanup',
                resource=server_name,
                details={
                    'status': result.status.value,
                    'cleaned_artifacts': len(result.cleaned_artifacts),
                    'bytes_freed': result.bytes_freed,
                    'dry_run': dry_run
                }
            ))
            
            self.logger.info(f"Artifact cleanup completed for {server_name}", extra={
                'status': result.status.value,
                'cleaned_artifacts': len(result.cleaned_artifacts),
                'bytes_freed': result.bytes_freed,
                'dry_run': dry_run
            })
            
            return result
            
        except Exception as e:
            cleanup_end = datetime.now(timezone.utc)
            result.completed_at = cleanup_end
            result.status = ArtifactCleanupStatus.FAILED
            result.errors.append(f"Cleanup execution error: {e}")
            
            self.logger.error(f"Artifact cleanup failed for {server_name}: {e}", exc_info=True)
            
            # Log cleanup failure
            await self.audit_logger.log_event(AuditEvent(
                event_type=AuditEventType.CLEANUP_FAILED,
                timestamp=cleanup_end,
                component='artifact_cleanup',
                user='system',
                action='execute_cleanup',
                resource=server_name,
                details={'error': str(e)}
            ))
            
            return result
    
    async def _get_server_paths(self, server_name: str) -> List[Path]:
        """Get all paths to search for artifacts for a specific server."""
        paths = []
        
        # Server-specific staging directory
        server_staging = self.config.paths.staging_root / server_name
        if server_staging.exists():
            paths.append(server_staging)
        
        # Server-specific backup directory
        server_backup = self.config.paths.backup_root / server_name
        if server_backup.exists():
            paths.append(server_backup)
        
        # Server-specific logs directory
        server_logs = self.config.paths.logs_root / server_name
        if server_logs.exists():
            paths.append(server_logs)
        
        # MCP automation temporary directories
        automation_temp = self.config.paths.automation_root / "temp" / server_name
        if automation_temp.exists():
            paths.append(automation_temp)
        
        # Global temporary and cache directories
        global_paths = [
            self.config.paths.staging_root / "temp",
            self.config.paths.automation_root / "cache",
            self.config.paths.automation_root / "downloads",
            Path("/tmp") / "mcp_automation",
            Path("/var/tmp") / "mcp_automation"
        ]
        
        for path in global_paths:
            if path.exists():
                paths.append(path)
        
        return paths
    
    async def _analyze_artifact_type(
        self,
        search_paths: List[Path],
        artifact_type: ArtifactType,
        retention_policy: Optional[RetentionPolicy]
    ) -> List[ArtifactCleanupCandidate]:
        """Analyze artifacts of a specific type."""
        
        candidates = []
        patterns = self.artifact_patterns.get(artifact_type, [])
        age_threshold = self.age_thresholds.get(artifact_type, 7)
        
        # Override age threshold with retention policy if provided
        if retention_policy:
            age_threshold = retention_policy.min_age_days
        
        for search_path in search_paths:
            if not search_path.exists():
                continue
            
            try:
                # Find matching files/directories
                for pattern in patterns:
                    matches = await self._find_matching_artifacts(search_path, pattern)
                    
                    for artifact_path in matches:
                        if await self._is_critical_file(artifact_path):
                            continue  # Skip critical files
                        
                        artifact_info = ArtifactInfo.from_path(artifact_path, artifact_type)
                        candidate = await self._analyze_single_artifact(
                            artifact_info, age_threshold
                        )
                        candidates.append(candidate)
            
            except Exception as e:
                self.logger.warning(f"Error analyzing {search_path} for {artifact_type.value}: {e}")
        
        return candidates
    
    async def _find_matching_artifacts(self, search_path: Path, pattern: str) -> List[Path]:
        """Find files matching a pattern in a directory."""
        matches = []
        
        try:
            if search_path.is_file():
                if fnmatch.fnmatch(search_path.name, pattern):
                    matches.append(search_path)
            elif search_path.is_dir():
                # Recursive search with pattern matching
                for item in search_path.rglob(pattern):
                    matches.append(item)
        except Exception as e:
            self.logger.warning(f"Error searching {search_path} with pattern {pattern}: {e}")
        
        return matches
    
    async def _analyze_single_artifact(
        self,
        artifact_info: ArtifactInfo,
        age_threshold_days: int
    ) -> ArtifactCleanupCandidate:
        """Analyze a single artifact for cleanup eligibility."""
        
        now = datetime.now(timezone.utc)
        age_days = (now - artifact_info.modified_at).days
        last_access_days = None
        
        if artifact_info.accessed_at:
            last_access_days = (now - artifact_info.accessed_at).days
        
        # Determine cleanup safety
        safety = 'safe'
        cleanup_reason = f'old_{artifact_info.artifact_type.value}'
        cleanup_priority = 5
        
        # Age-based safety assessment
        if age_days < 1:
            safety = 'unsafe'
            cleanup_reason = f'too_recent_{age_days}_days'
            cleanup_priority = 1
        elif age_days < age_threshold_days:
            safety = 'caution'
            cleanup_reason = f'under_threshold_{age_days}/{age_threshold_days}_days'
            cleanup_priority = 3
        else:
            safety = 'safe'
            cleanup_priority = min(10, 5 + (age_days // 7))  # Higher priority for older files
        
        # Size-based adjustments
        if artifact_info.size_bytes > 100 * 1024 * 1024:  # > 100MB
            cleanup_priority += 2
            cleanup_reason += '_large_file'
        
        # Directory vs file adjustments
        if artifact_info.is_directory:
            # Be more cautious with directories
            if safety == 'safe':
                safety = 'caution'
            cleanup_reason += '_directory'
        
        # Check for related processes
        related_processes = await self._check_artifact_in_use(artifact_info.path)
        if related_processes:
            safety = 'unsafe'
            cleanup_reason += '_in_use'
            cleanup_priority = 1
        
        return ArtifactCleanupCandidate(
            artifact_info=artifact_info,
            cleanup_safety=safety,
            cleanup_reason=cleanup_reason,
            cleanup_priority=cleanup_priority,
            age_days=age_days,
            last_access_days=last_access_days,
            related_processes=related_processes
        )
    
    async def _is_critical_file(self, path: Path) -> bool:
        """Check if a file is critical and should never be deleted."""
        
        # Check against critical file patterns
        for pattern in self.critical_file_patterns:
            if fnmatch.fnmatch(path.name, pattern):
                return True
        
        # Check for specific critical paths
        critical_paths = [
            '.mcp.json',
            'package.json',
            'requirements.txt',
            'Dockerfile',
            'docker-compose.yml'
        ]
        
        if path.name in critical_paths:
            return True
        
        # Check if path is within critical directories
        critical_dirs = [
            'node_modules',
            '.git',
            '.mcp',
            'src',
            'lib'
        ]
        
        for critical_dir in critical_dirs:
            if critical_dir in path.parts:
                return True
        
        return False
    
    async def _check_artifact_in_use(self, path: Path) -> List[str]:
        """Check if an artifact is currently in use by running processes."""
        related_processes = []
        
        try:
            # Check for open file handles
            result = await asyncio.create_subprocess_exec(
                'lsof', str(path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await result.communicate()
            
            if result.returncode == 0 and stdout:
                # Parse lsof output to extract process names
                lines = stdout.decode().strip().split('\n')[1:]  # Skip header
                for line in lines:
                    parts = line.split()
                    if len(parts) > 1:
                        related_processes.append(parts[0])  # Process name
        
        except Exception:
            # lsof might not be available or fail, that's OK
            pass
        
        return related_processes
    
    async def _validate_artifact_cleanup_safety(
        self,
        server_name: str,
        analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validate the safety of artifact cleanup operation."""
        
        validation_result = {
            'safe': True,
            'errors': [],
            'warnings': []
        }
        
        total_candidates = analysis['total_candidates']
        estimated_space = analysis['estimated_space_freed']
        
        # Check for large cleanup operations
        if total_candidates > 1000:
            validation_result['warnings'].append(
                f"Large cleanup operation: {total_candidates} artifacts to process"
            )
        
        if estimated_space > 1024 * 1024 * 1024:  # > 1GB
            validation_result['warnings'].append(
                f"Large space impact: {estimated_space:,} bytes to be freed"
            )
        
        # Check for unsafe artifacts in candidates
        for artifact_type, candidates in analysis['candidates_by_type'].items():
            unsafe_count = len([c for c in candidates if c['cleanup_safety'] == 'unsafe'])
            if unsafe_count > 0:
                validation_result['warnings'].append(
                    f"Found {unsafe_count} unsafe {artifact_type} artifacts (will be skipped)"
                )
        
        # Check server status
        server_running = await self._check_server_running(server_name)
        if server_running:
            validation_result['warnings'].append(
                f"Server {server_name} is running - some artifacts may be locked"
            )
        
        return validation_result
    
    async def _check_server_running(self, server_name: str) -> bool:
        """Check if an MCP server is currently running."""
        try:
            # Check for running processes
            result = await asyncio.create_subprocess_exec(
                'pgrep', '-f', server_name,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await result.communicate()
            return len(stdout.decode().strip()) > 0
        except Exception:
            return False
    
    async def _remove_artifact(self, artifact_info: ArtifactInfo) -> bool:
        """Remove a specific artifact from the system."""
        try:
            if artifact_info.is_directory:
                # Remove directory and all contents
                shutil.rmtree(artifact_info.path)
                self.logger.debug(f"Removed directory: {artifact_info.path}")
            else:
                # Remove file
                artifact_info.path.unlink()
                self.logger.debug(f"Removed file: {artifact_info.path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error removing artifact {artifact_info.path}: {e}", exc_info=True)
            return False
    
    async def _generate_artifact_recommendations(
        self,
        server_name: str,
        analysis: Dict[str, Any]
    ) -> List[str]:
        """Generate cleanup recommendations for artifacts."""
        recommendations = []
        
        total_candidates = analysis['total_candidates']
        estimated_space = analysis['estimated_space_freed']
        
        if total_candidates == 0:
            recommendations.append("No artifacts available for cleanup")
        else:
            recommendations.append(
                f"{total_candidates} artifacts can be cleaned, freeing ~{estimated_space:,} bytes"
            )
        
        # Type-specific recommendations
        for artifact_type, candidates in analysis['candidates_by_type'].items():
            if candidates:
                safe_count = len([c for c in candidates if c['cleanup_safety'] == 'safe'])
                if safe_count > 0:
                    recommendations.append(f"{safe_count} {artifact_type} items are safe to remove")
        
        if estimated_space > 1024 * 1024 * 100:  # > 100MB
            recommendations.append("Significant space savings available from artifact cleanup")
        
        recommendations.extend([
            "Run in dry-run mode first to validate cleanup plan",
            "Consider automating artifact cleanup with scheduled jobs",
            "Monitor server functionality after cleanup"
        ])
        
        return recommendations


async def main():
    """Main entry point for artifact cleanup testing."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        from ..config import get_config
        from .audit_logger import AuditLogger
        from .retention_policies import RetentionPolicy
        
        config = get_config()
        audit_logger = AuditLogger(config)
        artifact_cleanup = ArtifactCleanupService(config, audit_logger)
        
        # Example: Analyze artifact cleanup candidates
        logger.info("Analyzing artifact cleanup candidates...")
        servers = ['postgres', 'files']  # Example servers
        
        for server in servers:
            try:
                analysis = await artifact_cleanup.analyze_cleanup_candidates(server)
                logger.info(f"\nArtifact analysis for {server}:")
                logger.info(json.dumps(analysis, indent=2, default=str))
            except Exception as e:
                logger.error(f"Error analyzing {server}: {e}")
        
    except Exception as e:
        logging.error(f"Error in artifact cleanup: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())