#!/usr/bin/env python3
"""
pentestgpt Agent Implementation
"""
import os
import json
import asyncio
import aiohttp
from typing import Dict, Any, Optional

class PentestgptAgent:
    def __init__(self):
        self.name = "pentestgpt"
        self.ollama_base = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
        self.model = os.getenv("DEFAULT_MODEL", "tinyllama")
        
    async def process(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Process incoming request"""
        # Implement agent-specific logic here
        prompt = request.get("prompt", "")
        
        # Call Ollama for LLM processing
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.ollama_base}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False
                }
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return {
                        "agent": self.name,
                        "response": result.get("response", ""),
                        "status": "success"
                    }
                else:
                    return {
                        "agent": self.name,
                        "error": f"Ollama request failed: {response.status}",
                        "status": "error"
                    }

def run():
    """Run the agent"""
    print(f"Starting pentestgpt agent...")
    agent = PentestgptAgent()
    
    # In production, this would start a web server
    # For now, just indicate the agent is ready
    print(f"pentestgpt agent ready on port :8080")
    
    # Keep the agent running
    try:
        asyncio.get_event_loop().run_forever()
    except KeyboardInterrupt:
        print(f"Shutting down pentestgpt agent...")

if __name__ == "__main__":
    run()
