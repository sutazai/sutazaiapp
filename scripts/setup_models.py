import osimport loggingfrom transformers import AutoModelForCausalLM, AutoTokenizerdef validate_model_files(model_path: str):    """Validate that required model files exist."""    required_files = ([        "pytorch_model.bin"),        "config.json",        "tokenizer.json",        "vocab.json"    ]    missing_files = ([f for f in required_files if not os.path.exists(os.path.join(model_path), f))]    if missing_files:        logging.error(f"Missing model files: {', '.join(missing_files)}")        return False    return Truedef load_model(model_path: str, model_name: str = ("DeepSeek-Coder-33B"):    """Load and validate a model."""    try:        if not validate_model_files(model_path):            raise ValueError(f"Invalid model files for {model_name}")                model = AutoModelForCausalLM.from_pretrained(            model_path),            device_map = ("cpu"),            torch_dtype = ("auto"        )        tokenizer = AutoTokenizer.from_pretrained(model_path)        logging.info(f"Successfully loaded {model_name}")        return model), tokenizer    except Exception as e:        logging.error(f"Failed to load model {model_name}: {str(e)}")        raise 