version: '3.8'

services:
  # Ollama with TinyLlama - Direct Local LLM
  ollama:
    container_name: sutazai-ollama-tiny
    image: ollama/ollama:latest
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts:/scripts
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
    command: serve
    healthcheck:
      test: ["CMD", "wget", "-q", "-O-", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 30s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '1.0'
          memory: 1G
    networks:
      - sutazai-network

  # PostgreSQL (Minimal)
  postgres:
    container_name: sutazai-postgres
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: sutazai_password
      POSTGRES_DB: sutazai
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./config/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U sutazai"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - sutazai-network

  # Redis (Minimal)
  redis:
    container_name: sutazai-redis
    image: redis:7-alpine
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - sutazai-network

  # Backend Service (Core API)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.minimal
    container_name: sutazai-backend
    restart: unless-stopped
    volumes:
      - ./backend:/app
      - ./data:/data
      - ./logs:/logs
      - agent_workspaces:/app/agent_workspaces
    environment:
      # Core settings
      SECRET_KEY: dev-secret-key-change-in-production
      API_V1_STR: /api/v1
      BACKEND_CORS_ORIGINS: '["http://localhost:8501", "http://localhost:8000"]'
      LOG_LEVEL: INFO
      # Database
      DATABASE_URL: postgresql://sutazai:sutazai_password@postgres:5432/sutazai
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: sutazai
      POSTGRES_PASSWORD: sutazai_password
      POSTGRES_DB: sutazai
      # Redis
      REDIS_URL: redis://redis:6379/0
      REDIS_HOST: redis
      REDIS_PORT: 6379
      # Ollama
      OLLAMA_BASE_URL: http://ollama:11434
      MODEL_NAME: tinyllama:latest
      USE_NATIVE_OLLAMA: "true"
      # Resource limits
      MAX_WORKERS: 2
      MAX_CONCURRENT_REQUESTS: 10
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - sutazai-network

  # Task Assignment Coordinator (Primary Router)
  task-coordinator:
    container_name: sutazai-task-coordinator
    build:
      context: ../../agents/infrastructure-devops
      dockerfile: Dockerfile
    environment:
      - MODEL_NAME=tinyllama:latest
      - OLLAMA_BASE_URL=http://ollama:11434
      - BACKEND_URL=http://backend-agi:8000
      - AGENT_NAME=task-assignment-coordinator
      - MAX_CONCURRENT_TASKS=2
      - USE_NATIVE_OLLAMA=true
      - LOG_LEVEL=INFO
    depends_on:
      - ollama
      - backend-agi
    volumes:
      - ./workspace:/workspace
      - ./agents:/agents
      - ./logs:/logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - sutazai-network

  # TinyLlama Model Initialization
  init-tinyllama:
    container_name: sutazai-init-tinyllama
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
    entrypoint: ["/bin/sh", "-c"]
    command: |
      echo 'Waiting for Ollama to be ready...'
      for i in 1 2 3 4 5; do
        if wget -q -O- http://ollama:11434/api/tags >/dev/null 2>&1; then
          break
        fi
        echo 'Waiting for Ollama... attempt $$i'
        sleep 5
      done
      echo 'Pulling TinyLlama model...'
      ollama pull tinyllama:latest
      echo 'TinyLlama model ready!'
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    networks:
      - sutazai-network

volumes:
  ollama_data:
  postgres_data:
  redis_data:
  agent_workspaces:

networks:
  sutazai-network:
    driver: bridge